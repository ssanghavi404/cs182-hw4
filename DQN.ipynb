{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Networks\n",
    "Previously, we trained policies using policy gradient algorithms, which directly estimated the gradient of the returns for the policy and performed stochastic gradient ascent. In this section, we will now implement deep Q-learning, which does not explicitly optimize a policy, but simply infers the policy from a learned Q function that is trained via dynamic programming.\n",
    "\n",
    "We will assume discrete action spaces for this notebook as to enable us to easily select actions that maximize the Q-function at given states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual, a bit of setup\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import deeprl.infrastructure.pytorch_util as ptu\n",
    "\n",
    "from deeprl.infrastructure.rl_trainer import RL_Trainer\n",
    "from deeprl.infrastructure.trainers import PG_Trainer\n",
    "from deeprl.infrastructure.trainers import DQN_Trainer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def remove_folder(path):\n",
    "    # check if folder exists\n",
    "    if os.path.exists(path): \n",
    "        print(\"Clearing old results at {}\".format(path))\n",
    "        # remove if exists\n",
    "        shutil.rmtree(path)\n",
    "    else:\n",
    "        print(\"Folder {} does not exist yet. No old results to delete\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_base_args_dict = dict(\n",
    "    env_name = 'LunarLander-v3', #@param \n",
    "    exp_name = 'test_dqn', #@param\n",
    "    save_params = False, #@param {type: \"boolean\"}\n",
    "    \n",
    "    ## PDF will tell you how to set ep_len\n",
    "    ## and discount for each environment\n",
    "    ep_len = 200, #@param {type: \"integer\"}\n",
    "    # discount = 0.95, #@param {type: \"number\"}\n",
    "\n",
    "    # Training\n",
    "    num_agent_train_steps_per_iter = 1, #@param {type: \"integer\"})\n",
    "    num_critic_updates_per_agent_update = 1, #@param {type: \"integer\"}\n",
    "  \n",
    "    #@markdown Q-learning parameters\n",
    "    double_q = False, #@param {type: \"boolean\"}\n",
    "\n",
    "    # batches & buffers\n",
    "    batch_size = 32, #@param {type: \"integer\"})\n",
    "    batch_size_initial=1000,\n",
    "\n",
    "    #@markdown logging\n",
    "    video_log_freq = -1, #@param {type: \"integer\"}\n",
    "    scalar_log_freq = 1000, #@param {type: \"integer\"}\n",
    "\n",
    "    #@markdown gpu & run-time settings\n",
    "    no_gpu = False, #@param {type: \"boolean\"}\n",
    "    which_gpu = 0, #@param {type: \"integer\"}\n",
    "    seed = 2, #@param {type: \"integer\"}\n",
    "    logdir = 'test',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN updates\n",
    "Recall in Q-learning, we attempt to solve the optimal state-action values (which we refer to as Q-values $Q(s,a)$), by finding solutions to the Bellman equation given by\n",
    "$$Q(s,a) = r(s,a) + \\gamma \\mathbb{E}_{s' \\sim p(s'\\vert s,a)}[\\max_{a'}Q(s', a')].$$\n",
    "\n",
    "Regular tabular Q-learning would take sample transitions $(s, a, r, s')$ and perform updates according to\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha (r(s,a) + \\gamma \\max_{a'} Q(s', a') - Q(s,a)),$$\n",
    "where $\\alpha$ is a stepsize parameter.\n",
    "\n",
    "This can be interpreted as updating $Q(s,a)$ by taking one gradient step on a squared Bellman error objective\n",
    "$$(r(s, a) +\\gamma \\max_{a'} \\tilde Q(s', a') - Q(s,a))^2,$$\n",
    "where $\\tilde Q$ is a copy of $Q$, but is not differentiated when taking the gradient step.\n",
    "\n",
    "Adapting this update to the setting where we use a neural network with parameters $\\theta$ to approximate $Q(s,a)$, we then train $\\theta$ with the loss function \n",
    "$$\\min_{\\theta} \\mathbb{E}_{s, a, s' \\sim D} [L(Q_{\\theta}(s,a), r(s,a) + \\gamma \\max_{a'} Q_{\\tilde \\theta}(s', a'))]$$\n",
    "where $D$ is our replay buffer containing past transitions we've experienced, $L$ is some loss function capturing how far the predicted Q-values are from the target values, and $\\tilde \\theta$ are the target Q function parameters, which are usually a delayed copy of $\\theta$ for stability reasons.\n",
    "\n",
    "We note our previous policy gradient algorithms were _on-policy_ algoritms, which meant they updated the policy using only the data collected from the most recent policy, and discard all the data after using it just once. In contrast, DQN uses _off-policy_ updates by sampling data from all past interactions, allowing for data reuse over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill out the missing components for the basic Q-learning update in <code>critics/dqn_critic.py</code> (not including the double_q section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################\n",
      "logging outputs to  test\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "LunarLander-v3\n",
      "Weight before update (first row) [ 0.07646337 -0.07932475  0.09140956 -0.01702595  0.14239588  0.07935759\n",
      " -0.03831157 -0.2694876   0.07610479]\n",
      "Initial loss 0.9408444\n",
      "Initial Loss Error 8.831612855468697e-09 should be on the order of 1e-6 or lower\n",
      "0.9007309\n",
      "0.8621322\n",
      "0.82467556\n",
      "0.7889254\n",
      "Loss Error 5.904878045285727e-09 should be on the order of 1e-6 or lower\n",
      "Weight after update (first row) (64, 9)\n",
      "[ 0.07154972 -0.08432525  0.08641808 -0.02193824  0.13749464  0.08425293\n",
      " -0.04113942 -0.27120373  0.08096083]\n",
      "Weight Update Error 8.937585787696078e-07 should be on the order of 1e-6 or lower\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sagarasanghavi/.pyenv/versions/3.7.7/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "#### Test DQN updates\n",
    "dqn_args = dict(dqn_base_args_dict)\n",
    "\n",
    "env_str = 'LunarLander'\n",
    "dqn_args['env_name'] = '{}-v3'.format(env_str)\n",
    "dqn_args['double_q'] = False\n",
    "dqntrainer = DQN_Trainer(dqn_args)\n",
    "dqnagent = dqntrainer.rl_trainer.agent\n",
    "critic = dqnagent.critic\n",
    "\n",
    "ob_dim = critic.ob_dim\n",
    "ac_dim = 6\n",
    "N = 5\n",
    "\n",
    "np.random.seed(0)\n",
    "obs = np.random.normal(size=(N, ob_dim))\n",
    "acts = np.random.choice(ac_dim, size=(N,))\n",
    "next_obs = np.random.normal(size=(N, ob_dim))\n",
    "rewards = np.random.normal(size=N)\n",
    "terminals = np.zeros(N)\n",
    "terminals[0] = 1\n",
    "\n",
    "first_weight_before = np.array(ptu.to_numpy(next(critic.q_net.parameters())))\n",
    "print(\"Weight before update (first row)\", first_weight_before[0])\n",
    "\n",
    "\n",
    "loss = critic.update(obs, acts, next_obs, rewards, terminals)['Training Loss']\n",
    "expected_loss = 0.9408444\n",
    "loss_error = rel_error(loss, expected_loss)\n",
    "print(\"Initial loss\", loss)\n",
    "print(\"Initial Loss Error\", loss_error, \"should be on the order of 1e-6 or lower\")\n",
    "\n",
    "for i in range(4):\n",
    "    loss = critic.update(obs, acts, next_obs, rewards, terminals)['Training Loss']\n",
    "    print(loss)\n",
    "\n",
    "expected_loss = 0.7889254\n",
    "loss_error = rel_error(loss, expected_loss)\n",
    "print(\"Loss Error\", loss_error, \"should be on the order of 1e-6 or lower\")\n",
    "\n",
    "\n",
    "first_weight_after = np.array(ptu.to_numpy(next(critic.q_net.parameters())))\n",
    "print(\"Weight after update (first row)\", first_weight_after.shape)\n",
    "# Test DQN gradient\n",
    "print(first_weight_after[0])\n",
    "weight_change_partial = first_weight_after[0] - first_weight_before[0]\n",
    "expected_weight_change = np.array([-0.00491365, -0.00500049, -0.00499149, -0.00491229, -0.00490125,  0.00489534,\n",
    " -0.00282785, -0.00171614,  0.00485604])\n",
    "\n",
    "\n",
    "updated_weight_error = rel_error(weight_change_partial, expected_weight_change)\n",
    "print(\"Weight Update Error\", updated_weight_error, \"should be on the order of 1e-6 or lower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the missing components in the get_action method of <code>policies/argmax_policy.py</code> and the step_env method in <code>agents/dqn_agent.py</code> to allow our agent to interact with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################\n",
      "logging outputs to  test\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "LunarLander-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sagarasanghavi/.pyenv/versions/3.7.7/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "### Test argmax policy\n",
    "dqn_args = dict(dqn_base_args_dict)\n",
    "\n",
    "env_str = 'LunarLander'\n",
    "dqn_args['env_name'] = '{}-v3'.format(env_str)\n",
    "dqn_args['double_q'] = False\n",
    "dqntrainer = DQN_Trainer(dqn_args)\n",
    "dqnagent = dqntrainer.rl_trainer.agent\n",
    "actor = dqnagent.actor\n",
    "\n",
    "ob_dim = critic.ob_dim\n",
    "ac_dim = 6\n",
    "N = 5\n",
    "\n",
    "np.random.seed(0)\n",
    "obs = np.random.normal(size=(N, ob_dim))\n",
    "\n",
    "actions = actor.get_action(obs)\n",
    "correct_actions = np.array([1, 0, 1, 0, 1])\n",
    "\n",
    "assert np.all(correct_actions == actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test our DQN implementation on the LunarLander environment. These experiments can take a while to run (over 10 minutes per seed) on CPU, so start early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing old results at logs/dqn/LunarLander/vanilla_dqn\n",
      "Running DQN experiment with seed 0\n",
      "########################\n",
      "logging outputs to  logs/dqn/LunarLander/vanilla_dqn/seed0\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "LunarLander-v3\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1\n",
      "mean reward (100 episodes) nan\n",
      "best mean reward -inf\n",
      "running time 0.001006\n",
      "Train_EnvstepsSoFar : 1\n",
      "TimeSinceStart : 0.0010058879852294922\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sagarasanghavi/.pyenv/versions/3.7.7/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 1000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1001\n",
      "mean reward (100 episodes) -361.458927\n",
      "best mean reward -inf\n",
      "running time 0.663646\n",
      "Train_EnvstepsSoFar : 1001\n",
      "Train_AverageReturn : -361.4589273606985\n",
      "TimeSinceStart : 0.6636459827423096\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2001\n",
      "mean reward (100 episodes) -310.004133\n",
      "best mean reward -inf\n",
      "running time 4.109574\n",
      "Train_EnvstepsSoFar : 2001\n",
      "Train_AverageReturn : -310.0041333667857\n",
      "TimeSinceStart : 4.109573841094971\n",
      "Training Loss : 0.5019471645355225\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3001\n",
      "mean reward (100 episodes) -310.660305\n",
      "best mean reward -inf\n",
      "running time 6.949341\n",
      "Train_EnvstepsSoFar : 3001\n",
      "Train_AverageReturn : -310.660304801111\n",
      "TimeSinceStart : 6.9493408203125\n",
      "Training Loss : 0.5492397546768188\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4001\n",
      "mean reward (100 episodes) -260.203444\n",
      "best mean reward -inf\n",
      "running time 10.008879\n",
      "Train_EnvstepsSoFar : 4001\n",
      "Train_AverageReturn : -260.20344413606557\n",
      "TimeSinceStart : 10.008878707885742\n",
      "Training Loss : 4.444127559661865\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5001\n",
      "mean reward (100 episodes) -259.046996\n",
      "best mean reward -inf\n",
      "running time 12.875608\n",
      "Train_EnvstepsSoFar : 5001\n",
      "Train_AverageReturn : -259.04699627350914\n",
      "TimeSinceStart : 12.87560772895813\n",
      "Training Loss : 0.3273715078830719\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6001\n",
      "mean reward (100 episodes) -258.973608\n",
      "best mean reward -inf\n",
      "running time 15.754717\n",
      "Train_EnvstepsSoFar : 6001\n",
      "Train_AverageReturn : -258.9736079329956\n",
      "TimeSinceStart : 15.754716873168945\n",
      "Training Loss : 1.4801983833312988\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7001\n",
      "mean reward (100 episodes) -250.116947\n",
      "best mean reward -inf\n",
      "running time 18.583018\n",
      "Train_EnvstepsSoFar : 7001\n",
      "Train_AverageReturn : -250.1169470353506\n",
      "TimeSinceStart : 18.5830180644989\n",
      "Training Loss : 0.47548556327819824\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8001\n",
      "mean reward (100 episodes) -239.319432\n",
      "best mean reward -inf\n",
      "running time 21.165849\n",
      "Train_EnvstepsSoFar : 8001\n",
      "Train_AverageReturn : -239.3194317914511\n",
      "TimeSinceStart : 21.16584873199463\n",
      "Training Loss : 0.42923206090927124\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9001\n",
      "mean reward (100 episodes) -227.668444\n",
      "best mean reward -inf\n",
      "running time 23.709707\n",
      "Train_EnvstepsSoFar : 9001\n",
      "Train_AverageReturn : -227.66844402787598\n",
      "TimeSinceStart : 23.709706783294678\n",
      "Training Loss : 0.73915034532547\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10001\n",
      "mean reward (100 episodes) -223.720492\n",
      "best mean reward -inf\n",
      "running time 26.292246\n",
      "Train_EnvstepsSoFar : 10001\n",
      "Train_AverageReturn : -223.7204920224475\n",
      "TimeSinceStart : 26.292245626449585\n",
      "Training Loss : 0.544935405254364\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 11000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 11001\n",
      "mean reward (100 episodes) -217.376993\n",
      "best mean reward -inf\n",
      "running time 28.830749\n",
      "Train_EnvstepsSoFar : 11001\n",
      "Train_AverageReturn : -217.37699338380787\n",
      "TimeSinceStart : 28.830748796463013\n",
      "Training Loss : 1.0178295373916626\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 12000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 12001\n",
      "mean reward (100 episodes) -214.701040\n",
      "best mean reward -inf\n",
      "running time 31.418510\n",
      "Train_EnvstepsSoFar : 12001\n",
      "Train_AverageReturn : -214.70104005380958\n",
      "TimeSinceStart : 31.41850972175598\n",
      "Training Loss : 0.2422022968530655\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 13000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 13001\n",
      "mean reward (100 episodes) -218.788557\n",
      "best mean reward -inf\n",
      "running time 34.233469\n",
      "Train_EnvstepsSoFar : 13001\n",
      "Train_AverageReturn : -218.788557212546\n",
      "TimeSinceStart : 34.233469009399414\n",
      "Training Loss : 3.062743902206421\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 14000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 14001\n",
      "mean reward (100 episodes) -217.728995\n",
      "best mean reward -inf\n",
      "running time 37.721302\n",
      "Train_EnvstepsSoFar : 14001\n",
      "Train_AverageReturn : -217.7289954784977\n",
      "TimeSinceStart : 37.721301794052124\n",
      "Training Loss : 2.7418625354766846\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 15000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 15001\n",
      "mean reward (100 episodes) -208.378243\n",
      "best mean reward -inf\n",
      "running time 40.521719\n",
      "Train_EnvstepsSoFar : 15001\n",
      "Train_AverageReturn : -208.37824341970943\n",
      "TimeSinceStart : 40.52171874046326\n",
      "Training Loss : 0.715223491191864\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 16000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 16001\n",
      "mean reward (100 episodes) -209.164021\n",
      "best mean reward -inf\n",
      "running time 43.577272\n",
      "Train_EnvstepsSoFar : 16001\n",
      "Train_AverageReturn : -209.1640211882104\n",
      "TimeSinceStart : 43.577271699905396\n",
      "Training Loss : 0.23142299056053162\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 17000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 17001\n",
      "mean reward (100 episodes) -208.203287\n",
      "best mean reward -inf\n",
      "running time 46.773580\n",
      "Train_EnvstepsSoFar : 17001\n",
      "Train_AverageReturn : -208.2032872151909\n",
      "TimeSinceStart : 46.773579835891724\n",
      "Training Loss : 0.5793458223342896\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 18000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 18001\n",
      "mean reward (100 episodes) -200.291413\n",
      "best mean reward -inf\n",
      "running time 49.879487\n",
      "Train_EnvstepsSoFar : 18001\n",
      "Train_AverageReturn : -200.29141338397469\n",
      "TimeSinceStart : 49.87948703765869\n",
      "Training Loss : 0.20295222103595734\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 19000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 19001\n",
      "mean reward (100 episodes) -197.447249\n",
      "best mean reward -inf\n",
      "running time 53.381960\n",
      "Train_EnvstepsSoFar : 19001\n",
      "Train_AverageReturn : -197.44724871503308\n",
      "TimeSinceStart : 53.38195991516113\n",
      "Training Loss : 0.4408423602581024\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 20001\n",
      "mean reward (100 episodes) -196.757233\n",
      "best mean reward -inf\n",
      "running time 57.136883\n",
      "Train_EnvstepsSoFar : 20001\n",
      "Train_AverageReturn : -196.7572331951212\n",
      "TimeSinceStart : 57.136883020401\n",
      "Training Loss : 1.3890149593353271\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 21000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 21001\n",
      "mean reward (100 episodes) -194.597419\n",
      "best mean reward -inf\n",
      "running time 61.086752\n",
      "Train_EnvstepsSoFar : 21001\n",
      "Train_AverageReturn : -194.59741905916422\n",
      "TimeSinceStart : 61.08675193786621\n",
      "Training Loss : 0.8927267789840698\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 22000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 22001\n",
      "mean reward (100 episodes) -192.203018\n",
      "best mean reward -inf\n",
      "running time 65.494201\n",
      "Train_EnvstepsSoFar : 22001\n",
      "Train_AverageReturn : -192.20301760376276\n",
      "TimeSinceStart : 65.49420070648193\n",
      "Training Loss : 0.7960103154182434\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 23000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 23001\n",
      "mean reward (100 episodes) -188.484189\n",
      "best mean reward -188.484189\n",
      "running time 69.450959\n",
      "Train_EnvstepsSoFar : 23001\n",
      "Train_AverageReturn : -188.48418883546717\n",
      "Train_BestReturn : -188.48418883546717\n",
      "TimeSinceStart : 69.45095872879028\n",
      "Training Loss : 1.3927065134048462\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 24000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 24001\n",
      "mean reward (100 episodes) -183.224770\n",
      "best mean reward -183.224770\n",
      "running time 74.110919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 24001\n",
      "Train_AverageReturn : -183.22477033921928\n",
      "Train_BestReturn : -183.22477033921928\n",
      "TimeSinceStart : 74.11091876029968\n",
      "Training Loss : 0.6776131987571716\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 25000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 25001\n",
      "mean reward (100 episodes) -181.724345\n",
      "best mean reward -181.724345\n",
      "running time 77.561211\n",
      "Train_EnvstepsSoFar : 25001\n",
      "Train_AverageReturn : -181.7243454470348\n",
      "Train_BestReturn : -181.7243454470348\n",
      "TimeSinceStart : 77.5612108707428\n",
      "Training Loss : 0.7930812835693359\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 26000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 26001\n",
      "mean reward (100 episodes) -180.220301\n",
      "best mean reward -180.220301\n",
      "running time 81.028885\n",
      "Train_EnvstepsSoFar : 26001\n",
      "Train_AverageReturn : -180.22030074503732\n",
      "Train_BestReturn : -180.22030074503732\n",
      "TimeSinceStart : 81.02888488769531\n",
      "Training Loss : 1.0074446201324463\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 27000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 27001\n",
      "mean reward (100 episodes) -172.799811\n",
      "best mean reward -172.799811\n",
      "running time 84.361809\n",
      "Train_EnvstepsSoFar : 27001\n",
      "Train_AverageReturn : -172.7998106903462\n",
      "Train_BestReturn : -172.7998106903462\n",
      "TimeSinceStart : 84.36180877685547\n",
      "Training Loss : 1.1883621215820312\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 28000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 28001\n",
      "mean reward (100 episodes) -171.949353\n",
      "best mean reward -171.949353\n",
      "running time 87.743570\n",
      "Train_EnvstepsSoFar : 28001\n",
      "Train_AverageReturn : -171.94935284247285\n",
      "Train_BestReturn : -171.94935284247285\n",
      "TimeSinceStart : 87.74356985092163\n",
      "Training Loss : 1.0587964057922363\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 29000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 29001\n",
      "mean reward (100 episodes) -169.530039\n",
      "best mean reward -169.530039\n",
      "running time 91.222980\n",
      "Train_EnvstepsSoFar : 29001\n",
      "Train_AverageReturn : -169.5300388967271\n",
      "Train_BestReturn : -169.5300388967271\n",
      "TimeSinceStart : 91.22297978401184\n",
      "Training Loss : 0.45702528953552246\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 30001\n",
      "mean reward (100 episodes) -168.619835\n",
      "best mean reward -168.619835\n",
      "running time 94.763288\n",
      "Train_EnvstepsSoFar : 30001\n",
      "Train_AverageReturn : -168.6198354401267\n",
      "Train_BestReturn : -168.6198354401267\n",
      "TimeSinceStart : 94.76328778266907\n",
      "Training Loss : 0.31595852971076965\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 31000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 31001\n",
      "mean reward (100 episodes) -165.685720\n",
      "best mean reward -165.685720\n",
      "running time 98.682649\n",
      "Train_EnvstepsSoFar : 31001\n",
      "Train_AverageReturn : -165.68572010466568\n",
      "Train_BestReturn : -165.68572010466568\n",
      "TimeSinceStart : 98.68264889717102\n",
      "Training Loss : 0.9570837616920471\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 32000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 32001\n",
      "mean reward (100 episodes) -165.315431\n",
      "best mean reward -165.315431\n",
      "running time 102.280874\n",
      "Train_EnvstepsSoFar : 32001\n",
      "Train_AverageReturn : -165.3154314842037\n",
      "Train_BestReturn : -165.3154314842037\n",
      "TimeSinceStart : 102.28087401390076\n",
      "Training Loss : 2.0245041847229004\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 33000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 33001\n",
      "mean reward (100 episodes) -164.240657\n",
      "best mean reward -164.240657\n",
      "running time 105.739541\n",
      "Train_EnvstepsSoFar : 33001\n",
      "Train_AverageReturn : -164.24065708076338\n",
      "Train_BestReturn : -164.24065708076338\n",
      "TimeSinceStart : 105.73954105377197\n",
      "Training Loss : 0.32102450728416443\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 34000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 34001\n",
      "mean reward (100 episodes) -164.264064\n",
      "best mean reward -164.240657\n",
      "running time 110.413539\n",
      "Train_EnvstepsSoFar : 34001\n",
      "Train_AverageReturn : -164.26406376049377\n",
      "Train_BestReturn : -164.24065708076338\n",
      "TimeSinceStart : 110.41353869438171\n",
      "Training Loss : 0.5301583409309387\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 35000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 35001\n",
      "mean reward (100 episodes) -164.178562\n",
      "best mean reward -164.178562\n",
      "running time 114.285803\n",
      "Train_EnvstepsSoFar : 35001\n",
      "Train_AverageReturn : -164.17856204822604\n",
      "Train_BestReturn : -164.17856204822604\n",
      "TimeSinceStart : 114.2858030796051\n",
      "Training Loss : 0.9432364702224731\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 36000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 36001\n",
      "mean reward (100 episodes) -158.625268\n",
      "best mean reward -158.625268\n",
      "running time 117.653659\n",
      "Train_EnvstepsSoFar : 36001\n",
      "Train_AverageReturn : -158.62526784972496\n",
      "Train_BestReturn : -158.62526784972496\n",
      "TimeSinceStart : 117.65365886688232\n",
      "Training Loss : 0.4941882789134979\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 37000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 37001\n",
      "mean reward (100 episodes) -155.852202\n",
      "best mean reward -155.852202\n",
      "running time 121.868989\n",
      "Train_EnvstepsSoFar : 37001\n",
      "Train_AverageReturn : -155.85220229027985\n",
      "Train_BestReturn : -155.85220229027985\n",
      "TimeSinceStart : 121.86898899078369\n",
      "Training Loss : 0.4283725619316101\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 38000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 38001\n",
      "mean reward (100 episodes) -153.032586\n",
      "best mean reward -153.032586\n",
      "running time 126.204064\n",
      "Train_EnvstepsSoFar : 38001\n",
      "Train_AverageReturn : -153.03258628558916\n",
      "Train_BestReturn : -153.03258628558916\n",
      "TimeSinceStart : 126.2040638923645\n",
      "Training Loss : 0.2980938255786896\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 39000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 39001\n",
      "mean reward (100 episodes) -150.228530\n",
      "best mean reward -150.228530\n",
      "running time 129.848715\n",
      "Train_EnvstepsSoFar : 39001\n",
      "Train_AverageReturn : -150.22852963803487\n",
      "Train_BestReturn : -150.22852963803487\n",
      "TimeSinceStart : 129.8487148284912\n",
      "Training Loss : 0.4314427971839905\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 40001\n",
      "mean reward (100 episodes) -149.929765\n",
      "best mean reward -149.929765\n",
      "running time 133.896753\n",
      "Train_EnvstepsSoFar : 40001\n",
      "Train_AverageReturn : -149.929764542672\n",
      "Train_BestReturn : -149.929764542672\n",
      "TimeSinceStart : 133.89675283432007\n",
      "Training Loss : 1.6143492460250854\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 41000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 41001\n",
      "mean reward (100 episodes) -149.608705\n",
      "best mean reward -149.608705\n",
      "running time 137.627058\n",
      "Train_EnvstepsSoFar : 41001\n",
      "Train_AverageReturn : -149.608705422384\n",
      "Train_BestReturn : -149.608705422384\n",
      "TimeSinceStart : 137.62705779075623\n",
      "Training Loss : 0.3883841931819916\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 42000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 42001\n",
      "mean reward (100 episodes) -149.464773\n",
      "best mean reward -149.464773\n",
      "running time 141.354634\n",
      "Train_EnvstepsSoFar : 42001\n",
      "Train_AverageReturn : -149.46477256544978\n",
      "Train_BestReturn : -149.46477256544978\n",
      "TimeSinceStart : 141.35463404655457\n",
      "Training Loss : 0.8649400472640991\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 43000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 43001\n",
      "mean reward (100 episodes) -151.981546\n",
      "best mean reward -149.464773\n",
      "running time 145.424877\n",
      "Train_EnvstepsSoFar : 43001\n",
      "Train_AverageReturn : -151.98154642896034\n",
      "Train_BestReturn : -149.46477256544978\n",
      "TimeSinceStart : 145.4248766899109\n",
      "Training Loss : 0.3451099395751953\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 44000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 44001\n",
      "mean reward (100 episodes) -150.611678\n",
      "best mean reward -149.464773\n",
      "running time 149.828991\n",
      "Train_EnvstepsSoFar : 44001\n",
      "Train_AverageReturn : -150.61167762337155\n",
      "Train_BestReturn : -149.46477256544978\n",
      "TimeSinceStart : 149.8289909362793\n",
      "Training Loss : 0.8856818079948425\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 45000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 45001\n",
      "mean reward (100 episodes) -149.898153\n",
      "best mean reward -149.464773\n",
      "running time 153.974644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 45001\n",
      "Train_AverageReturn : -149.8981531322144\n",
      "Train_BestReturn : -149.46477256544978\n",
      "TimeSinceStart : 153.97464394569397\n",
      "Training Loss : 0.2759976387023926\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 46000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 46001\n",
      "mean reward (100 episodes) -150.156117\n",
      "best mean reward -149.464773\n",
      "running time 157.800271\n",
      "Train_EnvstepsSoFar : 46001\n",
      "Train_AverageReturn : -150.15611669220007\n",
      "Train_BestReturn : -149.46477256544978\n",
      "TimeSinceStart : 157.80027079582214\n",
      "Training Loss : 4.0942888259887695\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 47000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 47001\n",
      "mean reward (100 episodes) -150.335903\n",
      "best mean reward -149.464773\n",
      "running time 162.017846\n",
      "Train_EnvstepsSoFar : 47001\n",
      "Train_AverageReturn : -150.33590301807683\n",
      "Train_BestReturn : -149.46477256544978\n",
      "TimeSinceStart : 162.01784563064575\n",
      "Training Loss : 0.8969570398330688\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 48000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 48001\n",
      "mean reward (100 episodes) -149.203573\n",
      "best mean reward -149.203573\n",
      "running time 166.186898\n",
      "Train_EnvstepsSoFar : 48001\n",
      "Train_AverageReturn : -149.20357270468358\n",
      "Train_BestReturn : -149.20357270468358\n",
      "TimeSinceStart : 166.18689799308777\n",
      "Training Loss : 1.197047233581543\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 49000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 49001\n",
      "mean reward (100 episodes) -149.146947\n",
      "best mean reward -149.146947\n",
      "running time 169.942510\n",
      "Train_EnvstepsSoFar : 49001\n",
      "Train_AverageReturn : -149.146946860069\n",
      "Train_BestReturn : -149.146946860069\n",
      "TimeSinceStart : 169.94250988960266\n",
      "Training Loss : 1.215440034866333\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 50001\n",
      "mean reward (100 episodes) -146.927007\n",
      "best mean reward -146.927007\n",
      "running time 173.570352\n",
      "Train_EnvstepsSoFar : 50001\n",
      "Train_AverageReturn : -146.92700707765158\n",
      "Train_BestReturn : -146.92700707765158\n",
      "TimeSinceStart : 173.57035183906555\n",
      "Training Loss : 0.8233020305633545\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 51000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 51001\n",
      "mean reward (100 episodes) -146.077214\n",
      "best mean reward -146.077214\n",
      "running time 177.721413\n",
      "Train_EnvstepsSoFar : 51001\n",
      "Train_AverageReturn : -146.07721419599198\n",
      "Train_BestReturn : -146.07721419599198\n",
      "TimeSinceStart : 177.72141289710999\n",
      "Training Loss : 3.684788227081299\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 52000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 52001\n",
      "mean reward (100 episodes) -143.216557\n",
      "best mean reward -143.216557\n",
      "running time 181.985195\n",
      "Train_EnvstepsSoFar : 52001\n",
      "Train_AverageReturn : -143.21655683572882\n",
      "Train_BestReturn : -143.21655683572882\n",
      "TimeSinceStart : 181.98519492149353\n",
      "Training Loss : 0.4115926921367645\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 53000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 53001\n",
      "mean reward (100 episodes) -140.911191\n",
      "best mean reward -140.911191\n",
      "running time 185.347344\n",
      "Train_EnvstepsSoFar : 53001\n",
      "Train_AverageReturn : -140.91119088451572\n",
      "Train_BestReturn : -140.91119088451572\n",
      "TimeSinceStart : 185.34734392166138\n",
      "Training Loss : 0.21273168921470642\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 54000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 54001\n",
      "mean reward (100 episodes) -138.528770\n",
      "best mean reward -138.528770\n",
      "running time 188.844153\n",
      "Train_EnvstepsSoFar : 54001\n",
      "Train_AverageReturn : -138.52877039291542\n",
      "Train_BestReturn : -138.52877039291542\n",
      "TimeSinceStart : 188.8441526889801\n",
      "Training Loss : 0.23293668031692505\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 55000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 55001\n",
      "mean reward (100 episodes) -139.621258\n",
      "best mean reward -138.528770\n",
      "running time 193.351405\n",
      "Train_EnvstepsSoFar : 55001\n",
      "Train_AverageReturn : -139.62125771764576\n",
      "Train_BestReturn : -138.52877039291542\n",
      "TimeSinceStart : 193.35140466690063\n",
      "Training Loss : 0.6940999031066895\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 56000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 56001\n",
      "mean reward (100 episodes) -136.573514\n",
      "best mean reward -136.573514\n",
      "running time 196.541514\n",
      "Train_EnvstepsSoFar : 56001\n",
      "Train_AverageReturn : -136.57351407049984\n",
      "Train_BestReturn : -136.57351407049984\n",
      "TimeSinceStart : 196.54151391983032\n",
      "Training Loss : 1.187416434288025\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 57000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 57001\n",
      "mean reward (100 episodes) -132.239277\n",
      "best mean reward -132.239277\n",
      "running time 199.726839\n",
      "Train_EnvstepsSoFar : 57001\n",
      "Train_AverageReturn : -132.23927681755052\n",
      "Train_BestReturn : -132.23927681755052\n",
      "TimeSinceStart : 199.72683882713318\n",
      "Training Loss : 0.26135382056236267\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 58000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 58001\n",
      "mean reward (100 episodes) -133.740496\n",
      "best mean reward -132.239277\n",
      "running time 203.318322\n",
      "Train_EnvstepsSoFar : 58001\n",
      "Train_AverageReturn : -133.74049636297008\n",
      "Train_BestReturn : -132.23927681755052\n",
      "TimeSinceStart : 203.31832194328308\n",
      "Training Loss : 0.1696711927652359\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 59000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 59001\n",
      "mean reward (100 episodes) -128.442861\n",
      "best mean reward -128.442861\n",
      "running time 206.744098\n",
      "Train_EnvstepsSoFar : 59001\n",
      "Train_AverageReturn : -128.44286113063114\n",
      "Train_BestReturn : -128.44286113063114\n",
      "TimeSinceStart : 206.74409770965576\n",
      "Training Loss : 0.8349955677986145\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 60001\n",
      "mean reward (100 episodes) -134.384808\n",
      "best mean reward -128.442861\n",
      "running time 210.601446\n",
      "Train_EnvstepsSoFar : 60001\n",
      "Train_AverageReturn : -134.38480811835984\n",
      "Train_BestReturn : -128.44286113063114\n",
      "TimeSinceStart : 210.60144591331482\n",
      "Training Loss : 0.457072377204895\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 61000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 61001\n",
      "mean reward (100 episodes) -133.501404\n",
      "best mean reward -128.442861\n",
      "running time 214.426847\n",
      "Train_EnvstepsSoFar : 61001\n",
      "Train_AverageReturn : -133.5014041175802\n",
      "Train_BestReturn : -128.44286113063114\n",
      "TimeSinceStart : 214.42684698104858\n",
      "Training Loss : 0.3866829574108124\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 62000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 62001\n",
      "mean reward (100 episodes) -133.322723\n",
      "best mean reward -128.442861\n",
      "running time 217.934589\n",
      "Train_EnvstepsSoFar : 62001\n",
      "Train_AverageReturn : -133.32272341731513\n",
      "Train_BestReturn : -128.44286113063114\n",
      "TimeSinceStart : 217.93458890914917\n",
      "Training Loss : 0.405256450176239\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 63000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 63001\n",
      "mean reward (100 episodes) -129.185596\n",
      "best mean reward -128.442861\n",
      "running time 221.180422\n",
      "Train_EnvstepsSoFar : 63001\n",
      "Train_AverageReturn : -129.18559619934834\n",
      "Train_BestReturn : -128.44286113063114\n",
      "TimeSinceStart : 221.18042182922363\n",
      "Training Loss : 0.9275678396224976\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 64000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 64001\n",
      "mean reward (100 episodes) -129.393996\n",
      "best mean reward -128.442861\n",
      "running time 224.893143\n",
      "Train_EnvstepsSoFar : 64001\n",
      "Train_AverageReturn : -129.39399585232553\n",
      "Train_BestReturn : -128.44286113063114\n",
      "TimeSinceStart : 224.8931427001953\n",
      "Training Loss : 0.40020009875297546\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 65000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 65001\n",
      "mean reward (100 episodes) -128.082356\n",
      "best mean reward -128.082356\n",
      "running time 229.008549\n",
      "Train_EnvstepsSoFar : 65001\n",
      "Train_AverageReturn : -128.08235591943617\n",
      "Train_BestReturn : -128.08235591943617\n",
      "TimeSinceStart : 229.00854873657227\n",
      "Training Loss : 0.4634207487106323\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 66000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 66001\n",
      "mean reward (100 episodes) -125.457510\n",
      "best mean reward -125.457510\n",
      "running time 232.915916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 66001\n",
      "Train_AverageReturn : -125.45751014808347\n",
      "Train_BestReturn : -125.45751014808347\n",
      "TimeSinceStart : 232.91591572761536\n",
      "Training Loss : 0.572282075881958\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 67000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 67001\n",
      "mean reward (100 episodes) -126.984340\n",
      "best mean reward -125.457510\n",
      "running time 236.038064\n",
      "Train_EnvstepsSoFar : 67001\n",
      "Train_AverageReturn : -126.9843401307386\n",
      "Train_BestReturn : -125.45751014808347\n",
      "TimeSinceStart : 236.03806376457214\n",
      "Training Loss : 0.3295862674713135\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 68000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 68001\n",
      "mean reward (100 episodes) -127.220023\n",
      "best mean reward -125.457510\n",
      "running time 239.619195\n",
      "Train_EnvstepsSoFar : 68001\n",
      "Train_AverageReturn : -127.2200230149203\n",
      "Train_BestReturn : -125.45751014808347\n",
      "TimeSinceStart : 239.61919474601746\n",
      "Training Loss : 0.2092868685722351\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 69000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 69001\n",
      "mean reward (100 episodes) -125.301007\n",
      "best mean reward -125.301007\n",
      "running time 243.166007\n",
      "Train_EnvstepsSoFar : 69001\n",
      "Train_AverageReturn : -125.30100670412936\n",
      "Train_BestReturn : -125.30100670412936\n",
      "TimeSinceStart : 243.16600680351257\n",
      "Training Loss : 0.22702006995677948\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 70001\n",
      "mean reward (100 episodes) -119.726554\n",
      "best mean reward -119.726554\n",
      "running time 246.178144\n",
      "Train_EnvstepsSoFar : 70001\n",
      "Train_AverageReturn : -119.72655415631367\n",
      "Train_BestReturn : -119.72655415631367\n",
      "TimeSinceStart : 246.1781439781189\n",
      "Training Loss : 0.4326617121696472\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 71000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 71001\n",
      "mean reward (100 episodes) -119.814703\n",
      "best mean reward -119.726554\n",
      "running time 249.549926\n",
      "Train_EnvstepsSoFar : 71001\n",
      "Train_AverageReturn : -119.81470325954616\n",
      "Train_BestReturn : -119.72655415631367\n",
      "TimeSinceStart : 249.54992580413818\n",
      "Training Loss : 0.23917527496814728\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 72000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 72001\n",
      "mean reward (100 episodes) -117.668192\n",
      "best mean reward -117.668192\n",
      "running time 253.667448\n",
      "Train_EnvstepsSoFar : 72001\n",
      "Train_AverageReturn : -117.66819190823877\n",
      "Train_BestReturn : -117.66819190823877\n",
      "TimeSinceStart : 253.66744780540466\n",
      "Training Loss : 0.7979163527488708\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 73000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 73001\n",
      "mean reward (100 episodes) -117.566879\n",
      "best mean reward -117.566879\n",
      "running time 257.694058\n",
      "Train_EnvstepsSoFar : 73001\n",
      "Train_AverageReturn : -117.56687866781215\n",
      "Train_BestReturn : -117.56687866781215\n",
      "TimeSinceStart : 257.69405794143677\n",
      "Training Loss : 0.5192299485206604\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 74000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 74001\n",
      "mean reward (100 episodes) -116.831903\n",
      "best mean reward -116.831903\n",
      "running time 262.124382\n",
      "Train_EnvstepsSoFar : 74001\n",
      "Train_AverageReturn : -116.8319026865968\n",
      "Train_BestReturn : -116.8319026865968\n",
      "TimeSinceStart : 262.12438201904297\n",
      "Training Loss : 0.3242945969104767\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 75000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 75001\n",
      "mean reward (100 episodes) -119.097754\n",
      "best mean reward -116.831903\n",
      "running time 266.139666\n",
      "Train_EnvstepsSoFar : 75001\n",
      "Train_AverageReturn : -119.097754390697\n",
      "Train_BestReturn : -116.8319026865968\n",
      "TimeSinceStart : 266.1396658420563\n",
      "Training Loss : 0.31578442454338074\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 76000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 76001\n",
      "mean reward (100 episodes) -119.098796\n",
      "best mean reward -116.831903\n",
      "running time 269.489989\n",
      "Train_EnvstepsSoFar : 76001\n",
      "Train_AverageReturn : -119.09879587114699\n",
      "Train_BestReturn : -116.8319026865968\n",
      "TimeSinceStart : 269.4899888038635\n",
      "Training Loss : 0.2959435284137726\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 77000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 77001\n",
      "mean reward (100 episodes) -115.661639\n",
      "best mean reward -115.661639\n",
      "running time 273.167281\n",
      "Train_EnvstepsSoFar : 77001\n",
      "Train_AverageReturn : -115.6616391684467\n",
      "Train_BestReturn : -115.6616391684467\n",
      "TimeSinceStart : 273.1672809123993\n",
      "Training Loss : 0.47273555397987366\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 78000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 78001\n",
      "mean reward (100 episodes) -114.199907\n",
      "best mean reward -114.199907\n",
      "running time 277.684194\n",
      "Train_EnvstepsSoFar : 78001\n",
      "Train_AverageReturn : -114.1999071457438\n",
      "Train_BestReturn : -114.1999071457438\n",
      "TimeSinceStart : 277.6841938495636\n",
      "Training Loss : 0.3442176878452301\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 79000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 79001\n",
      "mean reward (100 episodes) -114.328646\n",
      "best mean reward -114.199907\n",
      "running time 281.620947\n",
      "Train_EnvstepsSoFar : 79001\n",
      "Train_AverageReturn : -114.32864631454584\n",
      "Train_BestReturn : -114.1999071457438\n",
      "TimeSinceStart : 281.6209468841553\n",
      "Training Loss : 0.3711181581020355\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 80001\n",
      "mean reward (100 episodes) -111.105394\n",
      "best mean reward -111.105394\n",
      "running time 286.020368\n",
      "Train_EnvstepsSoFar : 80001\n",
      "Train_AverageReturn : -111.10539439298338\n",
      "Train_BestReturn : -111.10539439298338\n",
      "TimeSinceStart : 286.02036786079407\n",
      "Training Loss : 1.3333361148834229\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 81000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 81001\n",
      "mean reward (100 episodes) -110.643310\n",
      "best mean reward -110.643310\n",
      "running time 289.336865\n",
      "Train_EnvstepsSoFar : 81001\n",
      "Train_AverageReturn : -110.64331035405245\n",
      "Train_BestReturn : -110.64331035405245\n",
      "TimeSinceStart : 289.3368647098541\n",
      "Training Loss : 0.1683969646692276\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 82000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 82001\n",
      "mean reward (100 episodes) -104.320817\n",
      "best mean reward -104.320817\n",
      "running time 292.953031\n",
      "Train_EnvstepsSoFar : 82001\n",
      "Train_AverageReturn : -104.32081705533439\n",
      "Train_BestReturn : -104.32081705533439\n",
      "TimeSinceStart : 292.95303082466125\n",
      "Training Loss : 0.2088221311569214\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 83000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 83001\n",
      "mean reward (100 episodes) -102.623311\n",
      "best mean reward -102.623311\n",
      "running time 296.606416\n",
      "Train_EnvstepsSoFar : 83001\n",
      "Train_AverageReturn : -102.6233106646128\n",
      "Train_BestReturn : -102.6233106646128\n",
      "TimeSinceStart : 296.6064157485962\n",
      "Training Loss : 0.33028167486190796\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 84000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 84001\n",
      "mean reward (100 episodes) -101.312360\n",
      "best mean reward -101.312360\n",
      "running time 300.397175\n",
      "Train_EnvstepsSoFar : 84001\n",
      "Train_AverageReturn : -101.31235981569705\n",
      "Train_BestReturn : -101.31235981569705\n",
      "TimeSinceStart : 300.3971748352051\n",
      "Training Loss : 0.19271039962768555\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 85000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 85001\n",
      "mean reward (100 episodes) -103.195916\n",
      "best mean reward -101.312360\n",
      "running time 303.871013\n",
      "Train_EnvstepsSoFar : 85001\n",
      "Train_AverageReturn : -103.19591600050126\n",
      "Train_BestReturn : -101.31235981569705\n",
      "TimeSinceStart : 303.8710129261017\n",
      "Training Loss : 0.15431062877178192\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 86000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 86001\n",
      "mean reward (100 episodes) -101.465554\n",
      "best mean reward -101.312360\n",
      "running time 306.959235\n",
      "Train_EnvstepsSoFar : 86001\n",
      "Train_AverageReturn : -101.4655544616377\n",
      "Train_BestReturn : -101.31235981569705\n",
      "TimeSinceStart : 306.95923471450806\n",
      "Training Loss : 0.1456744372844696\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 87000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 87001\n",
      "mean reward (100 episodes) -102.074911\n",
      "best mean reward -101.312360\n",
      "running time 310.579021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 87001\n",
      "Train_AverageReturn : -102.07491148447065\n",
      "Train_BestReturn : -101.31235981569705\n",
      "TimeSinceStart : 310.5790207386017\n",
      "Training Loss : 0.19763830304145813\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 88000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 88001\n",
      "mean reward (100 episodes) -102.534383\n",
      "best mean reward -101.312360\n",
      "running time 314.243753\n",
      "Train_EnvstepsSoFar : 88001\n",
      "Train_AverageReturn : -102.53438279414128\n",
      "Train_BestReturn : -101.31235981569705\n",
      "TimeSinceStart : 314.2437529563904\n",
      "Training Loss : 0.2599029839038849\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 89000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 89001\n",
      "mean reward (100 episodes) -99.595139\n",
      "best mean reward -99.595139\n",
      "running time 317.293139\n",
      "Train_EnvstepsSoFar : 89001\n",
      "Train_AverageReturn : -99.59513888166853\n",
      "Train_BestReturn : -99.59513888166853\n",
      "TimeSinceStart : 317.2931389808655\n",
      "Training Loss : 0.40164095163345337\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 90001\n",
      "mean reward (100 episodes) -101.869341\n",
      "best mean reward -99.595139\n",
      "running time 321.205173\n",
      "Train_EnvstepsSoFar : 90001\n",
      "Train_AverageReturn : -101.86934119312855\n",
      "Train_BestReturn : -99.59513888166853\n",
      "TimeSinceStart : 321.2051730155945\n",
      "Training Loss : 0.17977100610733032\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 91000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 91001\n",
      "mean reward (100 episodes) -104.275660\n",
      "best mean reward -99.595139\n",
      "running time 325.112660\n",
      "Train_EnvstepsSoFar : 91001\n",
      "Train_AverageReturn : -104.27566002817322\n",
      "Train_BestReturn : -99.59513888166853\n",
      "TimeSinceStart : 325.11265993118286\n",
      "Training Loss : 0.22407561540603638\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 92000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 92001\n",
      "mean reward (100 episodes) -104.221480\n",
      "best mean reward -99.595139\n",
      "running time 328.944990\n",
      "Train_EnvstepsSoFar : 92001\n",
      "Train_AverageReturn : -104.22147965769567\n",
      "Train_BestReturn : -99.59513888166853\n",
      "TimeSinceStart : 328.9449899196625\n",
      "Training Loss : 0.21688014268875122\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 93000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 93001\n",
      "mean reward (100 episodes) -106.207023\n",
      "best mean reward -99.595139\n",
      "running time 332.640055\n",
      "Train_EnvstepsSoFar : 93001\n",
      "Train_AverageReturn : -106.2070232787637\n",
      "Train_BestReturn : -99.59513888166853\n",
      "TimeSinceStart : 332.6400547027588\n",
      "Training Loss : 0.16324849426746368\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 94000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 94001\n",
      "mean reward (100 episodes) -106.092272\n",
      "best mean reward -99.595139\n",
      "running time 337.995861\n",
      "Train_EnvstepsSoFar : 94001\n",
      "Train_AverageReturn : -106.09227169983913\n",
      "Train_BestReturn : -99.59513888166853\n",
      "TimeSinceStart : 337.9958610534668\n",
      "Training Loss : 0.1782355159521103\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 95000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 95001\n",
      "mean reward (100 episodes) -103.726338\n",
      "best mean reward -99.595139\n",
      "running time 341.732545\n",
      "Train_EnvstepsSoFar : 95001\n",
      "Train_AverageReturn : -103.72633839979261\n",
      "Train_BestReturn : -99.59513888166853\n",
      "TimeSinceStart : 341.73254466056824\n",
      "Training Loss : 0.2662947177886963\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 96000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 96001\n",
      "mean reward (100 episodes) -104.594620\n",
      "best mean reward -99.595139\n",
      "running time 346.908992\n",
      "Train_EnvstepsSoFar : 96001\n",
      "Train_AverageReturn : -104.59462045882238\n",
      "Train_BestReturn : -99.59513888166853\n",
      "TimeSinceStart : 346.90899205207825\n",
      "Training Loss : 0.20948243141174316\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 97000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 97001\n",
      "mean reward (100 episodes) -106.565536\n",
      "best mean reward -99.595139\n",
      "running time 350.865729\n",
      "Train_EnvstepsSoFar : 97001\n",
      "Train_AverageReturn : -106.56553587100136\n",
      "Train_BestReturn : -99.59513888166853\n",
      "TimeSinceStart : 350.8657286167145\n",
      "Training Loss : 0.14980311691761017\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 98000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 98001\n",
      "mean reward (100 episodes) -106.932535\n",
      "best mean reward -99.595139\n",
      "running time 354.871009\n",
      "Train_EnvstepsSoFar : 98001\n",
      "Train_AverageReturn : -106.93253510864528\n",
      "Train_BestReturn : -99.59513888166853\n",
      "TimeSinceStart : 354.87100863456726\n",
      "Training Loss : 0.17333561182022095\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 99000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 99001\n",
      "mean reward (100 episodes) -106.926273\n",
      "best mean reward -99.595139\n",
      "running time 359.192426\n",
      "Train_EnvstepsSoFar : 99001\n",
      "Train_AverageReturn : -106.92627341236819\n",
      "Train_BestReturn : -99.59513888166853\n",
      "TimeSinceStart : 359.1924259662628\n",
      "Training Loss : 0.449126660823822\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 100001\n",
      "mean reward (100 episodes) -106.752936\n",
      "best mean reward -99.595139\n",
      "running time 362.889578\n",
      "Train_EnvstepsSoFar : 100001\n",
      "Train_AverageReturn : -106.75293603911365\n",
      "Train_BestReturn : -99.59513888166853\n",
      "TimeSinceStart : 362.8895778656006\n",
      "Training Loss : 0.480973482131958\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 101001\n",
      "mean reward (100 episodes) -106.659735\n",
      "best mean reward -99.595139\n",
      "running time 366.956966\n",
      "Train_EnvstepsSoFar : 101001\n",
      "Train_AverageReturn : -106.65973524030969\n",
      "Train_BestReturn : -99.59513888166853\n",
      "TimeSinceStart : 366.9569659233093\n",
      "Training Loss : 0.48589447140693665\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 102001\n",
      "mean reward (100 episodes) -105.891392\n",
      "best mean reward -99.595139\n",
      "running time 370.836351\n",
      "Train_EnvstepsSoFar : 102001\n",
      "Train_AverageReturn : -105.89139203848929\n",
      "Train_BestReturn : -99.59513888166853\n",
      "TimeSinceStart : 370.83635091781616\n",
      "Training Loss : 0.3121512234210968\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 103001\n",
      "mean reward (100 episodes) -105.924833\n",
      "best mean reward -99.595139\n",
      "running time 374.943202\n",
      "Train_EnvstepsSoFar : 103001\n",
      "Train_AverageReturn : -105.92483250241936\n",
      "Train_BestReturn : -99.59513888166853\n",
      "TimeSinceStart : 374.9432017803192\n",
      "Training Loss : 0.18382923305034637\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 104001\n",
      "mean reward (100 episodes) -105.904780\n",
      "best mean reward -99.595139\n",
      "running time 378.382725\n",
      "Train_EnvstepsSoFar : 104001\n",
      "Train_AverageReturn : -105.90478023348079\n",
      "Train_BestReturn : -99.59513888166853\n",
      "TimeSinceStart : 378.3827247619629\n",
      "Training Loss : 0.39996984601020813\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 105001\n",
      "mean reward (100 episodes) -104.302862\n",
      "best mean reward -99.595139\n",
      "running time 382.186147\n",
      "Train_EnvstepsSoFar : 105001\n",
      "Train_AverageReturn : -104.30286242216295\n",
      "Train_BestReturn : -99.59513888166853\n",
      "TimeSinceStart : 382.186146736145\n",
      "Training Loss : 0.3912542760372162\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 106001\n",
      "mean reward (100 episodes) -104.498558\n",
      "best mean reward -99.595139\n",
      "running time 386.374590\n",
      "Train_EnvstepsSoFar : 106001\n",
      "Train_AverageReturn : -104.49855833414009\n",
      "Train_BestReturn : -99.59513888166853\n",
      "TimeSinceStart : 386.37458992004395\n",
      "Training Loss : 0.24791181087493896\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 107001\n",
      "mean reward (100 episodes) -104.246701\n",
      "best mean reward -99.595139\n",
      "running time 390.181799\n",
      "Train_EnvstepsSoFar : 107001\n",
      "Train_AverageReturn : -104.24670074281687\n",
      "Train_BestReturn : -99.59513888166853\n",
      "TimeSinceStart : 390.1817989349365\n",
      "Training Loss : 0.11889782547950745\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 108001\n",
      "mean reward (100 episodes) -104.008984\n",
      "best mean reward -99.595139\n",
      "running time 393.953054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 108001\n",
      "Train_AverageReturn : -104.00898428013339\n",
      "Train_BestReturn : -99.59513888166853\n",
      "TimeSinceStart : 393.9530539512634\n",
      "Training Loss : 0.20738233625888824\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 109001\n",
      "mean reward (100 episodes) -99.837549\n",
      "best mean reward -99.595139\n",
      "running time 397.447776\n",
      "Train_EnvstepsSoFar : 109001\n",
      "Train_AverageReturn : -99.83754908079607\n",
      "Train_BestReturn : -99.59513888166853\n",
      "TimeSinceStart : 397.44777607917786\n",
      "Training Loss : 0.24212327599525452\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 110001\n",
      "mean reward (100 episodes) -99.677394\n",
      "best mean reward -99.595139\n",
      "running time 401.700662\n",
      "Train_EnvstepsSoFar : 110001\n",
      "Train_AverageReturn : -99.67739447152532\n",
      "Train_BestReturn : -99.59513888166853\n",
      "TimeSinceStart : 401.7006616592407\n",
      "Training Loss : 0.548340916633606\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 111001\n",
      "mean reward (100 episodes) -99.976960\n",
      "best mean reward -99.595139\n",
      "running time 405.931029\n",
      "Train_EnvstepsSoFar : 111001\n",
      "Train_AverageReturn : -99.97695962209626\n",
      "Train_BestReturn : -99.59513888166853\n",
      "TimeSinceStart : 405.931028842926\n",
      "Training Loss : 0.4445313811302185\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 112001\n",
      "mean reward (100 episodes) -99.837107\n",
      "best mean reward -99.595139\n",
      "running time 409.913848\n",
      "Train_EnvstepsSoFar : 112001\n",
      "Train_AverageReturn : -99.83710682641095\n",
      "Train_BestReturn : -99.59513888166853\n",
      "TimeSinceStart : 409.91384768486023\n",
      "Training Loss : 0.11746875196695328\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 113001\n",
      "mean reward (100 episodes) -95.961762\n",
      "best mean reward -95.961762\n",
      "running time 413.667576\n",
      "Train_EnvstepsSoFar : 113001\n",
      "Train_AverageReturn : -95.96176201022828\n",
      "Train_BestReturn : -95.96176201022828\n",
      "TimeSinceStart : 413.66757583618164\n",
      "Training Loss : 0.19030341506004333\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 114001\n",
      "mean reward (100 episodes) -91.745412\n",
      "best mean reward -91.745412\n",
      "running time 417.988735\n",
      "Train_EnvstepsSoFar : 114001\n",
      "Train_AverageReturn : -91.7454116373981\n",
      "Train_BestReturn : -91.7454116373981\n",
      "TimeSinceStart : 417.98873496055603\n",
      "Training Loss : 0.23501484096050262\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 115001\n",
      "mean reward (100 episodes) -91.541385\n",
      "best mean reward -91.541385\n",
      "running time 422.410717\n",
      "Train_EnvstepsSoFar : 115001\n",
      "Train_AverageReturn : -91.5413850411573\n",
      "Train_BestReturn : -91.5413850411573\n",
      "TimeSinceStart : 422.41071701049805\n",
      "Training Loss : 3.2642595767974854\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 116001\n",
      "mean reward (100 episodes) -90.776998\n",
      "best mean reward -90.776998\n",
      "running time 426.560463\n",
      "Train_EnvstepsSoFar : 116001\n",
      "Train_AverageReturn : -90.77699838161972\n",
      "Train_BestReturn : -90.77699838161972\n",
      "TimeSinceStart : 426.5604627132416\n",
      "Training Loss : 0.15725518763065338\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 117001\n",
      "mean reward (100 episodes) -89.320463\n",
      "best mean reward -89.320463\n",
      "running time 430.171986\n",
      "Train_EnvstepsSoFar : 117001\n",
      "Train_AverageReturn : -89.32046263800865\n",
      "Train_BestReturn : -89.32046263800865\n",
      "TimeSinceStart : 430.1719858646393\n",
      "Training Loss : 0.23824164271354675\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 118001\n",
      "mean reward (100 episodes) -88.225960\n",
      "best mean reward -88.225960\n",
      "running time 434.209756\n",
      "Train_EnvstepsSoFar : 118001\n",
      "Train_AverageReturn : -88.22595991127237\n",
      "Train_BestReturn : -88.22595991127237\n",
      "TimeSinceStart : 434.209755897522\n",
      "Training Loss : 0.10648994892835617\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 119001\n",
      "mean reward (100 episodes) -87.542069\n",
      "best mean reward -87.542069\n",
      "running time 438.963368\n",
      "Train_EnvstepsSoFar : 119001\n",
      "Train_AverageReturn : -87.54206878891499\n",
      "Train_BestReturn : -87.54206878891499\n",
      "TimeSinceStart : 438.96336793899536\n",
      "Training Loss : 0.10216641426086426\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 120001\n",
      "mean reward (100 episodes) -86.865799\n",
      "best mean reward -86.865799\n",
      "running time 443.003057\n",
      "Train_EnvstepsSoFar : 120001\n",
      "Train_AverageReturn : -86.86579858587352\n",
      "Train_BestReturn : -86.86579858587352\n",
      "TimeSinceStart : 443.00305676460266\n",
      "Training Loss : 0.09865912050008774\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 121001\n",
      "mean reward (100 episodes) -86.783469\n",
      "best mean reward -86.783469\n",
      "running time 446.764201\n",
      "Train_EnvstepsSoFar : 121001\n",
      "Train_AverageReturn : -86.78346898311261\n",
      "Train_BestReturn : -86.78346898311261\n",
      "TimeSinceStart : 446.764200925827\n",
      "Training Loss : 0.3970148265361786\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 122001\n",
      "mean reward (100 episodes) -84.774824\n",
      "best mean reward -84.774824\n",
      "running time 450.886087\n",
      "Train_EnvstepsSoFar : 122001\n",
      "Train_AverageReturn : -84.77482435051584\n",
      "Train_BestReturn : -84.77482435051584\n",
      "TimeSinceStart : 450.8860869407654\n",
      "Training Loss : 0.09523975849151611\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 123001\n",
      "mean reward (100 episodes) -84.052144\n",
      "best mean reward -84.052144\n",
      "running time 454.222457\n",
      "Train_EnvstepsSoFar : 123001\n",
      "Train_AverageReturn : -84.05214374439501\n",
      "Train_BestReturn : -84.05214374439501\n",
      "TimeSinceStart : 454.2224566936493\n",
      "Training Loss : 0.1731022745370865\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 124001\n",
      "mean reward (100 episodes) -76.597774\n",
      "best mean reward -76.597774\n",
      "running time 457.436762\n",
      "Train_EnvstepsSoFar : 124001\n",
      "Train_AverageReturn : -76.59777435468835\n",
      "Train_BestReturn : -76.59777435468835\n",
      "TimeSinceStart : 457.4367618560791\n",
      "Training Loss : 0.2939488887786865\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 125001\n",
      "mean reward (100 episodes) -76.120809\n",
      "best mean reward -76.120809\n",
      "running time 461.657263\n",
      "Train_EnvstepsSoFar : 125001\n",
      "Train_AverageReturn : -76.12080864119685\n",
      "Train_BestReturn : -76.12080864119685\n",
      "TimeSinceStart : 461.657262802124\n",
      "Training Loss : 0.43908289074897766\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 126001\n",
      "mean reward (100 episodes) -75.462718\n",
      "best mean reward -75.462718\n",
      "running time 465.165873\n",
      "Train_EnvstepsSoFar : 126001\n",
      "Train_AverageReturn : -75.46271784465766\n",
      "Train_BestReturn : -75.46271784465766\n",
      "TimeSinceStart : 465.1658728122711\n",
      "Training Loss : 0.1577005833387375\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 127001\n",
      "mean reward (100 episodes) -74.588473\n",
      "best mean reward -74.588473\n",
      "running time 468.765762\n",
      "Train_EnvstepsSoFar : 127001\n",
      "Train_AverageReturn : -74.5884726544634\n",
      "Train_BestReturn : -74.5884726544634\n",
      "TimeSinceStart : 468.7657618522644\n",
      "Training Loss : 0.09044665843248367\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 128001\n",
      "mean reward (100 episodes) -72.839692\n",
      "best mean reward -72.839692\n",
      "running time 472.157594\n",
      "Train_EnvstepsSoFar : 128001\n",
      "Train_AverageReturn : -72.83969152944765\n",
      "Train_BestReturn : -72.83969152944765\n",
      "TimeSinceStart : 472.1575937271118\n",
      "Training Loss : 0.28790661692619324\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 129001\n",
      "mean reward (100 episodes) -70.827382\n",
      "best mean reward -70.827382\n",
      "running time 476.676787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 129001\n",
      "Train_AverageReturn : -70.82738196736045\n",
      "Train_BestReturn : -70.82738196736045\n",
      "TimeSinceStart : 476.67678689956665\n",
      "Training Loss : 0.12682610750198364\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 130001\n",
      "mean reward (100 episodes) -65.028424\n",
      "best mean reward -65.028424\n",
      "running time 480.154533\n",
      "Train_EnvstepsSoFar : 130001\n",
      "Train_AverageReturn : -65.02842392143246\n",
      "Train_BestReturn : -65.02842392143246\n",
      "TimeSinceStart : 480.1545329093933\n",
      "Training Loss : 0.2731015086174011\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 131001\n",
      "mean reward (100 episodes) -64.372451\n",
      "best mean reward -64.372451\n",
      "running time 483.653093\n",
      "Train_EnvstepsSoFar : 131001\n",
      "Train_AverageReturn : -64.3724511673571\n",
      "Train_BestReturn : -64.3724511673571\n",
      "TimeSinceStart : 483.65309286117554\n",
      "Training Loss : 0.1849430501461029\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 132001\n",
      "mean reward (100 episodes) -54.268453\n",
      "best mean reward -54.268453\n",
      "running time 487.612064\n",
      "Train_EnvstepsSoFar : 132001\n",
      "Train_AverageReturn : -54.268452843699286\n",
      "Train_BestReturn : -54.268452843699286\n",
      "TimeSinceStart : 487.6120638847351\n",
      "Training Loss : 0.20664185285568237\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 133001\n",
      "mean reward (100 episodes) -50.679184\n",
      "best mean reward -50.679184\n",
      "running time 491.024157\n",
      "Train_EnvstepsSoFar : 133001\n",
      "Train_AverageReturn : -50.67918389906139\n",
      "Train_BestReturn : -50.67918389906139\n",
      "TimeSinceStart : 491.02415680885315\n",
      "Training Loss : 0.15575888752937317\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 134001\n",
      "mean reward (100 episodes) -49.808214\n",
      "best mean reward -49.808214\n",
      "running time 494.561126\n",
      "Train_EnvstepsSoFar : 134001\n",
      "Train_AverageReturn : -49.80821357636666\n",
      "Train_BestReturn : -49.80821357636666\n",
      "TimeSinceStart : 494.56112575531006\n",
      "Training Loss : 0.09580056369304657\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 135001\n",
      "mean reward (100 episodes) -47.694395\n",
      "best mean reward -47.694395\n",
      "running time 497.685810\n",
      "Train_EnvstepsSoFar : 135001\n",
      "Train_AverageReturn : -47.694394877287344\n",
      "Train_BestReturn : -47.694394877287344\n",
      "TimeSinceStart : 497.68580985069275\n",
      "Training Loss : 0.12852787971496582\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 136001\n",
      "mean reward (100 episodes) -46.474358\n",
      "best mean reward -46.474358\n",
      "running time 501.556191\n",
      "Train_EnvstepsSoFar : 136001\n",
      "Train_AverageReturn : -46.47435844299943\n",
      "Train_BestReturn : -46.47435844299943\n",
      "TimeSinceStart : 501.5561909675598\n",
      "Training Loss : 0.7618352174758911\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 137001\n",
      "mean reward (100 episodes) -46.221149\n",
      "best mean reward -46.221149\n",
      "running time 505.896157\n",
      "Train_EnvstepsSoFar : 137001\n",
      "Train_AverageReturn : -46.22114927123406\n",
      "Train_BestReturn : -46.22114927123406\n",
      "TimeSinceStart : 505.8961567878723\n",
      "Training Loss : 0.10369079560041428\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 138001\n",
      "mean reward (100 episodes) -43.623009\n",
      "best mean reward -43.623009\n",
      "running time 509.565854\n",
      "Train_EnvstepsSoFar : 138001\n",
      "Train_AverageReturn : -43.62300923487712\n",
      "Train_BestReturn : -43.62300923487712\n",
      "TimeSinceStart : 509.5658538341522\n",
      "Training Loss : 0.06693954020738602\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 139001\n",
      "mean reward (100 episodes) -41.252300\n",
      "best mean reward -41.252300\n",
      "running time 513.117157\n",
      "Train_EnvstepsSoFar : 139001\n",
      "Train_AverageReturn : -41.25230015597815\n",
      "Train_BestReturn : -41.25230015597815\n",
      "TimeSinceStart : 513.1171567440033\n",
      "Training Loss : 0.04925307631492615\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 140001\n",
      "mean reward (100 episodes) -39.721642\n",
      "best mean reward -39.721642\n",
      "running time 516.775640\n",
      "Train_EnvstepsSoFar : 140001\n",
      "Train_AverageReturn : -39.721641752015614\n",
      "Train_BestReturn : -39.721641752015614\n",
      "TimeSinceStart : 516.7756397724152\n",
      "Training Loss : 0.15867823362350464\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 141001\n",
      "mean reward (100 episodes) -37.360555\n",
      "best mean reward -37.360555\n",
      "running time 520.964625\n",
      "Train_EnvstepsSoFar : 141001\n",
      "Train_AverageReturn : -37.36055474226629\n",
      "Train_BestReturn : -37.36055474226629\n",
      "TimeSinceStart : 520.9646248817444\n",
      "Training Loss : 0.45352232456207275\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 142001\n",
      "mean reward (100 episodes) -36.885150\n",
      "best mean reward -36.885150\n",
      "running time 525.664328\n",
      "Train_EnvstepsSoFar : 142001\n",
      "Train_AverageReturn : -36.88514968359345\n",
      "Train_BestReturn : -36.88514968359345\n",
      "TimeSinceStart : 525.6643278598785\n",
      "Training Loss : 0.7648823261260986\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 143001\n",
      "mean reward (100 episodes) -29.463604\n",
      "best mean reward -29.463604\n",
      "running time 529.056348\n",
      "Train_EnvstepsSoFar : 143001\n",
      "Train_AverageReturn : -29.463604335701792\n",
      "Train_BestReturn : -29.463604335701792\n",
      "TimeSinceStart : 529.0563478469849\n",
      "Training Loss : 0.19052428007125854\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 144001\n",
      "mean reward (100 episodes) -26.586281\n",
      "best mean reward -26.586281\n",
      "running time 532.279565\n",
      "Train_EnvstepsSoFar : 144001\n",
      "Train_AverageReturn : -26.586280639577115\n",
      "Train_BestReturn : -26.586280639577115\n",
      "TimeSinceStart : 532.2795648574829\n",
      "Training Loss : 0.14504820108413696\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 145001\n",
      "mean reward (100 episodes) -25.785858\n",
      "best mean reward -25.785858\n",
      "running time 536.068476\n",
      "Train_EnvstepsSoFar : 145001\n",
      "Train_AverageReturn : -25.785858222738923\n",
      "Train_BestReturn : -25.785858222738923\n",
      "TimeSinceStart : 536.0684759616852\n",
      "Training Loss : 0.2794722020626068\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 146001\n",
      "mean reward (100 episodes) -22.023104\n",
      "best mean reward -22.023104\n",
      "running time 539.682481\n",
      "Train_EnvstepsSoFar : 146001\n",
      "Train_AverageReturn : -22.023103920997446\n",
      "Train_BestReturn : -22.023103920997446\n",
      "TimeSinceStart : 539.6824808120728\n",
      "Training Loss : 0.11021282523870468\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 147001\n",
      "mean reward (100 episodes) -16.551056\n",
      "best mean reward -16.551056\n",
      "running time 542.810130\n",
      "Train_EnvstepsSoFar : 147001\n",
      "Train_AverageReturn : -16.551055741648998\n",
      "Train_BestReturn : -16.551055741648998\n",
      "TimeSinceStart : 542.8101298809052\n",
      "Training Loss : 0.08474763482809067\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 148001\n",
      "mean reward (100 episodes) -14.790946\n",
      "best mean reward -14.790946\n",
      "running time 546.295225\n",
      "Train_EnvstepsSoFar : 148001\n",
      "Train_AverageReturn : -14.790945801220591\n",
      "Train_BestReturn : -14.790945801220591\n",
      "TimeSinceStart : 546.295224905014\n",
      "Training Loss : 0.09766382724046707\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 149001\n",
      "mean reward (100 episodes) -14.003430\n",
      "best mean reward -14.003430\n",
      "running time 550.216798\n",
      "Train_EnvstepsSoFar : 149001\n",
      "Train_AverageReturn : -14.003430037810778\n",
      "Train_BestReturn : -14.003430037810778\n",
      "TimeSinceStart : 550.2167978286743\n",
      "Training Loss : 0.19757312536239624\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 150001\n",
      "mean reward (100 episodes) -9.126679\n",
      "best mean reward -9.126679\n",
      "running time 553.612704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 150001\n",
      "Train_AverageReturn : -9.126679408708032\n",
      "Train_BestReturn : -9.126679408708032\n",
      "TimeSinceStart : 553.61270403862\n",
      "Training Loss : 0.12610465288162231\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 151001\n",
      "mean reward (100 episodes) -9.061874\n",
      "best mean reward -9.061874\n",
      "running time 558.088909\n",
      "Train_EnvstepsSoFar : 151001\n",
      "Train_AverageReturn : -9.061874021478602\n",
      "Train_BestReturn : -9.061874021478602\n",
      "TimeSinceStart : 558.0889086723328\n",
      "Training Loss : 0.08390339463949203\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 152001\n",
      "mean reward (100 episodes) -2.446398\n",
      "best mean reward -2.446398\n",
      "running time 560.909734\n",
      "Train_EnvstepsSoFar : 152001\n",
      "Train_AverageReturn : -2.4463976016089406\n",
      "Train_BestReturn : -2.4463976016089406\n",
      "TimeSinceStart : 560.9097337722778\n",
      "Training Loss : 0.10231329500675201\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 153001\n",
      "mean reward (100 episodes) -1.167353\n",
      "best mean reward -1.167353\n",
      "running time 564.220177\n",
      "Train_EnvstepsSoFar : 153001\n",
      "Train_AverageReturn : -1.1673526387795357\n",
      "Train_BestReturn : -1.1673526387795357\n",
      "TimeSinceStart : 564.2201769351959\n",
      "Training Loss : 0.10560592263936996\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 154001\n",
      "mean reward (100 episodes) -3.531706\n",
      "best mean reward -1.167353\n",
      "running time 567.621730\n",
      "Train_EnvstepsSoFar : 154001\n",
      "Train_AverageReturn : -3.5317059329273808\n",
      "Train_BestReturn : -1.1673526387795357\n",
      "TimeSinceStart : 567.621729850769\n",
      "Training Loss : 0.13540132343769073\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 155001\n",
      "mean reward (100 episodes) 3.831780\n",
      "best mean reward 3.831780\n",
      "running time 570.834104\n",
      "Train_EnvstepsSoFar : 155001\n",
      "Train_AverageReturn : 3.831779518429786\n",
      "Train_BestReturn : 3.831779518429786\n",
      "TimeSinceStart : 570.8341038227081\n",
      "Training Loss : 0.13447260856628418\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 156001\n",
      "mean reward (100 episodes) 6.657353\n",
      "best mean reward 6.657353\n",
      "running time 574.793679\n",
      "Train_EnvstepsSoFar : 156001\n",
      "Train_AverageReturn : 6.657352805113778\n",
      "Train_BestReturn : 6.657352805113778\n",
      "TimeSinceStart : 574.7936787605286\n",
      "Training Loss : 0.22786837816238403\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 157001\n",
      "mean reward (100 episodes) 15.476127\n",
      "best mean reward 15.476127\n",
      "running time 577.828178\n",
      "Train_EnvstepsSoFar : 157001\n",
      "Train_AverageReturn : 15.476127149669011\n",
      "Train_BestReturn : 15.476127149669011\n",
      "TimeSinceStart : 577.8281779289246\n",
      "Training Loss : 0.07180188596248627\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 158001\n",
      "mean reward (100 episodes) 17.077465\n",
      "best mean reward 17.077465\n",
      "running time 581.302146\n",
      "Train_EnvstepsSoFar : 158001\n",
      "Train_AverageReturn : 17.077465226485298\n",
      "Train_BestReturn : 17.077465226485298\n",
      "TimeSinceStart : 581.3021457195282\n",
      "Training Loss : 0.07409434020519257\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 159001\n",
      "mean reward (100 episodes) 18.818806\n",
      "best mean reward 18.818806\n",
      "running time 585.186630\n",
      "Train_EnvstepsSoFar : 159001\n",
      "Train_AverageReturn : 18.81880580311634\n",
      "Train_BestReturn : 18.81880580311634\n",
      "TimeSinceStart : 585.1866297721863\n",
      "Training Loss : 2.227221727371216\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 160001\n",
      "mean reward (100 episodes) 22.493175\n",
      "best mean reward 22.493175\n",
      "running time 588.083881\n",
      "Train_EnvstepsSoFar : 160001\n",
      "Train_AverageReturn : 22.493175152146943\n",
      "Train_BestReturn : 22.493175152146943\n",
      "TimeSinceStart : 588.0838809013367\n",
      "Training Loss : 0.20653721690177917\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 161001\n",
      "mean reward (100 episodes) 24.385483\n",
      "best mean reward 24.385483\n",
      "running time 591.460638\n",
      "Train_EnvstepsSoFar : 161001\n",
      "Train_AverageReturn : 24.385482766914933\n",
      "Train_BestReturn : 24.385482766914933\n",
      "TimeSinceStart : 591.4606378078461\n",
      "Training Loss : 3.0096335411071777\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 162001\n",
      "mean reward (100 episodes) 25.096610\n",
      "best mean reward 25.096610\n",
      "running time 595.916647\n",
      "Train_EnvstepsSoFar : 162001\n",
      "Train_AverageReturn : 25.096610399407815\n",
      "Train_BestReturn : 25.096610399407815\n",
      "TimeSinceStart : 595.9166469573975\n",
      "Training Loss : 0.8232060670852661\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 163001\n",
      "mean reward (100 episodes) 25.758256\n",
      "best mean reward 25.758256\n",
      "running time 600.309553\n",
      "Train_EnvstepsSoFar : 163001\n",
      "Train_AverageReturn : 25.75825562158752\n",
      "Train_BestReturn : 25.75825562158752\n",
      "TimeSinceStart : 600.3095529079437\n",
      "Training Loss : 0.24691198766231537\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 164001\n",
      "mean reward (100 episodes) 27.214919\n",
      "best mean reward 27.214919\n",
      "running time 603.745025\n",
      "Train_EnvstepsSoFar : 164001\n",
      "Train_AverageReturn : 27.21491941635174\n",
      "Train_BestReturn : 27.21491941635174\n",
      "TimeSinceStart : 603.7450246810913\n",
      "Training Loss : 0.1459019035100937\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 165001\n",
      "mean reward (100 episodes) 26.187381\n",
      "best mean reward 27.214919\n",
      "running time 607.225458\n",
      "Train_EnvstepsSoFar : 165001\n",
      "Train_AverageReturn : 26.18738139506851\n",
      "Train_BestReturn : 27.21491941635174\n",
      "TimeSinceStart : 607.225457906723\n",
      "Training Loss : 0.12919439375400543\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 166001\n",
      "mean reward (100 episodes) 30.833989\n",
      "best mean reward 30.833989\n",
      "running time 610.410449\n",
      "Train_EnvstepsSoFar : 166001\n",
      "Train_AverageReturn : 30.833989164856572\n",
      "Train_BestReturn : 30.833989164856572\n",
      "TimeSinceStart : 610.4104487895966\n",
      "Training Loss : 0.49375128746032715\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 167001\n",
      "mean reward (100 episodes) 33.530215\n",
      "best mean reward 33.530215\n",
      "running time 613.634963\n",
      "Train_EnvstepsSoFar : 167001\n",
      "Train_AverageReturn : 33.53021462288692\n",
      "Train_BestReturn : 33.53021462288692\n",
      "TimeSinceStart : 613.6349627971649\n",
      "Training Loss : 0.12918704748153687\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 168001\n",
      "mean reward (100 episodes) 37.016161\n",
      "best mean reward 37.016161\n",
      "running time 617.055468\n",
      "Train_EnvstepsSoFar : 168001\n",
      "Train_AverageReturn : 37.016161489566855\n",
      "Train_BestReturn : 37.016161489566855\n",
      "TimeSinceStart : 617.0554678440094\n",
      "Training Loss : 0.08945343643426895\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 169001\n",
      "mean reward (100 episodes) 36.954897\n",
      "best mean reward 37.016161\n",
      "running time 620.520405\n",
      "Train_EnvstepsSoFar : 169001\n",
      "Train_AverageReturn : 36.954896694907546\n",
      "Train_BestReturn : 37.016161489566855\n",
      "TimeSinceStart : 620.5204050540924\n",
      "Training Loss : 0.302185595035553\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 170001\n",
      "mean reward (100 episodes) 42.370799\n",
      "best mean reward 42.370799\n",
      "running time 624.494539\n",
      "Train_EnvstepsSoFar : 170001\n",
      "Train_AverageReturn : 42.37079933369172\n",
      "Train_BestReturn : 42.37079933369172\n",
      "TimeSinceStart : 624.4945387840271\n",
      "Training Loss : 0.18089871108531952\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 171001\n",
      "mean reward (100 episodes) 41.752818\n",
      "best mean reward 42.370799\n",
      "running time 627.843957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 171001\n",
      "Train_AverageReturn : 41.75281807944117\n",
      "Train_BestReturn : 42.37079933369172\n",
      "TimeSinceStart : 627.8439569473267\n",
      "Training Loss : 0.10063111782073975\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 172001\n",
      "mean reward (100 episodes) 43.416134\n",
      "best mean reward 43.416134\n",
      "running time 631.759931\n",
      "Train_EnvstepsSoFar : 172001\n",
      "Train_AverageReturn : 43.41613395609527\n",
      "Train_BestReturn : 43.41613395609527\n",
      "TimeSinceStart : 631.7599308490753\n",
      "Training Loss : 1.8448090553283691\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 173001\n",
      "mean reward (100 episodes) 43.037295\n",
      "best mean reward 43.416134\n",
      "running time 635.126220\n",
      "Train_EnvstepsSoFar : 173001\n",
      "Train_AverageReturn : 43.03729457140954\n",
      "Train_BestReturn : 43.41613395609527\n",
      "TimeSinceStart : 635.1262197494507\n",
      "Training Loss : 0.5921532511711121\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 174001\n",
      "mean reward (100 episodes) 46.337043\n",
      "best mean reward 46.337043\n",
      "running time 638.121136\n",
      "Train_EnvstepsSoFar : 174001\n",
      "Train_AverageReturn : 46.3370428534789\n",
      "Train_BestReturn : 46.3370428534789\n",
      "TimeSinceStart : 638.1211359500885\n",
      "Training Loss : 0.21577730774879456\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 175001\n",
      "mean reward (100 episodes) 46.070184\n",
      "best mean reward 46.337043\n",
      "running time 641.186285\n",
      "Train_EnvstepsSoFar : 175001\n",
      "Train_AverageReturn : 46.07018441817907\n",
      "Train_BestReturn : 46.3370428534789\n",
      "TimeSinceStart : 641.1862847805023\n",
      "Training Loss : 0.14186301827430725\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 176001\n",
      "mean reward (100 episodes) 48.901112\n",
      "best mean reward 48.901112\n",
      "running time 644.645314\n",
      "Train_EnvstepsSoFar : 176001\n",
      "Train_AverageReturn : 48.90111192439098\n",
      "Train_BestReturn : 48.90111192439098\n",
      "TimeSinceStart : 644.6453137397766\n",
      "Training Loss : 0.8302553296089172\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 177001\n",
      "mean reward (100 episodes) 49.657591\n",
      "best mean reward 49.657591\n",
      "running time 647.868858\n",
      "Train_EnvstepsSoFar : 177001\n",
      "Train_AverageReturn : 49.6575905911048\n",
      "Train_BestReturn : 49.6575905911048\n",
      "TimeSinceStart : 647.8688578605652\n",
      "Training Loss : 0.9236788153648376\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 178001\n",
      "mean reward (100 episodes) 44.610080\n",
      "best mean reward 49.657591\n",
      "running time 650.890350\n",
      "Train_EnvstepsSoFar : 178001\n",
      "Train_AverageReturn : 44.61008022394408\n",
      "Train_BestReturn : 49.6575905911048\n",
      "TimeSinceStart : 650.8903498649597\n",
      "Training Loss : 0.1269010752439499\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 179001\n",
      "mean reward (100 episodes) 45.573840\n",
      "best mean reward 49.657591\n",
      "running time 654.224336\n",
      "Train_EnvstepsSoFar : 179001\n",
      "Train_AverageReturn : 45.5738396901204\n",
      "Train_BestReturn : 49.6575905911048\n",
      "TimeSinceStart : 654.2243359088898\n",
      "Training Loss : 0.7672645449638367\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 180001\n",
      "mean reward (100 episodes) 50.056687\n",
      "best mean reward 50.056687\n",
      "running time 657.288622\n",
      "Train_EnvstepsSoFar : 180001\n",
      "Train_AverageReturn : 50.0566871645402\n",
      "Train_BestReturn : 50.0566871645402\n",
      "TimeSinceStart : 657.2886219024658\n",
      "Training Loss : 0.07005500048398972\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 181001\n",
      "mean reward (100 episodes) 52.040641\n",
      "best mean reward 52.040641\n",
      "running time 660.217680\n",
      "Train_EnvstepsSoFar : 181001\n",
      "Train_AverageReturn : 52.04064111091443\n",
      "Train_BestReturn : 52.04064111091443\n",
      "TimeSinceStart : 660.2176797389984\n",
      "Training Loss : 0.39738282561302185\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 182001\n",
      "mean reward (100 episodes) 55.714409\n",
      "best mean reward 55.714409\n",
      "running time 663.355461\n",
      "Train_EnvstepsSoFar : 182001\n",
      "Train_AverageReturn : 55.71440884579123\n",
      "Train_BestReturn : 55.71440884579123\n",
      "TimeSinceStart : 663.3554608821869\n",
      "Training Loss : 0.25735288858413696\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 183001\n",
      "mean reward (100 episodes) 58.148283\n",
      "best mean reward 58.148283\n",
      "running time 666.270108\n",
      "Train_EnvstepsSoFar : 183001\n",
      "Train_AverageReturn : 58.14828264550093\n",
      "Train_BestReturn : 58.14828264550093\n",
      "TimeSinceStart : 666.2701079845428\n",
      "Training Loss : 0.22678527235984802\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 184001\n",
      "mean reward (100 episodes) 61.193493\n",
      "best mean reward 61.193493\n",
      "running time 669.404313\n",
      "Train_EnvstepsSoFar : 184001\n",
      "Train_AverageReturn : 61.19349333887603\n",
      "Train_BestReturn : 61.19349333887603\n",
      "TimeSinceStart : 669.4043128490448\n",
      "Training Loss : 0.13668981194496155\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 185001\n",
      "mean reward (100 episodes) 61.716729\n",
      "best mean reward 61.716729\n",
      "running time 673.015346\n",
      "Train_EnvstepsSoFar : 185001\n",
      "Train_AverageReturn : 61.71672940131723\n",
      "Train_BestReturn : 61.71672940131723\n",
      "TimeSinceStart : 673.0153458118439\n",
      "Training Loss : 0.8966608643531799\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 186001\n",
      "mean reward (100 episodes) 64.124381\n",
      "best mean reward 64.124381\n",
      "running time 676.113866\n",
      "Train_EnvstepsSoFar : 186001\n",
      "Train_AverageReturn : 64.1243805731695\n",
      "Train_BestReturn : 64.1243805731695\n",
      "TimeSinceStart : 676.113865852356\n",
      "Training Loss : 0.10285855084657669\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 187001\n",
      "mean reward (100 episodes) 65.878991\n",
      "best mean reward 65.878991\n",
      "running time 679.137687\n",
      "Train_EnvstepsSoFar : 187001\n",
      "Train_AverageReturn : 65.87899103381453\n",
      "Train_BestReturn : 65.87899103381453\n",
      "TimeSinceStart : 679.1376867294312\n",
      "Training Loss : 0.12935195863246918\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 188001\n",
      "mean reward (100 episodes) 66.135215\n",
      "best mean reward 66.135215\n",
      "running time 683.026130\n",
      "Train_EnvstepsSoFar : 188001\n",
      "Train_AverageReturn : 66.13521484434288\n",
      "Train_BestReturn : 66.13521484434288\n",
      "TimeSinceStart : 683.0261299610138\n",
      "Training Loss : 0.10416333377361298\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 189001\n",
      "mean reward (100 episodes) 69.492302\n",
      "best mean reward 69.492302\n",
      "running time 686.035936\n",
      "Train_EnvstepsSoFar : 189001\n",
      "Train_AverageReturn : 69.49230223489451\n",
      "Train_BestReturn : 69.49230223489451\n",
      "TimeSinceStart : 686.0359358787537\n",
      "Training Loss : 0.14810869097709656\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 190001\n",
      "mean reward (100 episodes) 69.548653\n",
      "best mean reward 69.548653\n",
      "running time 689.891675\n",
      "Train_EnvstepsSoFar : 190001\n",
      "Train_AverageReturn : 69.54865310318058\n",
      "Train_BestReturn : 69.54865310318058\n",
      "TimeSinceStart : 689.8916747570038\n",
      "Training Loss : 2.1497414112091064\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 191001\n",
      "mean reward (100 episodes) 67.813303\n",
      "best mean reward 69.548653\n",
      "running time 692.891995\n",
      "Train_EnvstepsSoFar : 191001\n",
      "Train_AverageReturn : 67.81330287138053\n",
      "Train_BestReturn : 69.54865310318058\n",
      "TimeSinceStart : 692.8919949531555\n",
      "Training Loss : 0.14057117700576782\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 192001\n",
      "mean reward (100 episodes) 71.981255\n",
      "best mean reward 71.981255\n",
      "running time 695.765345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 192001\n",
      "Train_AverageReturn : 71.9812551379289\n",
      "Train_BestReturn : 71.9812551379289\n",
      "TimeSinceStart : 695.765344619751\n",
      "Training Loss : 0.7016628384590149\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 193001\n",
      "mean reward (100 episodes) 72.132247\n",
      "best mean reward 72.132247\n",
      "running time 699.192281\n",
      "Train_EnvstepsSoFar : 193001\n",
      "Train_AverageReturn : 72.13224708498277\n",
      "Train_BestReturn : 72.13224708498277\n",
      "TimeSinceStart : 699.1922807693481\n",
      "Training Loss : 0.46577930450439453\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 194001\n",
      "mean reward (100 episodes) 72.892416\n",
      "best mean reward 72.892416\n",
      "running time 704.190026\n",
      "Train_EnvstepsSoFar : 194001\n",
      "Train_AverageReturn : 72.89241568214784\n",
      "Train_BestReturn : 72.89241568214784\n",
      "TimeSinceStart : 704.1900260448456\n",
      "Training Loss : 1.9745227098464966\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 195001\n",
      "mean reward (100 episodes) 77.426409\n",
      "best mean reward 77.426409\n",
      "running time 706.959408\n",
      "Train_EnvstepsSoFar : 195001\n",
      "Train_AverageReturn : 77.42640927496113\n",
      "Train_BestReturn : 77.42640927496113\n",
      "TimeSinceStart : 706.9594078063965\n",
      "Training Loss : 1.5253379344940186\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 196001\n",
      "mean reward (100 episodes) 74.355459\n",
      "best mean reward 77.426409\n",
      "running time 710.250751\n",
      "Train_EnvstepsSoFar : 196001\n",
      "Train_AverageReturn : 74.35545888148242\n",
      "Train_BestReturn : 77.42640927496113\n",
      "TimeSinceStart : 710.2507510185242\n",
      "Training Loss : 0.12540242075920105\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 197001\n",
      "mean reward (100 episodes) 76.926550\n",
      "best mean reward 77.426409\n",
      "running time 713.397123\n",
      "Train_EnvstepsSoFar : 197001\n",
      "Train_AverageReturn : 76.92655043003245\n",
      "Train_BestReturn : 77.42640927496113\n",
      "TimeSinceStart : 713.3971228599548\n",
      "Training Loss : 1.170271873474121\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 198001\n",
      "mean reward (100 episodes) 77.803274\n",
      "best mean reward 77.803274\n",
      "running time 716.920104\n",
      "Train_EnvstepsSoFar : 198001\n",
      "Train_AverageReturn : 77.80327420089645\n",
      "Train_BestReturn : 77.80327420089645\n",
      "TimeSinceStart : 716.9201037883759\n",
      "Training Loss : 1.975972056388855\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 199001\n",
      "mean reward (100 episodes) 74.992329\n",
      "best mean reward 77.803274\n",
      "running time 721.737640\n",
      "Train_EnvstepsSoFar : 199001\n",
      "Train_AverageReturn : 74.99232930661078\n",
      "Train_BestReturn : 77.80327420089645\n",
      "TimeSinceStart : 721.7376399040222\n",
      "Training Loss : 0.28637444972991943\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 200001\n",
      "mean reward (100 episodes) 76.164091\n",
      "best mean reward 77.803274\n",
      "running time 724.889713\n",
      "Train_EnvstepsSoFar : 200001\n",
      "Train_AverageReturn : 76.16409058917357\n",
      "Train_BestReturn : 77.80327420089645\n",
      "TimeSinceStart : 724.8897128105164\n",
      "Training Loss : 0.12216558307409286\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 201001\n",
      "mean reward (100 episodes) 72.886764\n",
      "best mean reward 77.803274\n",
      "running time 728.633532\n",
      "Train_EnvstepsSoFar : 201001\n",
      "Train_AverageReturn : 72.88676420184449\n",
      "Train_BestReturn : 77.80327420089645\n",
      "TimeSinceStart : 728.6335318088531\n",
      "Training Loss : 0.3672434687614441\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 202001\n",
      "mean reward (100 episodes) 70.380590\n",
      "best mean reward 77.803274\n",
      "running time 731.865173\n",
      "Train_EnvstepsSoFar : 202001\n",
      "Train_AverageReturn : 70.38059027013581\n",
      "Train_BestReturn : 77.80327420089645\n",
      "TimeSinceStart : 731.8651728630066\n",
      "Training Loss : 0.17337779700756073\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 203001\n",
      "mean reward (100 episodes) 70.714746\n",
      "best mean reward 77.803274\n",
      "running time 735.056104\n",
      "Train_EnvstepsSoFar : 203001\n",
      "Train_AverageReturn : 70.71474599795219\n",
      "Train_BestReturn : 77.80327420089645\n",
      "TimeSinceStart : 735.0561039447784\n",
      "Training Loss : 0.09989206492900848\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 204001\n",
      "mean reward (100 episodes) 74.592055\n",
      "best mean reward 77.803274\n",
      "running time 738.478266\n",
      "Train_EnvstepsSoFar : 204001\n",
      "Train_AverageReturn : 74.5920553495763\n",
      "Train_BestReturn : 77.80327420089645\n",
      "TimeSinceStart : 738.4782657623291\n",
      "Training Loss : 0.37836509943008423\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 205001\n",
      "mean reward (100 episodes) 75.759374\n",
      "best mean reward 77.803274\n",
      "running time 741.478249\n",
      "Train_EnvstepsSoFar : 205001\n",
      "Train_AverageReturn : 75.75937442664237\n",
      "Train_BestReturn : 77.80327420089645\n",
      "TimeSinceStart : 741.47824883461\n",
      "Training Loss : 0.09084220230579376\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 206001\n",
      "mean reward (100 episodes) 77.898644\n",
      "best mean reward 77.898644\n",
      "running time 744.667529\n",
      "Train_EnvstepsSoFar : 206001\n",
      "Train_AverageReturn : 77.8986436129246\n",
      "Train_BestReturn : 77.8986436129246\n",
      "TimeSinceStart : 744.6675288677216\n",
      "Training Loss : 2.848573684692383\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 207001\n",
      "mean reward (100 episodes) 77.646849\n",
      "best mean reward 77.898644\n",
      "running time 747.778977\n",
      "Train_EnvstepsSoFar : 207001\n",
      "Train_AverageReturn : 77.64684854759292\n",
      "Train_BestReturn : 77.8986436129246\n",
      "TimeSinceStart : 747.7789766788483\n",
      "Training Loss : 0.2337174415588379\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 208001\n",
      "mean reward (100 episodes) 77.757206\n",
      "best mean reward 77.898644\n",
      "running time 750.888805\n",
      "Train_EnvstepsSoFar : 208001\n",
      "Train_AverageReturn : 77.7572063910675\n",
      "Train_BestReturn : 77.8986436129246\n",
      "TimeSinceStart : 750.8888049125671\n",
      "Training Loss : 0.16799458861351013\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 209001\n",
      "mean reward (100 episodes) 76.973457\n",
      "best mean reward 77.898644\n",
      "running time 754.331898\n",
      "Train_EnvstepsSoFar : 209001\n",
      "Train_AverageReturn : 76.97345744428388\n",
      "Train_BestReturn : 77.8986436129246\n",
      "TimeSinceStart : 754.3318977355957\n",
      "Training Loss : 1.082099437713623\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 210001\n",
      "mean reward (100 episodes) 76.349447\n",
      "best mean reward 77.898644\n",
      "running time 758.171513\n",
      "Train_EnvstepsSoFar : 210001\n",
      "Train_AverageReturn : 76.3494468963752\n",
      "Train_BestReturn : 77.8986436129246\n",
      "TimeSinceStart : 758.1715128421783\n",
      "Training Loss : 0.1720072478055954\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 211001\n",
      "mean reward (100 episodes) 79.085304\n",
      "best mean reward 79.085304\n",
      "running time 761.020650\n",
      "Train_EnvstepsSoFar : 211001\n",
      "Train_AverageReturn : 79.08530425701348\n",
      "Train_BestReturn : 79.08530425701348\n",
      "TimeSinceStart : 761.0206499099731\n",
      "Training Loss : 0.04806753993034363\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 212001\n",
      "mean reward (100 episodes) 78.916868\n",
      "best mean reward 79.085304\n",
      "running time 764.268168\n",
      "Train_EnvstepsSoFar : 212001\n",
      "Train_AverageReturn : 78.91686761242953\n",
      "Train_BestReturn : 79.08530425701348\n",
      "TimeSinceStart : 764.2681679725647\n",
      "Training Loss : 0.29768213629722595\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 213001\n",
      "mean reward (100 episodes) 79.185652\n",
      "best mean reward 79.185652\n",
      "running time 767.298492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 213001\n",
      "Train_AverageReturn : 79.18565247276112\n",
      "Train_BestReturn : 79.18565247276112\n",
      "TimeSinceStart : 767.2984919548035\n",
      "Training Loss : 0.12912341952323914\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 214001\n",
      "mean reward (100 episodes) 79.491542\n",
      "best mean reward 79.491542\n",
      "running time 771.334680\n",
      "Train_EnvstepsSoFar : 214001\n",
      "Train_AverageReturn : 79.49154193806017\n",
      "Train_BestReturn : 79.49154193806017\n",
      "TimeSinceStart : 771.3346798419952\n",
      "Training Loss : 0.14437708258628845\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 215001\n",
      "mean reward (100 episodes) 83.471761\n",
      "best mean reward 83.471761\n",
      "running time 774.711917\n",
      "Train_EnvstepsSoFar : 215001\n",
      "Train_AverageReturn : 83.4717612795487\n",
      "Train_BestReturn : 83.4717612795487\n",
      "TimeSinceStart : 774.7119166851044\n",
      "Training Loss : 1.50114107131958\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 216001\n",
      "mean reward (100 episodes) 83.586784\n",
      "best mean reward 83.586784\n",
      "running time 778.766672\n",
      "Train_EnvstepsSoFar : 216001\n",
      "Train_AverageReturn : 83.5867839697358\n",
      "Train_BestReturn : 83.5867839697358\n",
      "TimeSinceStart : 778.7666718959808\n",
      "Training Loss : 0.15324260294437408\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 217001\n",
      "mean reward (100 episodes) 83.342111\n",
      "best mean reward 83.586784\n",
      "running time 781.678369\n",
      "Train_EnvstepsSoFar : 217001\n",
      "Train_AverageReturn : 83.34211052654018\n",
      "Train_BestReturn : 83.5867839697358\n",
      "TimeSinceStart : 781.678368806839\n",
      "Training Loss : 0.4222380816936493\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 218001\n",
      "mean reward (100 episodes) 83.802183\n",
      "best mean reward 83.802183\n",
      "running time 785.529821\n",
      "Train_EnvstepsSoFar : 218001\n",
      "Train_AverageReturn : 83.80218279141613\n",
      "Train_BestReturn : 83.80218279141613\n",
      "TimeSinceStart : 785.5298209190369\n",
      "Training Loss : 0.09058745950460434\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 219001\n",
      "mean reward (100 episodes) 87.991220\n",
      "best mean reward 87.991220\n",
      "running time 788.548343\n",
      "Train_EnvstepsSoFar : 219001\n",
      "Train_AverageReturn : 87.99122039520871\n",
      "Train_BestReturn : 87.99122039520871\n",
      "TimeSinceStart : 788.548342704773\n",
      "Training Loss : 0.47001516819000244\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 220001\n",
      "mean reward (100 episodes) 89.014012\n",
      "best mean reward 89.014012\n",
      "running time 791.686307\n",
      "Train_EnvstepsSoFar : 220001\n",
      "Train_AverageReturn : 89.01401176044608\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 791.6863067150116\n",
      "Training Loss : 0.49441277980804443\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 221001\n",
      "mean reward (100 episodes) 82.813055\n",
      "best mean reward 89.014012\n",
      "running time 795.300544\n",
      "Train_EnvstepsSoFar : 221001\n",
      "Train_AverageReturn : 82.81305453969293\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 795.3005440235138\n",
      "Training Loss : 0.10783655196428299\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 222001\n",
      "mean reward (100 episodes) 78.892133\n",
      "best mean reward 89.014012\n",
      "running time 799.058355\n",
      "Train_EnvstepsSoFar : 222001\n",
      "Train_AverageReturn : 78.89213319717936\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 799.0583548545837\n",
      "Training Loss : 4.235630512237549\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 223001\n",
      "mean reward (100 episodes) 76.111743\n",
      "best mean reward 89.014012\n",
      "running time 804.006173\n",
      "Train_EnvstepsSoFar : 223001\n",
      "Train_AverageReturn : 76.1117432264937\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 804.0061728954315\n",
      "Training Loss : 0.10104749351739883\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 224001\n",
      "mean reward (100 episodes) 80.002448\n",
      "best mean reward 89.014012\n",
      "running time 808.216017\n",
      "Train_EnvstepsSoFar : 224001\n",
      "Train_AverageReturn : 80.00244779568283\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 808.2160167694092\n",
      "Training Loss : 0.12206752598285675\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 225001\n",
      "mean reward (100 episodes) 77.649332\n",
      "best mean reward 89.014012\n",
      "running time 813.431942\n",
      "Train_EnvstepsSoFar : 225001\n",
      "Train_AverageReturn : 77.64933233853543\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 813.4319417476654\n",
      "Training Loss : 1.1750235557556152\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 226001\n",
      "mean reward (100 episodes) 78.424577\n",
      "best mean reward 89.014012\n",
      "running time 818.963367\n",
      "Train_EnvstepsSoFar : 226001\n",
      "Train_AverageReturn : 78.42457675273154\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 818.963366985321\n",
      "Training Loss : 0.5635226368904114\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 227001\n",
      "mean reward (100 episodes) 79.274421\n",
      "best mean reward 89.014012\n",
      "running time 827.331895\n",
      "Train_EnvstepsSoFar : 227001\n",
      "Train_AverageReturn : 79.27442084924043\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 827.3318946361542\n",
      "Training Loss : 0.18254722654819489\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 228001\n",
      "mean reward (100 episodes) 72.718398\n",
      "best mean reward 89.014012\n",
      "running time 832.236620\n",
      "Train_EnvstepsSoFar : 228001\n",
      "Train_AverageReturn : 72.7183978647709\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 832.2366197109222\n",
      "Training Loss : 0.18451060354709625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 229001\n",
      "mean reward (100 episodes) 72.362204\n",
      "best mean reward 89.014012\n",
      "running time 838.748864\n",
      "Train_EnvstepsSoFar : 229001\n",
      "Train_AverageReturn : 72.36220356100247\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 838.7488639354706\n",
      "Training Loss : 0.10493822395801544\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 230001\n",
      "mean reward (100 episodes) 74.158245\n",
      "best mean reward 89.014012\n",
      "running time 843.615292\n",
      "Train_EnvstepsSoFar : 230001\n",
      "Train_AverageReturn : 74.15824459045966\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 843.6152918338776\n",
      "Training Loss : 0.49960216879844666\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 231001\n",
      "mean reward (100 episodes) 67.972483\n",
      "best mean reward 89.014012\n",
      "running time 847.993472\n",
      "Train_EnvstepsSoFar : 231001\n",
      "Train_AverageReturn : 67.97248347324415\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 847.9934718608856\n",
      "Training Loss : 0.8484829068183899\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 232001\n",
      "mean reward (100 episodes) 67.868024\n",
      "best mean reward 89.014012\n",
      "running time 852.652698\n",
      "Train_EnvstepsSoFar : 232001\n",
      "Train_AverageReturn : 67.86802416196399\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 852.65269780159\n",
      "Training Loss : 0.3840218484401703\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 233001\n",
      "mean reward (100 episodes) 70.177146\n",
      "best mean reward 89.014012\n",
      "running time 857.207260\n",
      "Train_EnvstepsSoFar : 233001\n",
      "Train_AverageReturn : 70.1771456234877\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 857.2072598934174\n",
      "Training Loss : 0.09551849216222763\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 234001\n",
      "mean reward (100 episodes) 68.010532\n",
      "best mean reward 89.014012\n",
      "running time 860.758976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 234001\n",
      "Train_AverageReturn : 68.010531633467\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 860.758975982666\n",
      "Training Loss : 0.2807939946651459\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 235001\n",
      "mean reward (100 episodes) 66.949310\n",
      "best mean reward 89.014012\n",
      "running time 866.497810\n",
      "Train_EnvstepsSoFar : 235001\n",
      "Train_AverageReturn : 66.94930971546906\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 866.4978098869324\n",
      "Training Loss : 0.08750791847705841\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 236001\n",
      "mean reward (100 episodes) 65.609635\n",
      "best mean reward 89.014012\n",
      "running time 872.286431\n",
      "Train_EnvstepsSoFar : 236001\n",
      "Train_AverageReturn : 65.6096348828747\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 872.2864308357239\n",
      "Training Loss : 0.7335444092750549\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 237001\n",
      "mean reward (100 episodes) 70.125816\n",
      "best mean reward 89.014012\n",
      "running time 876.270822\n",
      "Train_EnvstepsSoFar : 237001\n",
      "Train_AverageReturn : 70.1258155578455\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 876.2708218097687\n",
      "Training Loss : 0.15908940136432648\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 238001\n",
      "mean reward (100 episodes) 71.242483\n",
      "best mean reward 89.014012\n",
      "running time 879.667362\n",
      "Train_EnvstepsSoFar : 238001\n",
      "Train_AverageReturn : 71.24248275631331\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 879.6673617362976\n",
      "Training Loss : 0.20428067445755005\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 239001\n",
      "mean reward (100 episodes) 75.296333\n",
      "best mean reward 89.014012\n",
      "running time 883.263888\n",
      "Train_EnvstepsSoFar : 239001\n",
      "Train_AverageReturn : 75.29633251380184\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 883.2638878822327\n",
      "Training Loss : 5.585076332092285\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 240001\n",
      "mean reward (100 episodes) 72.952841\n",
      "best mean reward 89.014012\n",
      "running time 890.185341\n",
      "Train_EnvstepsSoFar : 240001\n",
      "Train_AverageReturn : 72.95284113968002\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 890.1853408813477\n",
      "Training Loss : 0.09489278495311737\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 241001\n",
      "mean reward (100 episodes) 72.587195\n",
      "best mean reward 89.014012\n",
      "running time 896.106422\n",
      "Train_EnvstepsSoFar : 241001\n",
      "Train_AverageReturn : 72.5871949512176\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 896.1064217090607\n",
      "Training Loss : 1.423580288887024\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 242001\n",
      "mean reward (100 episodes) 70.747385\n",
      "best mean reward 89.014012\n",
      "running time 899.506235\n",
      "Train_EnvstepsSoFar : 242001\n",
      "Train_AverageReturn : 70.74738485019863\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 899.5062348842621\n",
      "Training Loss : 0.06309956312179565\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 243001\n",
      "mean reward (100 episodes) 69.601581\n",
      "best mean reward 89.014012\n",
      "running time 903.309693\n",
      "Train_EnvstepsSoFar : 243001\n",
      "Train_AverageReturn : 69.60158115847611\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 903.3096928596497\n",
      "Training Loss : 1.3098716735839844\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 244001\n",
      "mean reward (100 episodes) 69.239145\n",
      "best mean reward 89.014012\n",
      "running time 907.395455\n",
      "Train_EnvstepsSoFar : 244001\n",
      "Train_AverageReturn : 69.23914496895351\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 907.3954548835754\n",
      "Training Loss : 0.28527283668518066\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 245001\n",
      "mean reward (100 episodes) 68.711667\n",
      "best mean reward 89.014012\n",
      "running time 911.029875\n",
      "Train_EnvstepsSoFar : 245001\n",
      "Train_AverageReturn : 68.71166664217567\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 911.0298748016357\n",
      "Training Loss : 0.1473854035139084\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 246001\n",
      "mean reward (100 episodes) 76.288153\n",
      "best mean reward 89.014012\n",
      "running time 914.521632\n",
      "Train_EnvstepsSoFar : 246001\n",
      "Train_AverageReturn : 76.28815317460669\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 914.5216319561005\n",
      "Training Loss : 1.2447125911712646\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 247001\n",
      "mean reward (100 episodes) 77.023514\n",
      "best mean reward 89.014012\n",
      "running time 918.711652\n",
      "Train_EnvstepsSoFar : 247001\n",
      "Train_AverageReturn : 77.02351401901002\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 918.711651802063\n",
      "Training Loss : 0.3174723982810974\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 248001\n",
      "mean reward (100 episodes) 71.368594\n",
      "best mean reward 89.014012\n",
      "running time 924.109583\n",
      "Train_EnvstepsSoFar : 248001\n",
      "Train_AverageReturn : 71.36859356380887\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 924.109582901001\n",
      "Training Loss : 0.08731256425380707\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 249001\n",
      "mean reward (100 episodes) 74.260714\n",
      "best mean reward 89.014012\n",
      "running time 927.582147\n",
      "Train_EnvstepsSoFar : 249001\n",
      "Train_AverageReturn : 74.26071372462968\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 927.5821468830109\n",
      "Training Loss : 0.42667651176452637\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 250001\n",
      "mean reward (100 episodes) 73.271025\n",
      "best mean reward 89.014012\n",
      "running time 932.312537\n",
      "Train_EnvstepsSoFar : 250001\n",
      "Train_AverageReturn : 73.27102471589181\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 932.3125367164612\n",
      "Training Loss : 0.5798428058624268\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 251001\n",
      "mean reward (100 episodes) 69.349718\n",
      "best mean reward 89.014012\n",
      "running time 935.915498\n",
      "Train_EnvstepsSoFar : 251001\n",
      "Train_AverageReturn : 69.34971757839713\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 935.9154980182648\n",
      "Training Loss : 0.7305026054382324\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 252001\n",
      "mean reward (100 episodes) 72.227291\n",
      "best mean reward 89.014012\n",
      "running time 939.329831\n",
      "Train_EnvstepsSoFar : 252001\n",
      "Train_AverageReturn : 72.2272906421563\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 939.3298308849335\n",
      "Training Loss : 0.24735867977142334\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 253001\n",
      "mean reward (100 episodes) 71.723072\n",
      "best mean reward 89.014012\n",
      "running time 942.379210\n",
      "Train_EnvstepsSoFar : 253001\n",
      "Train_AverageReturn : 71.72307204777438\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 942.3792099952698\n",
      "Training Loss : 0.272226482629776\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 254001\n",
      "mean reward (100 episodes) 71.098231\n",
      "best mean reward 89.014012\n",
      "running time 946.956964\n",
      "Train_EnvstepsSoFar : 254001\n",
      "Train_AverageReturn : 71.09823136438965\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 946.9569637775421\n",
      "Training Loss : 0.0940287783741951\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 255001\n",
      "mean reward (100 episodes) 70.025451\n",
      "best mean reward 89.014012\n",
      "running time 950.646159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 255001\n",
      "Train_AverageReturn : 70.02545064606676\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 950.646158695221\n",
      "Training Loss : 3.140202760696411\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 256001\n",
      "mean reward (100 episodes) 68.873311\n",
      "best mean reward 89.014012\n",
      "running time 953.694918\n",
      "Train_EnvstepsSoFar : 256001\n",
      "Train_AverageReturn : 68.87331068885314\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 953.6949179172516\n",
      "Training Loss : 0.42642661929130554\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 257001\n",
      "mean reward (100 episodes) 72.093990\n",
      "best mean reward 89.014012\n",
      "running time 958.141695\n",
      "Train_EnvstepsSoFar : 257001\n",
      "Train_AverageReturn : 72.09398961744589\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 958.141695022583\n",
      "Training Loss : 0.3066883087158203\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 258001\n",
      "mean reward (100 episodes) 74.686678\n",
      "best mean reward 89.014012\n",
      "running time 961.101164\n",
      "Train_EnvstepsSoFar : 258001\n",
      "Train_AverageReturn : 74.68667767794537\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 961.1011636257172\n",
      "Training Loss : 1.6176297664642334\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 259001\n",
      "mean reward (100 episodes) 66.249914\n",
      "best mean reward 89.014012\n",
      "running time 963.999542\n",
      "Train_EnvstepsSoFar : 259001\n",
      "Train_AverageReturn : 66.24991359219672\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 963.999541759491\n",
      "Training Loss : 0.6749013662338257\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 260001\n",
      "mean reward (100 episodes) 61.887683\n",
      "best mean reward 89.014012\n",
      "running time 967.733526\n",
      "Train_EnvstepsSoFar : 260001\n",
      "Train_AverageReturn : 61.88768267086364\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 967.7335257530212\n",
      "Training Loss : 0.11925245821475983\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 261001\n",
      "mean reward (100 episodes) 71.008961\n",
      "best mean reward 89.014012\n",
      "running time 970.686317\n",
      "Train_EnvstepsSoFar : 261001\n",
      "Train_AverageReturn : 71.008961327085\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 970.6863167285919\n",
      "Training Loss : 0.14661061763763428\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 262001\n",
      "mean reward (100 episodes) 71.993915\n",
      "best mean reward 89.014012\n",
      "running time 974.404448\n",
      "Train_EnvstepsSoFar : 262001\n",
      "Train_AverageReturn : 71.99391491027934\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 974.4044480323792\n",
      "Training Loss : 0.08653752505779266\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 263001\n",
      "mean reward (100 episodes) 68.979397\n",
      "best mean reward 89.014012\n",
      "running time 977.657970\n",
      "Train_EnvstepsSoFar : 263001\n",
      "Train_AverageReturn : 68.97939690680955\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 977.6579699516296\n",
      "Training Loss : 1.246734380722046\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 264001\n",
      "mean reward (100 episodes) 75.086382\n",
      "best mean reward 89.014012\n",
      "running time 981.109018\n",
      "Train_EnvstepsSoFar : 264001\n",
      "Train_AverageReturn : 75.08638198934261\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 981.1090178489685\n",
      "Training Loss : 0.1022450253367424\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 265001\n",
      "mean reward (100 episodes) 75.096988\n",
      "best mean reward 89.014012\n",
      "running time 984.690019\n",
      "Train_EnvstepsSoFar : 265001\n",
      "Train_AverageReturn : 75.09698821600001\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 984.6900188922882\n",
      "Training Loss : 0.496940940618515\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 266001\n",
      "mean reward (100 episodes) 75.222247\n",
      "best mean reward 89.014012\n",
      "running time 989.014826\n",
      "Train_EnvstepsSoFar : 266001\n",
      "Train_AverageReturn : 75.2222469699019\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 989.0148260593414\n",
      "Training Loss : 0.32287827134132385\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 267001\n",
      "mean reward (100 episodes) 73.920641\n",
      "best mean reward 89.014012\n",
      "running time 992.714413\n",
      "Train_EnvstepsSoFar : 267001\n",
      "Train_AverageReturn : 73.92064086643813\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 992.714412689209\n",
      "Training Loss : 0.4339776039123535\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 268001\n",
      "mean reward (100 episodes) 72.992489\n",
      "best mean reward 89.014012\n",
      "running time 996.293005\n",
      "Train_EnvstepsSoFar : 268001\n",
      "Train_AverageReturn : 72.99248906887165\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 996.2930047512054\n",
      "Training Loss : 0.12439621239900589\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 269001\n",
      "mean reward (100 episodes) 70.862123\n",
      "best mean reward 89.014012\n",
      "running time 1000.711241\n",
      "Train_EnvstepsSoFar : 269001\n",
      "Train_AverageReturn : 70.86212319245203\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1000.7112410068512\n",
      "Training Loss : 0.612141489982605\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 270001\n",
      "mean reward (100 episodes) 71.448749\n",
      "best mean reward 89.014012\n",
      "running time 1004.625542\n",
      "Train_EnvstepsSoFar : 270001\n",
      "Train_AverageReturn : 71.4487489154462\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1004.6255419254303\n",
      "Training Loss : 0.15301740169525146\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 271001\n",
      "mean reward (100 episodes) 70.557027\n",
      "best mean reward 89.014012\n",
      "running time 1008.940870\n",
      "Train_EnvstepsSoFar : 271001\n",
      "Train_AverageReturn : 70.55702669513106\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1008.940869808197\n",
      "Training Loss : 0.2616243362426758\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 272001\n",
      "mean reward (100 episodes) 73.203988\n",
      "best mean reward 89.014012\n",
      "running time 1012.414069\n",
      "Train_EnvstepsSoFar : 272001\n",
      "Train_AverageReturn : 73.2039875080861\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1012.4140689373016\n",
      "Training Loss : 0.13089990615844727\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 273001\n",
      "mean reward (100 episodes) 75.287186\n",
      "best mean reward 89.014012\n",
      "running time 1015.457374\n",
      "Train_EnvstepsSoFar : 273001\n",
      "Train_AverageReturn : 75.28718594399203\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1015.4573738574982\n",
      "Training Loss : 2.763131618499756\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 274001\n",
      "mean reward (100 episodes) 78.174720\n",
      "best mean reward 89.014012\n",
      "running time 1019.948301\n",
      "Train_EnvstepsSoFar : 274001\n",
      "Train_AverageReturn : 78.17471978811663\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1019.9483008384705\n",
      "Training Loss : 0.826283872127533\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 275001\n",
      "mean reward (100 episodes) 75.676047\n",
      "best mean reward 89.014012\n",
      "running time 1024.031192\n",
      "Train_EnvstepsSoFar : 275001\n",
      "Train_AverageReturn : 75.6760466267882\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1024.0311918258667\n",
      "Training Loss : 0.13473661243915558\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 276001\n",
      "mean reward (100 episodes) 72.360640\n",
      "best mean reward 89.014012\n",
      "running time 1029.363467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 276001\n",
      "Train_AverageReturn : 72.36063960624897\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1029.3634667396545\n",
      "Training Loss : 0.5009975433349609\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 277001\n",
      "mean reward (100 episodes) 67.858672\n",
      "best mean reward 89.014012\n",
      "running time 1034.199267\n",
      "Train_EnvstepsSoFar : 277001\n",
      "Train_AverageReturn : 67.85867156082928\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1034.1992666721344\n",
      "Training Loss : 0.2840096950531006\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 278001\n",
      "mean reward (100 episodes) 64.871323\n",
      "best mean reward 89.014012\n",
      "running time 1041.999840\n",
      "Train_EnvstepsSoFar : 278001\n",
      "Train_AverageReturn : 64.87132295244066\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1041.9998400211334\n",
      "Training Loss : 0.16190633177757263\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 279001\n",
      "mean reward (100 episodes) 67.614540\n",
      "best mean reward 89.014012\n",
      "running time 1046.918264\n",
      "Train_EnvstepsSoFar : 279001\n",
      "Train_AverageReturn : 67.61454026817093\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1046.9182636737823\n",
      "Training Loss : 0.29604607820510864\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 280001\n",
      "mean reward (100 episodes) 69.754931\n",
      "best mean reward 89.014012\n",
      "running time 1050.801857\n",
      "Train_EnvstepsSoFar : 280001\n",
      "Train_AverageReturn : 69.75493129170233\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1050.8018567562103\n",
      "Training Loss : 1.5920228958129883\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 281001\n",
      "mean reward (100 episodes) 68.186122\n",
      "best mean reward 89.014012\n",
      "running time 1054.458016\n",
      "Train_EnvstepsSoFar : 281001\n",
      "Train_AverageReturn : 68.18612196240579\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1054.4580159187317\n",
      "Training Loss : 0.13291068375110626\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 282001\n",
      "mean reward (100 episodes) 75.042341\n",
      "best mean reward 89.014012\n",
      "running time 1058.838094\n",
      "Train_EnvstepsSoFar : 282001\n",
      "Train_AverageReturn : 75.04234133909746\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1058.8380937576294\n",
      "Training Loss : 0.14246255159378052\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 283001\n",
      "mean reward (100 episodes) 75.249916\n",
      "best mean reward 89.014012\n",
      "running time 1062.515744\n",
      "Train_EnvstepsSoFar : 283001\n",
      "Train_AverageReturn : 75.24991563975001\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1062.515743970871\n",
      "Training Loss : 0.3672037720680237\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 284001\n",
      "mean reward (100 episodes) 76.592928\n",
      "best mean reward 89.014012\n",
      "running time 1066.307760\n",
      "Train_EnvstepsSoFar : 284001\n",
      "Train_AverageReturn : 76.59292779262941\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1066.3077597618103\n",
      "Training Loss : 0.3383682668209076\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 285001\n",
      "mean reward (100 episodes) 72.423610\n",
      "best mean reward 89.014012\n",
      "running time 1069.747807\n",
      "Train_EnvstepsSoFar : 285001\n",
      "Train_AverageReturn : 72.423610377254\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1069.7478070259094\n",
      "Training Loss : 1.4106347560882568\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 286001\n",
      "mean reward (100 episodes) 69.343149\n",
      "best mean reward 89.014012\n",
      "running time 1073.379131\n",
      "Train_EnvstepsSoFar : 286001\n",
      "Train_AverageReturn : 69.34314911675955\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1073.3791308403015\n",
      "Training Loss : 0.9165222644805908\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 287001\n",
      "mean reward (100 episodes) 70.080904\n",
      "best mean reward 89.014012\n",
      "running time 1076.158698\n",
      "Train_EnvstepsSoFar : 287001\n",
      "Train_AverageReturn : 70.08090426423881\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1076.1586978435516\n",
      "Training Loss : 0.18266627192497253\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 288001\n",
      "mean reward (100 episodes) 77.912867\n",
      "best mean reward 89.014012\n",
      "running time 1078.860187\n",
      "Train_EnvstepsSoFar : 288001\n",
      "Train_AverageReturn : 77.9128665062003\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1078.8601868152618\n",
      "Training Loss : 0.21634776890277863\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 289001\n",
      "mean reward (100 episodes) 66.573276\n",
      "best mean reward 89.014012\n",
      "running time 1081.578955\n",
      "Train_EnvstepsSoFar : 289001\n",
      "Train_AverageReturn : 66.57327554015966\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1081.5789546966553\n",
      "Training Loss : 0.1773267239332199\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 290001\n",
      "mean reward (100 episodes) 62.352400\n",
      "best mean reward 89.014012\n",
      "running time 1084.330371\n",
      "Train_EnvstepsSoFar : 290001\n",
      "Train_AverageReturn : 62.35239967461698\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1084.3303706645966\n",
      "Training Loss : 0.09416209161281586\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 291001\n",
      "mean reward (100 episodes) 61.907754\n",
      "best mean reward 89.014012\n",
      "running time 1087.177420\n",
      "Train_EnvstepsSoFar : 291001\n",
      "Train_AverageReturn : 61.90775407963409\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1087.1774196624756\n",
      "Training Loss : 4.891110897064209\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 292001\n",
      "mean reward (100 episodes) 57.638149\n",
      "best mean reward 89.014012\n",
      "running time 1089.902551\n",
      "Train_EnvstepsSoFar : 292001\n",
      "Train_AverageReturn : 57.6381487275384\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1089.9025506973267\n",
      "Training Loss : 0.2419419288635254\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 293001\n",
      "mean reward (100 episodes) 56.288582\n",
      "best mean reward 89.014012\n",
      "running time 1092.558175\n",
      "Train_EnvstepsSoFar : 293001\n",
      "Train_AverageReturn : 56.28858237861532\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1092.5581748485565\n",
      "Training Loss : 3.3255228996276855\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 294001\n",
      "mean reward (100 episodes) 54.483528\n",
      "best mean reward 89.014012\n",
      "running time 1095.538131\n",
      "Train_EnvstepsSoFar : 294001\n",
      "Train_AverageReturn : 54.48352843207792\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1095.5381309986115\n",
      "Training Loss : 0.3167323172092438\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 295001\n",
      "mean reward (100 episodes) 57.220836\n",
      "best mean reward 89.014012\n",
      "running time 1098.589251\n",
      "Train_EnvstepsSoFar : 295001\n",
      "Train_AverageReturn : 57.22083596183329\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1098.5892508029938\n",
      "Training Loss : 0.21194148063659668\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 296001\n",
      "mean reward (100 episodes) 56.080575\n",
      "best mean reward 89.014012\n",
      "running time 1101.635235\n",
      "Train_EnvstepsSoFar : 296001\n",
      "Train_AverageReturn : 56.08057506459252\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1101.6352348327637\n",
      "Training Loss : 2.7232577800750732\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 297001\n",
      "mean reward (100 episodes) 57.594073\n",
      "best mean reward 89.014012\n",
      "running time 1104.364003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 297001\n",
      "Train_AverageReturn : 57.59407269021192\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1104.3640027046204\n",
      "Training Loss : 0.32857590913772583\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 298001\n",
      "mean reward (100 episodes) 68.529858\n",
      "best mean reward 89.014012\n",
      "running time 1107.052532\n",
      "Train_EnvstepsSoFar : 298001\n",
      "Train_AverageReturn : 68.52985826948158\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1107.0525319576263\n",
      "Training Loss : 0.13248154520988464\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 299001\n",
      "mean reward (100 episodes) 65.330246\n",
      "best mean reward 89.014012\n",
      "running time 1109.789802\n",
      "Train_EnvstepsSoFar : 299001\n",
      "Train_AverageReturn : 65.33024554141468\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1109.7898018360138\n",
      "Training Loss : 0.07267972826957703\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 300001\n",
      "mean reward (100 episodes) 67.684706\n",
      "best mean reward 89.014012\n",
      "running time 1112.572389\n",
      "Train_EnvstepsSoFar : 300001\n",
      "Train_AverageReturn : 67.68470617941553\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1112.5723888874054\n",
      "Training Loss : 0.4068509042263031\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 301001\n",
      "mean reward (100 episodes) 68.304099\n",
      "best mean reward 89.014012\n",
      "running time 1115.381544\n",
      "Train_EnvstepsSoFar : 301001\n",
      "Train_AverageReturn : 68.3040986996652\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1115.3815438747406\n",
      "Training Loss : 0.8190260529518127\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 302001\n",
      "mean reward (100 episodes) 67.594165\n",
      "best mean reward 89.014012\n",
      "running time 1118.202211\n",
      "Train_EnvstepsSoFar : 302001\n",
      "Train_AverageReturn : 67.59416516646469\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1118.2022106647491\n",
      "Training Loss : 0.28567010164260864\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 303001\n",
      "mean reward (100 episodes) 75.468600\n",
      "best mean reward 89.014012\n",
      "running time 1120.845836\n",
      "Train_EnvstepsSoFar : 303001\n",
      "Train_AverageReturn : 75.46859970943535\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1120.84583568573\n",
      "Training Loss : 1.3844654560089111\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 304001\n",
      "mean reward (100 episodes) 81.698814\n",
      "best mean reward 89.014012\n",
      "running time 1123.672875\n",
      "Train_EnvstepsSoFar : 304001\n",
      "Train_AverageReturn : 81.69881421590182\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1123.6728749275208\n",
      "Training Loss : 0.8053910136222839\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 305001\n",
      "mean reward (100 episodes) 84.943559\n",
      "best mean reward 89.014012\n",
      "running time 1126.387008\n",
      "Train_EnvstepsSoFar : 305001\n",
      "Train_AverageReturn : 84.9435588743108\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1126.3870079517365\n",
      "Training Loss : 0.2584771513938904\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 306001\n",
      "mean reward (100 episodes) 83.755189\n",
      "best mean reward 89.014012\n",
      "running time 1129.255916\n",
      "Train_EnvstepsSoFar : 306001\n",
      "Train_AverageReturn : 83.75518882273862\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1129.2559156417847\n",
      "Training Loss : 0.8178779482841492\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 307001\n",
      "mean reward (100 episodes) 81.719969\n",
      "best mean reward 89.014012\n",
      "running time 1133.233279\n",
      "Train_EnvstepsSoFar : 307001\n",
      "Train_AverageReturn : 81.71996885597115\n",
      "Train_BestReturn : 89.01401176044608\n",
      "TimeSinceStart : 1133.2332789897919\n",
      "Training Loss : 0.41026243567466736\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 308001\n",
      "mean reward (100 episodes) 90.809414\n",
      "best mean reward 90.809414\n",
      "running time 1135.919231\n",
      "Train_EnvstepsSoFar : 308001\n",
      "Train_AverageReturn : 90.80941416867772\n",
      "Train_BestReturn : 90.80941416867772\n",
      "TimeSinceStart : 1135.9192309379578\n",
      "Training Loss : 0.2878135144710541\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 309001\n",
      "mean reward (100 episodes) 98.979857\n",
      "best mean reward 98.979857\n",
      "running time 1138.637085\n",
      "Train_EnvstepsSoFar : 309001\n",
      "Train_AverageReturn : 98.9798573945028\n",
      "Train_BestReturn : 98.9798573945028\n",
      "TimeSinceStart : 1138.6370849609375\n",
      "Training Loss : 0.8390927314758301\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 310001\n",
      "mean reward (100 episodes) 101.122737\n",
      "best mean reward 101.122737\n",
      "running time 1141.506762\n",
      "Train_EnvstepsSoFar : 310001\n",
      "Train_AverageReturn : 101.12273676711361\n",
      "Train_BestReturn : 101.12273676711361\n",
      "TimeSinceStart : 1141.5067620277405\n",
      "Training Loss : 3.216174602508545\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 311001\n",
      "mean reward (100 episodes) 103.684700\n",
      "best mean reward 103.684700\n",
      "running time 1144.224877\n",
      "Train_EnvstepsSoFar : 311001\n",
      "Train_AverageReturn : 103.6846999219294\n",
      "Train_BestReturn : 103.6846999219294\n",
      "TimeSinceStart : 1144.2248768806458\n",
      "Training Loss : 0.49393221735954285\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 312001\n",
      "mean reward (100 episodes) 112.713147\n",
      "best mean reward 112.713147\n",
      "running time 1146.933320\n",
      "Train_EnvstepsSoFar : 312001\n",
      "Train_AverageReturn : 112.71314682673022\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1146.9333198070526\n",
      "Training Loss : 1.6843910217285156\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 313001\n",
      "mean reward (100 episodes) 110.852238\n",
      "best mean reward 112.713147\n",
      "running time 1149.678474\n",
      "Train_EnvstepsSoFar : 313001\n",
      "Train_AverageReturn : 110.85223842073367\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1149.6784739494324\n",
      "Training Loss : 0.4636892080307007\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 314001\n",
      "mean reward (100 episodes) 104.576176\n",
      "best mean reward 112.713147\n",
      "running time 1152.521310\n",
      "Train_EnvstepsSoFar : 314001\n",
      "Train_AverageReturn : 104.57617619687753\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1152.5213098526\n",
      "Training Loss : 0.1360509991645813\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 315001\n",
      "mean reward (100 episodes) 105.499687\n",
      "best mean reward 112.713147\n",
      "running time 1155.237777\n",
      "Train_EnvstepsSoFar : 315001\n",
      "Train_AverageReturn : 105.49968711065424\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1155.2377767562866\n",
      "Training Loss : 1.2067039012908936\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 316001\n",
      "mean reward (100 episodes) 101.501666\n",
      "best mean reward 112.713147\n",
      "running time 1157.964050\n",
      "Train_EnvstepsSoFar : 316001\n",
      "Train_AverageReturn : 101.50166637702694\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1157.9640498161316\n",
      "Training Loss : 0.1402624100446701\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 317001\n",
      "mean reward (100 episodes) 110.116006\n",
      "best mean reward 112.713147\n",
      "running time 1160.723793\n",
      "Train_EnvstepsSoFar : 317001\n",
      "Train_AverageReturn : 110.11600648983908\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1160.7237927913666\n",
      "Training Loss : 0.46354901790618896\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 318001\n",
      "mean reward (100 episodes) 105.623993\n",
      "best mean reward 112.713147\n",
      "running time 1163.459395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 318001\n",
      "Train_AverageReturn : 105.62399265716418\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1163.4593946933746\n",
      "Training Loss : 0.2225923091173172\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 319001\n",
      "mean reward (100 episodes) 103.940965\n",
      "best mean reward 112.713147\n",
      "running time 1166.264719\n",
      "Train_EnvstepsSoFar : 319001\n",
      "Train_AverageReturn : 103.9409646337599\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1166.2647187709808\n",
      "Training Loss : 7.183232307434082\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 320001\n",
      "mean reward (100 episodes) 101.100486\n",
      "best mean reward 112.713147\n",
      "running time 1168.963505\n",
      "Train_EnvstepsSoFar : 320001\n",
      "Train_AverageReturn : 101.10048613548584\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1168.9635047912598\n",
      "Training Loss : 2.81533145904541\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 321001\n",
      "mean reward (100 episodes) 97.085015\n",
      "best mean reward 112.713147\n",
      "running time 1171.836815\n",
      "Train_EnvstepsSoFar : 321001\n",
      "Train_AverageReturn : 97.08501494277583\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1171.836814880371\n",
      "Training Loss : 0.43046650290489197\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 322001\n",
      "mean reward (100 episodes) 99.777147\n",
      "best mean reward 112.713147\n",
      "running time 1174.619395\n",
      "Train_EnvstepsSoFar : 322001\n",
      "Train_AverageReturn : 99.77714745205884\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1174.6193947792053\n",
      "Training Loss : 0.09741608053445816\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 323001\n",
      "mean reward (100 episodes) 91.912025\n",
      "best mean reward 112.713147\n",
      "running time 1177.329415\n",
      "Train_EnvstepsSoFar : 323001\n",
      "Train_AverageReturn : 91.9120254605026\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1177.329414844513\n",
      "Training Loss : 0.31867215037345886\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 324001\n",
      "mean reward (100 episodes) 90.781768\n",
      "best mean reward 112.713147\n",
      "running time 1180.131419\n",
      "Train_EnvstepsSoFar : 324001\n",
      "Train_AverageReturn : 90.78176842340366\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1180.1314189434052\n",
      "Training Loss : 0.49988478422164917\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 325001\n",
      "mean reward (100 episodes) 97.752451\n",
      "best mean reward 112.713147\n",
      "running time 1182.834945\n",
      "Train_EnvstepsSoFar : 325001\n",
      "Train_AverageReturn : 97.75245077577497\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1182.8349447250366\n",
      "Training Loss : 0.2671893239021301\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 326001\n",
      "mean reward (100 episodes) 89.428690\n",
      "best mean reward 112.713147\n",
      "running time 1186.093772\n",
      "Train_EnvstepsSoFar : 326001\n",
      "Train_AverageReturn : 89.42869029917046\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1186.0937716960907\n",
      "Training Loss : 0.8320078253746033\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 327001\n",
      "mean reward (100 episodes) 91.625163\n",
      "best mean reward 112.713147\n",
      "running time 1188.740841\n",
      "Train_EnvstepsSoFar : 327001\n",
      "Train_AverageReturn : 91.62516307305104\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1188.7408409118652\n",
      "Training Loss : 0.8458887338638306\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 328001\n",
      "mean reward (100 episodes) 86.556542\n",
      "best mean reward 112.713147\n",
      "running time 1191.473556\n",
      "Train_EnvstepsSoFar : 328001\n",
      "Train_AverageReturn : 86.55654237925745\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1191.4735560417175\n",
      "Training Loss : 0.22314171493053436\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 329001\n",
      "mean reward (100 episodes) 81.923833\n",
      "best mean reward 112.713147\n",
      "running time 1194.083903\n",
      "Train_EnvstepsSoFar : 329001\n",
      "Train_AverageReturn : 81.92383325150676\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1194.083902835846\n",
      "Training Loss : 0.13341720402240753\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 330001\n",
      "mean reward (100 episodes) 84.863847\n",
      "best mean reward 112.713147\n",
      "running time 1196.757703\n",
      "Train_EnvstepsSoFar : 330001\n",
      "Train_AverageReturn : 84.86384688374078\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1196.7577028274536\n",
      "Training Loss : 1.640856385231018\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 331001\n",
      "mean reward (100 episodes) 83.642135\n",
      "best mean reward 112.713147\n",
      "running time 1199.563655\n",
      "Train_EnvstepsSoFar : 331001\n",
      "Train_AverageReturn : 83.64213527010031\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1199.5636548995972\n",
      "Training Loss : 0.12361136823892593\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 332001\n",
      "mean reward (100 episodes) 88.628567\n",
      "best mean reward 112.713147\n",
      "running time 1202.212600\n",
      "Train_EnvstepsSoFar : 332001\n",
      "Train_AverageReturn : 88.62856702391953\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1202.2125997543335\n",
      "Training Loss : 0.813258171081543\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 333001\n",
      "mean reward (100 episodes) 84.874722\n",
      "best mean reward 112.713147\n",
      "running time 1204.894205\n",
      "Train_EnvstepsSoFar : 333001\n",
      "Train_AverageReturn : 84.87472209846365\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1204.8942048549652\n",
      "Training Loss : 0.3967094421386719\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 334001\n",
      "mean reward (100 episodes) 91.269468\n",
      "best mean reward 112.713147\n",
      "running time 1207.641600\n",
      "Train_EnvstepsSoFar : 334001\n",
      "Train_AverageReturn : 91.26946826368685\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1207.64159989357\n",
      "Training Loss : 0.3488401174545288\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 335001\n",
      "mean reward (100 episodes) 90.390801\n",
      "best mean reward 112.713147\n",
      "running time 1210.303716\n",
      "Train_EnvstepsSoFar : 335001\n",
      "Train_AverageReturn : 90.39080085005133\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1210.3037157058716\n",
      "Training Loss : 0.5414635539054871\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 336001\n",
      "mean reward (100 episodes) 91.143706\n",
      "best mean reward 112.713147\n",
      "running time 1213.168990\n",
      "Train_EnvstepsSoFar : 336001\n",
      "Train_AverageReturn : 91.14370616791835\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1213.1689898967743\n",
      "Training Loss : 4.9517598152160645\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 337001\n",
      "mean reward (100 episodes) 85.953226\n",
      "best mean reward 112.713147\n",
      "running time 1215.798314\n",
      "Train_EnvstepsSoFar : 337001\n",
      "Train_AverageReturn : 85.95322595772191\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1215.7983138561249\n",
      "Training Loss : 0.15559786558151245\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 338001\n",
      "mean reward (100 episodes) 84.768574\n",
      "best mean reward 112.713147\n",
      "running time 1218.500007\n",
      "Train_EnvstepsSoFar : 338001\n",
      "Train_AverageReturn : 84.76857393770068\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1218.5000069141388\n",
      "Training Loss : 0.40505334734916687\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 339001\n",
      "mean reward (100 episodes) 88.147854\n",
      "best mean reward 112.713147\n",
      "running time 1221.275667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 339001\n",
      "Train_AverageReturn : 88.14785415331022\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1221.2756669521332\n",
      "Training Loss : 0.20074470341205597\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 340001\n",
      "mean reward (100 episodes) 79.587139\n",
      "best mean reward 112.713147\n",
      "running time 1224.263263\n",
      "Train_EnvstepsSoFar : 340001\n",
      "Train_AverageReturn : 79.58713893242697\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1224.2632629871368\n",
      "Training Loss : 0.23371581733226776\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 341001\n",
      "mean reward (100 episodes) 78.476098\n",
      "best mean reward 112.713147\n",
      "running time 1227.150803\n",
      "Train_EnvstepsSoFar : 341001\n",
      "Train_AverageReturn : 78.47609804226025\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1227.1508028507233\n",
      "Training Loss : 0.4239997863769531\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 342001\n",
      "mean reward (100 episodes) 79.819409\n",
      "best mean reward 112.713147\n",
      "running time 1230.084041\n",
      "Train_EnvstepsSoFar : 342001\n",
      "Train_AverageReturn : 79.81940895926341\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1230.0840408802032\n",
      "Training Loss : 0.5786288976669312\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 343001\n",
      "mean reward (100 episodes) 84.951409\n",
      "best mean reward 112.713147\n",
      "running time 1232.786026\n",
      "Train_EnvstepsSoFar : 343001\n",
      "Train_AverageReturn : 84.95140948049753\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1232.7860260009766\n",
      "Training Loss : 0.2550514340400696\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 344001\n",
      "mean reward (100 episodes) 87.964875\n",
      "best mean reward 112.713147\n",
      "running time 1235.538866\n",
      "Train_EnvstepsSoFar : 344001\n",
      "Train_AverageReturn : 87.96487462786492\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1235.5388658046722\n",
      "Training Loss : 0.2062116116285324\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 345001\n",
      "mean reward (100 episodes) 92.689825\n",
      "best mean reward 112.713147\n",
      "running time 1238.380213\n",
      "Train_EnvstepsSoFar : 345001\n",
      "Train_AverageReturn : 92.68982519687674\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1238.380213022232\n",
      "Training Loss : 0.5894222259521484\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 346001\n",
      "mean reward (100 episodes) 88.320334\n",
      "best mean reward 112.713147\n",
      "running time 1241.035921\n",
      "Train_EnvstepsSoFar : 346001\n",
      "Train_AverageReturn : 88.3203337747316\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1241.0359208583832\n",
      "Training Loss : 0.25825297832489014\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 347001\n",
      "mean reward (100 episodes) 85.891858\n",
      "best mean reward 112.713147\n",
      "running time 1243.998899\n",
      "Train_EnvstepsSoFar : 347001\n",
      "Train_AverageReturn : 85.89185770857303\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1243.9988989830017\n",
      "Training Loss : 0.27525362372398376\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 348001\n",
      "mean reward (100 episodes) 77.378064\n",
      "best mean reward 112.713147\n",
      "running time 1246.718196\n",
      "Train_EnvstepsSoFar : 348001\n",
      "Train_AverageReturn : 77.37806430763098\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1246.7181959152222\n",
      "Training Loss : 0.7704454660415649\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 349001\n",
      "mean reward (100 episodes) 74.199184\n",
      "best mean reward 112.713147\n",
      "running time 1249.461163\n",
      "Train_EnvstepsSoFar : 349001\n",
      "Train_AverageReturn : 74.19918422337354\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1249.4611630439758\n",
      "Training Loss : 0.3157344460487366\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 350001\n",
      "mean reward (100 episodes) 70.357857\n",
      "best mean reward 112.713147\n",
      "running time 1252.105901\n",
      "Train_EnvstepsSoFar : 350001\n",
      "Train_AverageReturn : 70.35785699421777\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1252.1059007644653\n",
      "Training Loss : 3.483119249343872\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 351001\n",
      "mean reward (100 episodes) 54.520392\n",
      "best mean reward 112.713147\n",
      "running time 1254.702823\n",
      "Train_EnvstepsSoFar : 351001\n",
      "Train_AverageReturn : 54.520392187630684\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1254.7028229236603\n",
      "Training Loss : 0.38694876432418823\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 352001\n",
      "mean reward (100 episodes) 50.812536\n",
      "best mean reward 112.713147\n",
      "running time 1257.402768\n",
      "Train_EnvstepsSoFar : 352001\n",
      "Train_AverageReturn : 50.812536052161406\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1257.4027678966522\n",
      "Training Loss : 0.24692362546920776\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 353001\n",
      "mean reward (100 episodes) 57.432178\n",
      "best mean reward 112.713147\n",
      "running time 1260.085392\n",
      "Train_EnvstepsSoFar : 353001\n",
      "Train_AverageReturn : 57.43217819827796\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1260.0853917598724\n",
      "Training Loss : 1.4792615175247192\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 354001\n",
      "mean reward (100 episodes) 62.744686\n",
      "best mean reward 112.713147\n",
      "running time 1262.797261\n",
      "Train_EnvstepsSoFar : 354001\n",
      "Train_AverageReturn : 62.744685954228736\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1262.797260761261\n",
      "Training Loss : 1.15949285030365\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 355001\n",
      "mean reward (100 episodes) 58.050840\n",
      "best mean reward 112.713147\n",
      "running time 1265.389358\n",
      "Train_EnvstepsSoFar : 355001\n",
      "Train_AverageReturn : 58.050839505453844\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1265.389357805252\n",
      "Training Loss : 0.5064264535903931\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 356001\n",
      "mean reward (100 episodes) 47.375421\n",
      "best mean reward 112.713147\n",
      "running time 1268.007440\n",
      "Train_EnvstepsSoFar : 356001\n",
      "Train_AverageReturn : 47.37542109559717\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1268.0074398517609\n",
      "Training Loss : 0.29634296894073486\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 357001\n",
      "mean reward (100 episodes) 41.496412\n",
      "best mean reward 112.713147\n",
      "running time 1270.612171\n",
      "Train_EnvstepsSoFar : 357001\n",
      "Train_AverageReturn : 41.49641187394016\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1270.6121706962585\n",
      "Training Loss : 1.026444673538208\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 358001\n",
      "mean reward (100 episodes) 38.594715\n",
      "best mean reward 112.713147\n",
      "running time 1273.396307\n",
      "Train_EnvstepsSoFar : 358001\n",
      "Train_AverageReturn : 38.59471482761992\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1273.3963069915771\n",
      "Training Loss : 1.05447256565094\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 359001\n",
      "mean reward (100 episodes) 24.458793\n",
      "best mean reward 112.713147\n",
      "running time 1276.134287\n",
      "Train_EnvstepsSoFar : 359001\n",
      "Train_AverageReturn : 24.45879257811674\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1276.1342868804932\n",
      "Training Loss : 0.4883878529071808\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 360001\n",
      "mean reward (100 episodes) 34.904933\n",
      "best mean reward 112.713147\n",
      "running time 1278.809900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 360001\n",
      "Train_AverageReturn : 34.904932591577605\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1278.8098998069763\n",
      "Training Loss : 1.3232483863830566\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 361001\n",
      "mean reward (100 episodes) 39.086737\n",
      "best mean reward 112.713147\n",
      "running time 1281.743326\n",
      "Train_EnvstepsSoFar : 361001\n",
      "Train_AverageReturn : 39.086737439560146\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1281.7433259487152\n",
      "Training Loss : 7.247232913970947\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 362001\n",
      "mean reward (100 episodes) 35.436764\n",
      "best mean reward 112.713147\n",
      "running time 1285.384439\n",
      "Train_EnvstepsSoFar : 362001\n",
      "Train_AverageReturn : 35.43676381971909\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1285.384438753128\n",
      "Training Loss : 1.2518861293792725\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 363001\n",
      "mean reward (100 episodes) 36.296436\n",
      "best mean reward 112.713147\n",
      "running time 1288.202071\n",
      "Train_EnvstepsSoFar : 363001\n",
      "Train_AverageReturn : 36.29643641923652\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1288.2020707130432\n",
      "Training Loss : 0.9430881142616272\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 364001\n",
      "mean reward (100 episodes) 31.929848\n",
      "best mean reward 112.713147\n",
      "running time 1290.920522\n",
      "Train_EnvstepsSoFar : 364001\n",
      "Train_AverageReturn : 31.929848308412975\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1290.9205219745636\n",
      "Training Loss : 3.030965805053711\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 365001\n",
      "mean reward (100 episodes) 16.477473\n",
      "best mean reward 112.713147\n",
      "running time 1293.912023\n",
      "Train_EnvstepsSoFar : 365001\n",
      "Train_AverageReturn : 16.477472908704694\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1293.9120228290558\n",
      "Training Loss : 0.9274507761001587\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 366001\n",
      "mean reward (100 episodes) 25.508780\n",
      "best mean reward 112.713147\n",
      "running time 1296.595766\n",
      "Train_EnvstepsSoFar : 366001\n",
      "Train_AverageReturn : 25.508779900890918\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1296.5957658290863\n",
      "Training Loss : 0.32040727138519287\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 367001\n",
      "mean reward (100 episodes) 31.826470\n",
      "best mean reward 112.713147\n",
      "running time 1299.668825\n",
      "Train_EnvstepsSoFar : 367001\n",
      "Train_AverageReturn : 31.82647017677114\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1299.6688249111176\n",
      "Training Loss : 0.5441266894340515\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 368001\n",
      "mean reward (100 episodes) 32.592938\n",
      "best mean reward 112.713147\n",
      "running time 1302.423905\n",
      "Train_EnvstepsSoFar : 368001\n",
      "Train_AverageReturn : 32.59293814672711\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1302.423904657364\n",
      "Training Loss : 0.13133180141448975\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 369001\n",
      "mean reward (100 episodes) 26.092113\n",
      "best mean reward 112.713147\n",
      "running time 1305.452098\n",
      "Train_EnvstepsSoFar : 369001\n",
      "Train_AverageReturn : 26.09211328185004\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1305.4520976543427\n",
      "Training Loss : 1.0980032682418823\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 370001\n",
      "mean reward (100 episodes) 34.871912\n",
      "best mean reward 112.713147\n",
      "running time 1308.151926\n",
      "Train_EnvstepsSoFar : 370001\n",
      "Train_AverageReturn : 34.87191201678254\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1308.1519258022308\n",
      "Training Loss : 0.8733453750610352\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 371001\n",
      "mean reward (100 episodes) 52.061590\n",
      "best mean reward 112.713147\n",
      "running time 1310.845050\n",
      "Train_EnvstepsSoFar : 371001\n",
      "Train_AverageReturn : 52.06159001580107\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1310.8450498580933\n",
      "Training Loss : 3.774324655532837\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 372001\n",
      "mean reward (100 episodes) 48.992854\n",
      "best mean reward 112.713147\n",
      "running time 1314.001209\n",
      "Train_EnvstepsSoFar : 372001\n",
      "Train_AverageReturn : 48.9928538170164\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1314.001208782196\n",
      "Training Loss : 0.21895477175712585\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 373001\n",
      "mean reward (100 episodes) 63.655132\n",
      "best mean reward 112.713147\n",
      "running time 1316.867014\n",
      "Train_EnvstepsSoFar : 373001\n",
      "Train_AverageReturn : 63.6551321437587\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1316.8670139312744\n",
      "Training Loss : 2.2991459369659424\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 374001\n",
      "mean reward (100 episodes) 69.218639\n",
      "best mean reward 112.713147\n",
      "running time 1319.530556\n",
      "Train_EnvstepsSoFar : 374001\n",
      "Train_AverageReturn : 69.21863946280048\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1319.5305559635162\n",
      "Training Loss : 0.16241154074668884\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 375001\n",
      "mean reward (100 episodes) 69.633840\n",
      "best mean reward 112.713147\n",
      "running time 1322.669244\n",
      "Train_EnvstepsSoFar : 375001\n",
      "Train_AverageReturn : 69.63383983877269\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1322.669243812561\n",
      "Training Loss : 2.5542757511138916\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 376001\n",
      "mean reward (100 episodes) 66.451353\n",
      "best mean reward 112.713147\n",
      "running time 1325.835173\n",
      "Train_EnvstepsSoFar : 376001\n",
      "Train_AverageReturn : 66.45135300025892\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1325.8351728916168\n",
      "Training Loss : 0.1616915911436081\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 377001\n",
      "mean reward (100 episodes) 71.161164\n",
      "best mean reward 112.713147\n",
      "running time 1330.144318\n",
      "Train_EnvstepsSoFar : 377001\n",
      "Train_AverageReturn : 71.1611644885486\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1330.1443178653717\n",
      "Training Loss : 0.17856228351593018\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 378001\n",
      "mean reward (100 episodes) 80.252700\n",
      "best mean reward 112.713147\n",
      "running time 1333.269739\n",
      "Train_EnvstepsSoFar : 378001\n",
      "Train_AverageReturn : 80.25270003268162\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1333.2697389125824\n",
      "Training Loss : 1.0176448822021484\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 379001\n",
      "mean reward (100 episodes) 87.632634\n",
      "best mean reward 112.713147\n",
      "running time 1336.293108\n",
      "Train_EnvstepsSoFar : 379001\n",
      "Train_AverageReturn : 87.63263445557276\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1336.2931079864502\n",
      "Training Loss : 0.4546336531639099\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 380001\n",
      "mean reward (100 episodes) 86.812302\n",
      "best mean reward 112.713147\n",
      "running time 1339.387573\n",
      "Train_EnvstepsSoFar : 380001\n",
      "Train_AverageReturn : 86.81230231774224\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1339.387573003769\n",
      "Training Loss : 2.190985679626465\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 381001\n",
      "mean reward (100 episodes) 88.424532\n",
      "best mean reward 112.713147\n",
      "running time 1342.066865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 381001\n",
      "Train_AverageReturn : 88.42453164274735\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1342.0668649673462\n",
      "Training Loss : 3.601830244064331\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 382001\n",
      "mean reward (100 episodes) 88.895625\n",
      "best mean reward 112.713147\n",
      "running time 1345.961121\n",
      "Train_EnvstepsSoFar : 382001\n",
      "Train_AverageReturn : 88.89562459979784\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1345.9611208438873\n",
      "Training Loss : 0.3802071809768677\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 383001\n",
      "mean reward (100 episodes) 90.887418\n",
      "best mean reward 112.713147\n",
      "running time 1349.791607\n",
      "Train_EnvstepsSoFar : 383001\n",
      "Train_AverageReturn : 90.88741772503803\n",
      "Train_BestReturn : 112.71314682673022\n",
      "TimeSinceStart : 1349.7916066646576\n",
      "Training Loss : 2.4383816719055176\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 384001\n",
      "mean reward (100 episodes) 114.868228\n",
      "best mean reward 114.868228\n",
      "running time 1352.615350\n",
      "Train_EnvstepsSoFar : 384001\n",
      "Train_AverageReturn : 114.8682277049134\n",
      "Train_BestReturn : 114.8682277049134\n",
      "TimeSinceStart : 1352.6153497695923\n",
      "Training Loss : 1.750103235244751\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 385001\n",
      "mean reward (100 episodes) 127.962716\n",
      "best mean reward 127.962716\n",
      "running time 1355.301320\n",
      "Train_EnvstepsSoFar : 385001\n",
      "Train_AverageReturn : 127.9627156402933\n",
      "Train_BestReturn : 127.9627156402933\n",
      "TimeSinceStart : 1355.3013198375702\n",
      "Training Loss : 14.018526077270508\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 386001\n",
      "mean reward (100 episodes) 114.067235\n",
      "best mean reward 127.962716\n",
      "running time 1358.024335\n",
      "Train_EnvstepsSoFar : 386001\n",
      "Train_AverageReturn : 114.06723503811722\n",
      "Train_BestReturn : 127.9627156402933\n",
      "TimeSinceStart : 1358.0243349075317\n",
      "Training Loss : 0.30244165658950806\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 387001\n",
      "mean reward (100 episodes) 121.934947\n",
      "best mean reward 127.962716\n",
      "running time 1360.754012\n",
      "Train_EnvstepsSoFar : 387001\n",
      "Train_AverageReturn : 121.93494669253263\n",
      "Train_BestReturn : 127.9627156402933\n",
      "TimeSinceStart : 1360.7540118694305\n",
      "Training Loss : 0.14618422091007233\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 388001\n",
      "mean reward (100 episodes) 123.940214\n",
      "best mean reward 127.962716\n",
      "running time 1363.475558\n",
      "Train_EnvstepsSoFar : 388001\n",
      "Train_AverageReturn : 123.9402143600652\n",
      "Train_BestReturn : 127.9627156402933\n",
      "TimeSinceStart : 1363.4755578041077\n",
      "Training Loss : 0.1583617627620697\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 389001\n",
      "mean reward (100 episodes) 124.593020\n",
      "best mean reward 127.962716\n",
      "running time 1367.162071\n",
      "Train_EnvstepsSoFar : 389001\n",
      "Train_AverageReturn : 124.59302013573695\n",
      "Train_BestReturn : 127.9627156402933\n",
      "TimeSinceStart : 1367.1620707511902\n",
      "Training Loss : 0.8364050388336182\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 390001\n",
      "mean reward (100 episodes) 114.194351\n",
      "best mean reward 127.962716\n",
      "running time 1369.999772\n",
      "Train_EnvstepsSoFar : 390001\n",
      "Train_AverageReturn : 114.19435060953916\n",
      "Train_BestReturn : 127.9627156402933\n",
      "TimeSinceStart : 1369.9997718334198\n",
      "Training Loss : 0.2611932158470154\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 391001\n",
      "mean reward (100 episodes) 115.145627\n",
      "best mean reward 127.962716\n",
      "running time 1372.960832\n",
      "Train_EnvstepsSoFar : 391001\n",
      "Train_AverageReturn : 115.14562683618449\n",
      "Train_BestReturn : 127.9627156402933\n",
      "TimeSinceStart : 1372.9608318805695\n",
      "Training Loss : 1.6431999206542969\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 392001\n",
      "mean reward (100 episodes) 110.405113\n",
      "best mean reward 127.962716\n",
      "running time 1376.099427\n",
      "Train_EnvstepsSoFar : 392001\n",
      "Train_AverageReturn : 110.40511321763188\n",
      "Train_BestReturn : 127.9627156402933\n",
      "TimeSinceStart : 1376.0994267463684\n",
      "Training Loss : 0.27443113923072815\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 393001\n",
      "mean reward (100 episodes) 112.401335\n",
      "best mean reward 127.962716\n",
      "running time 1379.227368\n",
      "Train_EnvstepsSoFar : 393001\n",
      "Train_AverageReturn : 112.4013352896251\n",
      "Train_BestReturn : 127.9627156402933\n",
      "TimeSinceStart : 1379.2273678779602\n",
      "Training Loss : 1.227945327758789\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 394001\n",
      "mean reward (100 episodes) 106.249787\n",
      "best mean reward 127.962716\n",
      "running time 1382.030220\n",
      "Train_EnvstepsSoFar : 394001\n",
      "Train_AverageReturn : 106.24978713639253\n",
      "Train_BestReturn : 127.9627156402933\n",
      "TimeSinceStart : 1382.0302197933197\n",
      "Training Loss : 0.6737594604492188\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 395001\n",
      "mean reward (100 episodes) 100.797225\n",
      "best mean reward 127.962716\n",
      "running time 1384.732822\n",
      "Train_EnvstepsSoFar : 395001\n",
      "Train_AverageReturn : 100.79722513874314\n",
      "Train_BestReturn : 127.9627156402933\n",
      "TimeSinceStart : 1384.7328217029572\n",
      "Training Loss : 0.14806798100471497\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 396001\n",
      "mean reward (100 episodes) 101.313323\n",
      "best mean reward 127.962716\n",
      "running time 1387.636759\n",
      "Train_EnvstepsSoFar : 396001\n",
      "Train_AverageReturn : 101.31332338332086\n",
      "Train_BestReturn : 127.9627156402933\n",
      "TimeSinceStart : 1387.6367588043213\n",
      "Training Loss : 0.15980519354343414\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 397001\n",
      "mean reward (100 episodes) 106.195859\n",
      "best mean reward 127.962716\n",
      "running time 1391.183429\n",
      "Train_EnvstepsSoFar : 397001\n",
      "Train_AverageReturn : 106.19585860358788\n",
      "Train_BestReturn : 127.9627156402933\n",
      "TimeSinceStart : 1391.1834287643433\n",
      "Training Loss : 0.4391099810600281\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 398001\n",
      "mean reward (100 episodes) 98.086249\n",
      "best mean reward 127.962716\n",
      "running time 1393.888734\n",
      "Train_EnvstepsSoFar : 398001\n",
      "Train_AverageReturn : 98.0862486009237\n",
      "Train_BestReturn : 127.9627156402933\n",
      "TimeSinceStart : 1393.8887338638306\n",
      "Training Loss : 1.4814815521240234\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 399001\n",
      "mean reward (100 episodes) 92.350526\n",
      "best mean reward 127.962716\n",
      "running time 1396.923126\n",
      "Train_EnvstepsSoFar : 399001\n",
      "Train_AverageReturn : 92.35052592630198\n",
      "Train_BestReturn : 127.9627156402933\n",
      "TimeSinceStart : 1396.923125743866\n",
      "Training Loss : 0.11390861123800278\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 400001\n",
      "mean reward (100 episodes) 92.050235\n",
      "best mean reward 127.962716\n",
      "running time 1399.645057\n",
      "Train_EnvstepsSoFar : 400001\n",
      "Train_AverageReturn : 92.0502352917966\n",
      "Train_BestReturn : 127.9627156402933\n",
      "TimeSinceStart : 1399.6450567245483\n",
      "Training Loss : 2.1422505378723145\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 401001\n",
      "mean reward (100 episodes) 103.919917\n",
      "best mean reward 127.962716\n",
      "running time 1402.466218\n",
      "Train_EnvstepsSoFar : 401001\n",
      "Train_AverageReturn : 103.91991669658431\n",
      "Train_BestReturn : 127.9627156402933\n",
      "TimeSinceStart : 1402.46621799469\n",
      "Training Loss : 0.6887362003326416\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 402001\n",
      "mean reward (100 episodes) 105.394487\n",
      "best mean reward 127.962716\n",
      "running time 1405.410737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 402001\n",
      "Train_AverageReturn : 105.39448679018633\n",
      "Train_BestReturn : 127.9627156402933\n",
      "TimeSinceStart : 1405.41073679924\n",
      "Training Loss : 1.843034267425537\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 403001\n",
      "mean reward (100 episodes) 105.721719\n",
      "best mean reward 127.962716\n",
      "running time 1408.320566\n",
      "Train_EnvstepsSoFar : 403001\n",
      "Train_AverageReturn : 105.72171945828221\n",
      "Train_BestReturn : 127.9627156402933\n",
      "TimeSinceStart : 1408.320565700531\n",
      "Training Loss : 0.8238515853881836\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 404001\n",
      "mean reward (100 episodes) 105.263671\n",
      "best mean reward 127.962716\n",
      "running time 1411.509156\n",
      "Train_EnvstepsSoFar : 404001\n",
      "Train_AverageReturn : 105.26367118277896\n",
      "Train_BestReturn : 127.9627156402933\n",
      "TimeSinceStart : 1411.5091557502747\n",
      "Training Loss : 0.23168708384037018\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 405001\n",
      "mean reward (100 episodes) 100.571375\n",
      "best mean reward 127.962716\n",
      "running time 1414.237002\n",
      "Train_EnvstepsSoFar : 405001\n",
      "Train_AverageReturn : 100.57137511668844\n",
      "Train_BestReturn : 127.9627156402933\n",
      "TimeSinceStart : 1414.2370018959045\n",
      "Training Loss : 0.4936002790927887\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 406001\n",
      "mean reward (100 episodes) 107.771155\n",
      "best mean reward 127.962716\n",
      "running time 1416.928677\n",
      "Train_EnvstepsSoFar : 406001\n",
      "Train_AverageReturn : 107.77115460008268\n",
      "Train_BestReturn : 127.9627156402933\n",
      "TimeSinceStart : 1416.9286768436432\n",
      "Training Loss : 0.4125468134880066\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 407001\n",
      "mean reward (100 episodes) 113.251947\n",
      "best mean reward 127.962716\n",
      "running time 1419.641493\n",
      "Train_EnvstepsSoFar : 407001\n",
      "Train_AverageReturn : 113.25194705366467\n",
      "Train_BestReturn : 127.9627156402933\n",
      "TimeSinceStart : 1419.641492843628\n",
      "Training Loss : 2.4471476078033447\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 408001\n",
      "mean reward (100 episodes) 109.079143\n",
      "best mean reward 127.962716\n",
      "running time 1422.379415\n",
      "Train_EnvstepsSoFar : 408001\n",
      "Train_AverageReturn : 109.07914250315993\n",
      "Train_BestReturn : 127.9627156402933\n",
      "TimeSinceStart : 1422.3794147968292\n",
      "Training Loss : 0.13563038408756256\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 409001\n",
      "mean reward (100 episodes) 104.441500\n",
      "best mean reward 127.962716\n",
      "running time 1425.273572\n",
      "Train_EnvstepsSoFar : 409001\n",
      "Train_AverageReturn : 104.44149969894038\n",
      "Train_BestReturn : 127.9627156402933\n",
      "TimeSinceStart : 1425.27357172966\n",
      "Training Loss : 0.38422533869743347\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 410001\n",
      "mean reward (100 episodes) 103.844513\n",
      "best mean reward 127.962716\n",
      "running time 1428.024717\n",
      "Train_EnvstepsSoFar : 410001\n",
      "Train_AverageReturn : 103.84451325168409\n",
      "Train_BestReturn : 127.9627156402933\n",
      "TimeSinceStart : 1428.0247168540955\n",
      "Training Loss : 0.2180752009153366\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 411001\n",
      "mean reward (100 episodes) 108.545566\n",
      "best mean reward 127.962716\n",
      "running time 1430.775257\n",
      "Train_EnvstepsSoFar : 411001\n",
      "Train_AverageReturn : 108.54556633255575\n",
      "Train_BestReturn : 127.9627156402933\n",
      "TimeSinceStart : 1430.7752568721771\n",
      "Training Loss : 0.49708476662635803\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 412001\n",
      "mean reward (100 episodes) 112.112854\n",
      "best mean reward 127.962716\n",
      "running time 1433.643852\n",
      "Train_EnvstepsSoFar : 412001\n",
      "Train_AverageReturn : 112.11285408251219\n",
      "Train_BestReturn : 127.9627156402933\n",
      "TimeSinceStart : 1433.6438517570496\n",
      "Training Loss : 1.4196724891662598\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 413001\n",
      "mean reward (100 episodes) 115.860478\n",
      "best mean reward 127.962716\n",
      "running time 1436.401905\n",
      "Train_EnvstepsSoFar : 413001\n",
      "Train_AverageReturn : 115.86047836299959\n",
      "Train_BestReturn : 127.9627156402933\n",
      "TimeSinceStart : 1436.4019048213959\n",
      "Training Loss : 1.6805306673049927\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 414001\n",
      "mean reward (100 episodes) 122.023953\n",
      "best mean reward 127.962716\n",
      "running time 1439.084472\n",
      "Train_EnvstepsSoFar : 414001\n",
      "Train_AverageReturn : 122.02395252210839\n",
      "Train_BestReturn : 127.9627156402933\n",
      "TimeSinceStart : 1439.0844717025757\n",
      "Training Loss : 0.1951831728219986\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 415001\n",
      "mean reward (100 episodes) 127.370998\n",
      "best mean reward 127.962716\n",
      "running time 1441.980134\n",
      "Train_EnvstepsSoFar : 415001\n",
      "Train_AverageReturn : 127.37099780868618\n",
      "Train_BestReturn : 127.9627156402933\n",
      "TimeSinceStart : 1441.9801337718964\n",
      "Training Loss : 0.2820699214935303\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 416001\n",
      "mean reward (100 episodes) 124.358625\n",
      "best mean reward 127.962716\n",
      "running time 1446.218412\n",
      "Train_EnvstepsSoFar : 416001\n",
      "Train_AverageReturn : 124.35862518521934\n",
      "Train_BestReturn : 127.9627156402933\n",
      "TimeSinceStart : 1446.2184116840363\n",
      "Training Loss : 0.18719534575939178\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 417001\n",
      "mean reward (100 episodes) 124.900801\n",
      "best mean reward 127.962716\n",
      "running time 1448.951318\n",
      "Train_EnvstepsSoFar : 417001\n",
      "Train_AverageReturn : 124.90080110136131\n",
      "Train_BestReturn : 127.9627156402933\n",
      "TimeSinceStart : 1448.9513177871704\n",
      "Training Loss : 0.3090914487838745\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 418001\n",
      "mean reward (100 episodes) 122.053693\n",
      "best mean reward 127.962716\n",
      "running time 1451.693227\n",
      "Train_EnvstepsSoFar : 418001\n",
      "Train_AverageReturn : 122.0536929497914\n",
      "Train_BestReturn : 127.9627156402933\n",
      "TimeSinceStart : 1451.6932270526886\n",
      "Training Loss : 0.22564400732517242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 419001\n",
      "mean reward (100 episodes) 122.331909\n",
      "best mean reward 127.962716\n",
      "running time 1454.406779\n",
      "Train_EnvstepsSoFar : 419001\n",
      "Train_AverageReturn : 122.33190881092679\n",
      "Train_BestReturn : 127.9627156402933\n",
      "TimeSinceStart : 1454.4067788124084\n",
      "Training Loss : 0.7943065762519836\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 420001\n",
      "mean reward (100 episodes) 120.880166\n",
      "best mean reward 127.962716\n",
      "running time 1457.168303\n",
      "Train_EnvstepsSoFar : 420001\n",
      "Train_AverageReturn : 120.88016618112226\n",
      "Train_BestReturn : 127.9627156402933\n",
      "TimeSinceStart : 1457.1683027744293\n",
      "Training Loss : 0.24388182163238525\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 421001\n",
      "mean reward (100 episodes) 132.893458\n",
      "best mean reward 132.893458\n",
      "running time 1459.870166\n",
      "Train_EnvstepsSoFar : 421001\n",
      "Train_AverageReturn : 132.89345823039\n",
      "Train_BestReturn : 132.89345823039\n",
      "TimeSinceStart : 1459.8701658248901\n",
      "Training Loss : 0.2980422079563141\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 422001\n",
      "mean reward (100 episodes) 128.009282\n",
      "best mean reward 132.893458\n",
      "running time 1462.584542\n",
      "Train_EnvstepsSoFar : 422001\n",
      "Train_AverageReturn : 128.00928179884568\n",
      "Train_BestReturn : 132.89345823039\n",
      "TimeSinceStart : 1462.5845420360565\n",
      "Training Loss : 0.20964020490646362\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 423001\n",
      "mean reward (100 episodes) 128.232293\n",
      "best mean reward 132.893458\n",
      "running time 1465.240295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 423001\n",
      "Train_AverageReturn : 128.2322932155743\n",
      "Train_BestReturn : 132.89345823039\n",
      "TimeSinceStart : 1465.240294933319\n",
      "Training Loss : 0.3722228407859802\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 424001\n",
      "mean reward (100 episodes) 128.321762\n",
      "best mean reward 132.893458\n",
      "running time 1468.685095\n",
      "Train_EnvstepsSoFar : 424001\n",
      "Train_AverageReturn : 128.32176211996867\n",
      "Train_BestReturn : 132.89345823039\n",
      "TimeSinceStart : 1468.685094833374\n",
      "Training Loss : 10.197915077209473\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 425001\n",
      "mean reward (100 episodes) 137.702542\n",
      "best mean reward 137.702542\n",
      "running time 1471.393197\n",
      "Train_EnvstepsSoFar : 425001\n",
      "Train_AverageReturn : 137.7025422102559\n",
      "Train_BestReturn : 137.7025422102559\n",
      "TimeSinceStart : 1471.3931968212128\n",
      "Training Loss : 0.19317761063575745\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 426001\n",
      "mean reward (100 episodes) 146.309857\n",
      "best mean reward 146.309857\n",
      "running time 1474.101688\n",
      "Train_EnvstepsSoFar : 426001\n",
      "Train_AverageReturn : 146.30985689334338\n",
      "Train_BestReturn : 146.30985689334338\n",
      "TimeSinceStart : 1474.101687669754\n",
      "Training Loss : 0.18047502636909485\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 427001\n",
      "mean reward (100 episodes) 148.988081\n",
      "best mean reward 148.988081\n",
      "running time 1476.802743\n",
      "Train_EnvstepsSoFar : 427001\n",
      "Train_AverageReturn : 148.98808063152674\n",
      "Train_BestReturn : 148.98808063152674\n",
      "TimeSinceStart : 1476.8027427196503\n",
      "Training Loss : 1.3227076530456543\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 428001\n",
      "mean reward (100 episodes) 158.981212\n",
      "best mean reward 158.981212\n",
      "running time 1479.634970\n",
      "Train_EnvstepsSoFar : 428001\n",
      "Train_AverageReturn : 158.98121240261554\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1479.6349697113037\n",
      "Training Loss : 1.4028719663619995\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 429001\n",
      "mean reward (100 episodes) 148.401347\n",
      "best mean reward 158.981212\n",
      "running time 1482.308363\n",
      "Train_EnvstepsSoFar : 429001\n",
      "Train_AverageReturn : 148.4013465118885\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1482.3083627223969\n",
      "Training Loss : 0.7529225945472717\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 430001\n",
      "mean reward (100 episodes) 142.205017\n",
      "best mean reward 158.981212\n",
      "running time 1485.017507\n",
      "Train_EnvstepsSoFar : 430001\n",
      "Train_AverageReturn : 142.20501688521753\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1485.0175068378448\n",
      "Training Loss : 7.681024551391602\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 431001\n",
      "mean reward (100 episodes) 140.706065\n",
      "best mean reward 158.981212\n",
      "running time 1487.668383\n",
      "Train_EnvstepsSoFar : 431001\n",
      "Train_AverageReturn : 140.7060654065142\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1487.668382883072\n",
      "Training Loss : 0.4105575382709503\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 432001\n",
      "mean reward (100 episodes) 149.555475\n",
      "best mean reward 158.981212\n",
      "running time 1490.400331\n",
      "Train_EnvstepsSoFar : 432001\n",
      "Train_AverageReturn : 149.55547530351782\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1490.4003307819366\n",
      "Training Loss : 1.5253853797912598\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 433001\n",
      "mean reward (100 episodes) 154.582117\n",
      "best mean reward 158.981212\n",
      "running time 1493.119784\n",
      "Train_EnvstepsSoFar : 433001\n",
      "Train_AverageReturn : 154.58211680534703\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1493.1197838783264\n",
      "Training Loss : 0.3202928304672241\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 434001\n",
      "mean reward (100 episodes) 151.704249\n",
      "best mean reward 158.981212\n",
      "running time 1495.800219\n",
      "Train_EnvstepsSoFar : 434001\n",
      "Train_AverageReturn : 151.70424922754574\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1495.800218820572\n",
      "Training Loss : 0.388761043548584\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 435001\n",
      "mean reward (100 episodes) 137.819398\n",
      "best mean reward 158.981212\n",
      "running time 1498.489146\n",
      "Train_EnvstepsSoFar : 435001\n",
      "Train_AverageReturn : 137.81939801302056\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1498.4891457557678\n",
      "Training Loss : 2.719614267349243\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 436000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 436001\n",
      "mean reward (100 episodes) 128.438257\n",
      "best mean reward 158.981212\n",
      "running time 1501.199878\n",
      "Train_EnvstepsSoFar : 436001\n",
      "Train_AverageReturn : 128.43825685899617\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1501.1998777389526\n",
      "Training Loss : 2.1969008445739746\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 437001\n",
      "mean reward (100 episodes) 124.955658\n",
      "best mean reward 158.981212\n",
      "running time 1503.840981\n",
      "Train_EnvstepsSoFar : 437001\n",
      "Train_AverageReturn : 124.95565827793774\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1503.8409810066223\n",
      "Training Loss : 0.5410494208335876\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 438001\n",
      "mean reward (100 episodes) 118.781576\n",
      "best mean reward 158.981212\n",
      "running time 1506.515748\n",
      "Train_EnvstepsSoFar : 438001\n",
      "Train_AverageReturn : 118.78157641193474\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1506.5157477855682\n",
      "Training Loss : 0.39852815866470337\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 439001\n",
      "mean reward (100 episodes) 114.748683\n",
      "best mean reward 158.981212\n",
      "running time 1509.182547\n",
      "Train_EnvstepsSoFar : 439001\n",
      "Train_AverageReturn : 114.74868333894997\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1509.1825468540192\n",
      "Training Loss : 0.3293035328388214\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 440001\n",
      "mean reward (100 episodes) 103.918561\n",
      "best mean reward 158.981212\n",
      "running time 1511.889982\n",
      "Train_EnvstepsSoFar : 440001\n",
      "Train_AverageReturn : 103.91856138510087\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1511.8899819850922\n",
      "Training Loss : 6.288867950439453\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 441001\n",
      "mean reward (100 episodes) 93.699610\n",
      "best mean reward 158.981212\n",
      "running time 1514.535579\n",
      "Train_EnvstepsSoFar : 441001\n",
      "Train_AverageReturn : 93.6996095667194\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1514.5355787277222\n",
      "Training Loss : 0.8570471405982971\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 442001\n",
      "mean reward (100 episodes) 94.113036\n",
      "best mean reward 158.981212\n",
      "running time 1517.216249\n",
      "Train_EnvstepsSoFar : 442001\n",
      "Train_AverageReturn : 94.11303596990325\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1517.2162487506866\n",
      "Training Loss : 0.33495286107063293\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 443001\n",
      "mean reward (100 episodes) 92.209011\n",
      "best mean reward 158.981212\n",
      "running time 1519.826590\n",
      "Train_EnvstepsSoFar : 443001\n",
      "Train_AverageReturn : 92.20901063624301\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1519.8265898227692\n",
      "Training Loss : 1.3010351657867432\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 444001\n",
      "mean reward (100 episodes) 89.193650\n",
      "best mean reward 158.981212\n",
      "running time 1522.478360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 444001\n",
      "Train_AverageReturn : 89.19364956887993\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1522.4783596992493\n",
      "Training Loss : 0.24663697183132172\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 445001\n",
      "mean reward (100 episodes) 77.484938\n",
      "best mean reward 158.981212\n",
      "running time 1525.146244\n",
      "Train_EnvstepsSoFar : 445001\n",
      "Train_AverageReturn : 77.4849383083648\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1525.1462438106537\n",
      "Training Loss : 0.1818065196275711\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 446001\n",
      "mean reward (100 episodes) 73.624598\n",
      "best mean reward 158.981212\n",
      "running time 1527.869932\n",
      "Train_EnvstepsSoFar : 446001\n",
      "Train_AverageReturn : 73.62459800170214\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1527.8699316978455\n",
      "Training Loss : 0.2838208079338074\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 447001\n",
      "mean reward (100 episodes) 70.251392\n",
      "best mean reward 158.981212\n",
      "running time 1531.604707\n",
      "Train_EnvstepsSoFar : 447001\n",
      "Train_AverageReturn : 70.25139226388059\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1531.6047067642212\n",
      "Training Loss : 1.1818326711654663\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 448001\n",
      "mean reward (100 episodes) 66.757632\n",
      "best mean reward 158.981212\n",
      "running time 1534.428224\n",
      "Train_EnvstepsSoFar : 448001\n",
      "Train_AverageReturn : 66.75763213254723\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1534.428223848343\n",
      "Training Loss : 0.24900329113006592\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 449001\n",
      "mean reward (100 episodes) 68.075454\n",
      "best mean reward 158.981212\n",
      "running time 1537.098540\n",
      "Train_EnvstepsSoFar : 449001\n",
      "Train_AverageReturn : 68.07545424465143\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1537.0985398292542\n",
      "Training Loss : 0.2911578118801117\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 450001\n",
      "mean reward (100 episodes) 68.207692\n",
      "best mean reward 158.981212\n",
      "running time 1539.724956\n",
      "Train_EnvstepsSoFar : 450001\n",
      "Train_AverageReturn : 68.20769174771252\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1539.7249557971954\n",
      "Training Loss : 0.23288141191005707\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 451001\n",
      "mean reward (100 episodes) 59.751819\n",
      "best mean reward 158.981212\n",
      "running time 1542.376899\n",
      "Train_EnvstepsSoFar : 451001\n",
      "Train_AverageReturn : 59.75181855693569\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1542.376898765564\n",
      "Training Loss : 0.44982054829597473\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 452001\n",
      "mean reward (100 episodes) 56.327657\n",
      "best mean reward 158.981212\n",
      "running time 1545.084633\n",
      "Train_EnvstepsSoFar : 452001\n",
      "Train_AverageReturn : 56.327657190779135\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1545.0846328735352\n",
      "Training Loss : 0.5220357179641724\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 453001\n",
      "mean reward (100 episodes) 54.047817\n",
      "best mean reward 158.981212\n",
      "running time 1547.750004\n",
      "Train_EnvstepsSoFar : 453001\n",
      "Train_AverageReturn : 54.04781681218004\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1547.7500038146973\n",
      "Training Loss : 0.156889870762825\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 454001\n",
      "mean reward (100 episodes) 55.048149\n",
      "best mean reward 158.981212\n",
      "running time 1550.393548\n",
      "Train_EnvstepsSoFar : 454001\n",
      "Train_AverageReturn : 55.04814864448028\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1550.3935477733612\n",
      "Training Loss : 0.35099539160728455\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 455001\n",
      "mean reward (100 episodes) 53.285899\n",
      "best mean reward 158.981212\n",
      "running time 1553.013695\n",
      "Train_EnvstepsSoFar : 455001\n",
      "Train_AverageReturn : 53.28589901090917\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1553.0136950016022\n",
      "Training Loss : 0.4760085642337799\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 456001\n",
      "mean reward (100 episodes) 59.673155\n",
      "best mean reward 158.981212\n",
      "running time 1555.679228\n",
      "Train_EnvstepsSoFar : 456001\n",
      "Train_AverageReturn : 59.673155159510046\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1555.6792278289795\n",
      "Training Loss : 0.4078405797481537\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 457001\n",
      "mean reward (100 episodes) 60.042612\n",
      "best mean reward 158.981212\n",
      "running time 1558.409400\n",
      "Train_EnvstepsSoFar : 457001\n",
      "Train_AverageReturn : 60.042612225738786\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1558.4093997478485\n",
      "Training Loss : 1.4952536821365356\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 458001\n",
      "mean reward (100 episodes) 55.800740\n",
      "best mean reward 158.981212\n",
      "running time 1561.144989\n",
      "Train_EnvstepsSoFar : 458001\n",
      "Train_AverageReturn : 55.80074004592232\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1561.1449887752533\n",
      "Training Loss : 0.22849565744400024\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 459001\n",
      "mean reward (100 episodes) 58.404591\n",
      "best mean reward 158.981212\n",
      "running time 1564.328488\n",
      "Train_EnvstepsSoFar : 459001\n",
      "Train_AverageReturn : 58.40459095373546\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1564.3284878730774\n",
      "Training Loss : 0.18053992092609406\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 460001\n",
      "mean reward (100 episodes) 41.437528\n",
      "best mean reward 158.981212\n",
      "running time 1567.074090\n",
      "Train_EnvstepsSoFar : 460001\n",
      "Train_AverageReturn : 41.43752756858803\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1567.0740900039673\n",
      "Training Loss : 1.9274553060531616\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 461001\n",
      "mean reward (100 episodes) 41.002496\n",
      "best mean reward 158.981212\n",
      "running time 1569.736059\n",
      "Train_EnvstepsSoFar : 461001\n",
      "Train_AverageReturn : 41.00249563347396\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1569.7360589504242\n",
      "Training Loss : 0.23683905601501465\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 462001\n",
      "mean reward (100 episodes) 41.732484\n",
      "best mean reward 158.981212\n",
      "running time 1572.392963\n",
      "Train_EnvstepsSoFar : 462001\n",
      "Train_AverageReturn : 41.732484061558786\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1572.392962694168\n",
      "Training Loss : 0.18163470923900604\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 463001\n",
      "mean reward (100 episodes) 48.830282\n",
      "best mean reward 158.981212\n",
      "running time 1575.068788\n",
      "Train_EnvstepsSoFar : 463001\n",
      "Train_AverageReturn : 48.830281758120336\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1575.0687880516052\n",
      "Training Loss : 0.9692665934562683\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 464001\n",
      "mean reward (100 episodes) 63.373618\n",
      "best mean reward 158.981212\n",
      "running time 1577.713865\n",
      "Train_EnvstepsSoFar : 464001\n",
      "Train_AverageReturn : 63.37361755134184\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1577.7138650417328\n",
      "Training Loss : 1.4374456405639648\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 465001\n",
      "mean reward (100 episodes) 66.038854\n",
      "best mean reward 158.981212\n",
      "running time 1580.338974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 465001\n",
      "Train_AverageReturn : 66.03885417814566\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1580.3389739990234\n",
      "Training Loss : 0.4711619019508362\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 466001\n",
      "mean reward (100 episodes) 50.793055\n",
      "best mean reward 158.981212\n",
      "running time 1582.932277\n",
      "Train_EnvstepsSoFar : 466001\n",
      "Train_AverageReturn : 50.79305548195736\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1582.9322769641876\n",
      "Training Loss : 0.24079059064388275\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 467001\n",
      "mean reward (100 episodes) 50.706680\n",
      "best mean reward 158.981212\n",
      "running time 1585.597980\n",
      "Train_EnvstepsSoFar : 467001\n",
      "Train_AverageReturn : 50.706680242039404\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1585.5979797840118\n",
      "Training Loss : 2.2208917140960693\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 468001\n",
      "mean reward (100 episodes) 44.863564\n",
      "best mean reward 158.981212\n",
      "running time 1588.227571\n",
      "Train_EnvstepsSoFar : 468001\n",
      "Train_AverageReturn : 44.863564238010184\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1588.2275710105896\n",
      "Training Loss : 0.26188579201698303\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 469001\n",
      "mean reward (100 episodes) 44.485950\n",
      "best mean reward 158.981212\n",
      "running time 1590.879109\n",
      "Train_EnvstepsSoFar : 469001\n",
      "Train_AverageReturn : 44.48595042066232\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1590.8791089057922\n",
      "Training Loss : 0.8252060413360596\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 470001\n",
      "mean reward (100 episodes) 34.254669\n",
      "best mean reward 158.981212\n",
      "running time 1593.703278\n",
      "Train_EnvstepsSoFar : 470001\n",
      "Train_AverageReturn : 34.2546685519877\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1593.7032778263092\n",
      "Training Loss : 0.4179913103580475\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 471001\n",
      "mean reward (100 episodes) 18.922126\n",
      "best mean reward 158.981212\n",
      "running time 1596.343264\n",
      "Train_EnvstepsSoFar : 471001\n",
      "Train_AverageReturn : 18.922126186450736\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1596.3432638645172\n",
      "Training Loss : 1.3346118927001953\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 472001\n",
      "mean reward (100 episodes) 21.022986\n",
      "best mean reward 158.981212\n",
      "running time 1599.112853\n",
      "Train_EnvstepsSoFar : 472001\n",
      "Train_AverageReturn : 21.022986329278538\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1599.112853050232\n",
      "Training Loss : 2.258699893951416\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 473001\n",
      "mean reward (100 episodes) 34.553988\n",
      "best mean reward 158.981212\n",
      "running time 1601.791542\n",
      "Train_EnvstepsSoFar : 473001\n",
      "Train_AverageReturn : 34.55398792064363\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1601.791541814804\n",
      "Training Loss : 0.2971397638320923\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 474001\n",
      "mean reward (100 episodes) 33.967615\n",
      "best mean reward 158.981212\n",
      "running time 1604.443584\n",
      "Train_EnvstepsSoFar : 474001\n",
      "Train_AverageReturn : 33.967614714289354\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1604.4435839653015\n",
      "Training Loss : 0.9550072550773621\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 475001\n",
      "mean reward (100 episodes) 31.588674\n",
      "best mean reward 158.981212\n",
      "running time 1607.174937\n",
      "Train_EnvstepsSoFar : 475001\n",
      "Train_AverageReturn : 31.588673885799164\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1607.1749367713928\n",
      "Training Loss : 0.9399500489234924\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 476001\n",
      "mean reward (100 episodes) 21.593279\n",
      "best mean reward 158.981212\n",
      "running time 1609.846723\n",
      "Train_EnvstepsSoFar : 476001\n",
      "Train_AverageReturn : 21.593278719719223\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1609.8467228412628\n",
      "Training Loss : 2.9384026527404785\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 477001\n",
      "mean reward (100 episodes) 5.614159\n",
      "best mean reward 158.981212\n",
      "running time 1612.466056\n",
      "Train_EnvstepsSoFar : 477001\n",
      "Train_AverageReturn : 5.6141587352455335\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1612.4660558700562\n",
      "Training Loss : 1.1626079082489014\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 478001\n",
      "mean reward (100 episodes) 3.494203\n",
      "best mean reward 158.981212\n",
      "running time 1615.089500\n",
      "Train_EnvstepsSoFar : 478001\n",
      "Train_AverageReturn : 3.4942033855490426\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1615.089499950409\n",
      "Training Loss : 0.7963789701461792\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 479001\n",
      "mean reward (100 episodes) 9.240053\n",
      "best mean reward 158.981212\n",
      "running time 1617.720588\n",
      "Train_EnvstepsSoFar : 479001\n",
      "Train_AverageReturn : 9.240052982113644\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1617.7205877304077\n",
      "Training Loss : 0.1251107007265091\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 480001\n",
      "mean reward (100 episodes) 6.157355\n",
      "best mean reward 158.981212\n",
      "running time 1620.331451\n",
      "Train_EnvstepsSoFar : 480001\n",
      "Train_AverageReturn : 6.15735488580931\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1620.3314509391785\n",
      "Training Loss : 5.124454498291016\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 481001\n",
      "mean reward (100 episodes) 11.234994\n",
      "best mean reward 158.981212\n",
      "running time 1622.999833\n",
      "Train_EnvstepsSoFar : 481001\n",
      "Train_AverageReturn : 11.234994182853846\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1622.999832868576\n",
      "Training Loss : 0.2544611692428589\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 482001\n",
      "mean reward (100 episodes) 20.348382\n",
      "best mean reward 158.981212\n",
      "running time 1625.622779\n",
      "Train_EnvstepsSoFar : 482001\n",
      "Train_AverageReturn : 20.348382248510664\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1625.622778892517\n",
      "Training Loss : 1.5637480020523071\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 483001\n",
      "mean reward (100 episodes) 29.607221\n",
      "best mean reward 158.981212\n",
      "running time 1628.228772\n",
      "Train_EnvstepsSoFar : 483001\n",
      "Train_AverageReturn : 29.607220763981186\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1628.2287719249725\n",
      "Training Loss : 1.3207018375396729\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 484001\n",
      "mean reward (100 episodes) 37.494323\n",
      "best mean reward 158.981212\n",
      "running time 1630.900678\n",
      "Train_EnvstepsSoFar : 484001\n",
      "Train_AverageReturn : 37.494322841602276\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1630.9006779193878\n",
      "Training Loss : 0.21473170816898346\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 485001\n",
      "mean reward (100 episodes) 43.454767\n",
      "best mean reward 158.981212\n",
      "running time 1633.605036\n",
      "Train_EnvstepsSoFar : 485001\n",
      "Train_AverageReturn : 43.45476734879155\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1633.6050357818604\n",
      "Training Loss : 1.945667028427124\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 486001\n",
      "mean reward (100 episodes) 42.766573\n",
      "best mean reward 158.981212\n",
      "running time 1636.313609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 486001\n",
      "Train_AverageReturn : 42.766572866292705\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1636.3136088848114\n",
      "Training Loss : 0.5933546423912048\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 487001\n",
      "mean reward (100 episodes) 30.219637\n",
      "best mean reward 158.981212\n",
      "running time 1638.956865\n",
      "Train_EnvstepsSoFar : 487001\n",
      "Train_AverageReturn : 30.2196366081615\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1638.9568648338318\n",
      "Training Loss : 0.21000882983207703\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 488001\n",
      "mean reward (100 episodes) 28.141394\n",
      "best mean reward 158.981212\n",
      "running time 1641.570862\n",
      "Train_EnvstepsSoFar : 488001\n",
      "Train_AverageReturn : 28.1413941950612\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1641.5708618164062\n",
      "Training Loss : 0.18153449892997742\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 489001\n",
      "mean reward (100 episodes) 32.619558\n",
      "best mean reward 158.981212\n",
      "running time 1644.216782\n",
      "Train_EnvstepsSoFar : 489001\n",
      "Train_AverageReturn : 32.61955843090252\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1644.2167818546295\n",
      "Training Loss : 7.5030741691589355\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 490001\n",
      "mean reward (100 episodes) 37.964557\n",
      "best mean reward 158.981212\n",
      "running time 1646.951123\n",
      "Train_EnvstepsSoFar : 490001\n",
      "Train_AverageReturn : 37.96455724694735\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1646.9511229991913\n",
      "Training Loss : 4.770632743835449\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 491001\n",
      "mean reward (100 episodes) 45.888014\n",
      "best mean reward 158.981212\n",
      "running time 1649.671956\n",
      "Train_EnvstepsSoFar : 491001\n",
      "Train_AverageReturn : 45.88801443600374\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1649.6719558238983\n",
      "Training Loss : 0.4546659290790558\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 492001\n",
      "mean reward (100 episodes) 43.426744\n",
      "best mean reward 158.981212\n",
      "running time 1652.300538\n",
      "Train_EnvstepsSoFar : 492001\n",
      "Train_AverageReturn : 43.42674384784537\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1652.3005378246307\n",
      "Training Loss : 0.14472727477550507\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 493001\n",
      "mean reward (100 episodes) 53.159262\n",
      "best mean reward 158.981212\n",
      "running time 1654.956795\n",
      "Train_EnvstepsSoFar : 493001\n",
      "Train_AverageReturn : 53.159261700541\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1654.9567947387695\n",
      "Training Loss : 1.9751397371292114\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 494001\n",
      "mean reward (100 episodes) 45.399857\n",
      "best mean reward 158.981212\n",
      "running time 1657.641659\n",
      "Train_EnvstepsSoFar : 494001\n",
      "Train_AverageReturn : 45.39985688627632\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1657.641658782959\n",
      "Training Loss : 7.935006618499756\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 495001\n",
      "mean reward (100 episodes) 30.800433\n",
      "best mean reward 158.981212\n",
      "running time 1660.256244\n",
      "Train_EnvstepsSoFar : 495001\n",
      "Train_AverageReturn : 30.80043285938741\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1660.256243944168\n",
      "Training Loss : 0.189435213804245\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 496001\n",
      "mean reward (100 episodes) 24.860774\n",
      "best mean reward 158.981212\n",
      "running time 1662.837741\n",
      "Train_EnvstepsSoFar : 496001\n",
      "Train_AverageReturn : 24.860773745395342\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1662.8377406597137\n",
      "Training Loss : 0.1610890030860901\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 497001\n",
      "mean reward (100 episodes) 19.622306\n",
      "best mean reward 158.981212\n",
      "running time 1665.954056\n",
      "Train_EnvstepsSoFar : 497001\n",
      "Train_AverageReturn : 19.62230576931716\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1665.9540557861328\n",
      "Training Loss : 0.22850947082042694\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 498001\n",
      "mean reward (100 episodes) 9.694943\n",
      "best mean reward 158.981212\n",
      "running time 1668.600553\n",
      "Train_EnvstepsSoFar : 498001\n",
      "Train_AverageReturn : 9.694942863209803\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1668.600553035736\n",
      "Training Loss : 0.27743151783943176\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 499000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 499001\n",
      "mean reward (100 episodes) 11.017563\n",
      "best mean reward 158.981212\n",
      "running time 1671.281152\n",
      "Train_EnvstepsSoFar : 499001\n",
      "Train_AverageReturn : 11.017562827656802\n",
      "Train_BestReturn : 158.98121240261554\n",
      "TimeSinceStart : 1671.281152009964\n",
      "Training Loss : 0.09074777364730835\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Running DQN experiment with seed 1\n",
      "########################\n",
      "logging outputs to  logs/dqn/LunarLander/vanilla_dqn/seed1\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "LunarLander-v3\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1\n",
      "mean reward (100 episodes) nan\n",
      "best mean reward -inf\n",
      "running time 0.000547\n",
      "Train_EnvstepsSoFar : 1\n",
      "TimeSinceStart : 0.0005466938018798828\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1001\n",
      "mean reward (100 episodes) -262.515855\n",
      "best mean reward -inf\n",
      "running time 0.605927\n",
      "Train_EnvstepsSoFar : 1001\n",
      "Train_AverageReturn : -262.5158551758566\n",
      "TimeSinceStart : 0.6059267520904541\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2001\n",
      "mean reward (100 episodes) -232.797561\n",
      "best mean reward -inf\n",
      "running time 3.068574\n",
      "Train_EnvstepsSoFar : 2001\n",
      "Train_AverageReturn : -232.79756134550615\n",
      "TimeSinceStart : 3.0685739517211914\n",
      "Training Loss : 0.2692234516143799\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3001\n",
      "mean reward (100 episodes) -234.825913\n",
      "best mean reward -inf\n",
      "running time 5.542808\n",
      "Train_EnvstepsSoFar : 3001\n",
      "Train_AverageReturn : -234.82591279800184\n",
      "TimeSinceStart : 5.5428078174591064\n",
      "Training Loss : 4.772612571716309\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4001\n",
      "mean reward (100 episodes) -268.588910\n",
      "best mean reward -inf\n",
      "running time 8.036205\n",
      "Train_EnvstepsSoFar : 4001\n",
      "Train_AverageReturn : -268.5889098695452\n",
      "TimeSinceStart : 8.036204814910889\n",
      "Training Loss : 0.276628702878952\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5001\n",
      "mean reward (100 episodes) -291.227353\n",
      "best mean reward -inf\n",
      "running time 10.544271\n",
      "Train_EnvstepsSoFar : 5001\n",
      "Train_AverageReturn : -291.2273527344423\n",
      "TimeSinceStart : 10.544270753860474\n",
      "Training Loss : 0.5742841958999634\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6001\n",
      "mean reward (100 episodes) -286.970093\n",
      "best mean reward -inf\n",
      "running time 13.041731\n",
      "Train_EnvstepsSoFar : 6001\n",
      "Train_AverageReturn : -286.97009276490127\n",
      "TimeSinceStart : 13.041730642318726\n",
      "Training Loss : 3.8030714988708496\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7001\n",
      "mean reward (100 episodes) -275.182876\n",
      "best mean reward -inf\n",
      "running time 15.622524\n",
      "Train_EnvstepsSoFar : 7001\n",
      "Train_AverageReturn : -275.1828762078687\n",
      "TimeSinceStart : 15.622523784637451\n",
      "Training Loss : 0.3533974587917328\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 8000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8001\n",
      "mean reward (100 episodes) -263.580618\n",
      "best mean reward -inf\n",
      "running time 18.183663\n",
      "Train_EnvstepsSoFar : 8001\n",
      "Train_AverageReturn : -263.5806179902761\n",
      "TimeSinceStart : 18.18366289138794\n",
      "Training Loss : 0.3545996844768524\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9001\n",
      "mean reward (100 episodes) -258.909854\n",
      "best mean reward -inf\n",
      "running time 21.049743\n",
      "Train_EnvstepsSoFar : 9001\n",
      "Train_AverageReturn : -258.90985427004284\n",
      "TimeSinceStart : 21.049742698669434\n",
      "Training Loss : 0.5896323919296265\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10001\n",
      "mean reward (100 episodes) -252.961994\n",
      "best mean reward -inf\n",
      "running time 24.903784\n",
      "Train_EnvstepsSoFar : 10001\n",
      "Train_AverageReturn : -252.96199442422886\n",
      "TimeSinceStart : 24.903783798217773\n",
      "Training Loss : 0.6888968348503113\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 11000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 11001\n",
      "mean reward (100 episodes) -249.452659\n",
      "best mean reward -inf\n",
      "running time 27.472850\n",
      "Train_EnvstepsSoFar : 11001\n",
      "Train_AverageReturn : -249.45265865237698\n",
      "TimeSinceStart : 27.47284984588623\n",
      "Training Loss : 4.52762508392334\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 12000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 12001\n",
      "mean reward (100 episodes) -243.291536\n",
      "best mean reward -inf\n",
      "running time 30.063538\n",
      "Train_EnvstepsSoFar : 12001\n",
      "Train_AverageReturn : -243.29153583829097\n",
      "TimeSinceStart : 30.06353783607483\n",
      "Training Loss : 1.0199790000915527\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 13000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 13001\n",
      "mean reward (100 episodes) -243.951556\n",
      "best mean reward -inf\n",
      "running time 32.752589\n",
      "Train_EnvstepsSoFar : 13001\n",
      "Train_AverageReturn : -243.95155590560736\n",
      "TimeSinceStart : 32.752588748931885\n",
      "Training Loss : 0.4541970491409302\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 14000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 14001\n",
      "mean reward (100 episodes) -240.215370\n",
      "best mean reward -inf\n",
      "running time 35.712377\n",
      "Train_EnvstepsSoFar : 14001\n",
      "Train_AverageReturn : -240.21537013698796\n",
      "TimeSinceStart : 35.712376832962036\n",
      "Training Loss : 7.165851593017578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 15000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 15001\n",
      "mean reward (100 episodes) -233.482981\n",
      "best mean reward -inf\n",
      "running time 38.365447\n",
      "Train_EnvstepsSoFar : 15001\n",
      "Train_AverageReturn : -233.48298119683784\n",
      "TimeSinceStart : 38.36544680595398\n",
      "Training Loss : 1.708884596824646\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 16000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 16001\n",
      "mean reward (100 episodes) -232.781291\n",
      "best mean reward -inf\n",
      "running time 41.145971\n",
      "Train_EnvstepsSoFar : 16001\n",
      "Train_AverageReturn : -232.78129144781946\n",
      "TimeSinceStart : 41.145970821380615\n",
      "Training Loss : 6.454198360443115\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 17000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 17001\n",
      "mean reward (100 episodes) -231.131589\n",
      "best mean reward -inf\n",
      "running time 44.990699\n",
      "Train_EnvstepsSoFar : 17001\n",
      "Train_AverageReturn : -231.13158873253641\n",
      "TimeSinceStart : 44.99069905281067\n",
      "Training Loss : 1.4276092052459717\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 18000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 18001\n",
      "mean reward (100 episodes) -227.452963\n",
      "best mean reward -227.452963\n",
      "running time 47.717951\n",
      "Train_EnvstepsSoFar : 18001\n",
      "Train_AverageReturn : -227.45296327674848\n",
      "Train_BestReturn : -227.45296327674848\n",
      "TimeSinceStart : 47.71795082092285\n",
      "Training Loss : 1.6783204078674316\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 19000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 19001\n",
      "mean reward (100 episodes) -226.327932\n",
      "best mean reward -226.327932\n",
      "running time 50.639005\n",
      "Train_EnvstepsSoFar : 19001\n",
      "Train_AverageReturn : -226.32793171013898\n",
      "Train_BestReturn : -226.32793171013898\n",
      "TimeSinceStart : 50.639004945755005\n",
      "Training Loss : 0.6408029794692993\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 20001\n",
      "mean reward (100 episodes) -223.649853\n",
      "best mean reward -223.649853\n",
      "running time 55.108773\n",
      "Train_EnvstepsSoFar : 20001\n",
      "Train_AverageReturn : -223.64985254955255\n",
      "Train_BestReturn : -223.64985254955255\n",
      "TimeSinceStart : 55.10877275466919\n",
      "Training Loss : 1.912262201309204\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 21000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 21001\n",
      "mean reward (100 episodes) -221.553366\n",
      "best mean reward -221.553366\n",
      "running time 59.369776\n",
      "Train_EnvstepsSoFar : 21001\n",
      "Train_AverageReturn : -221.55336566714715\n",
      "Train_BestReturn : -221.55336566714715\n",
      "TimeSinceStart : 59.36977577209473\n",
      "Training Loss : 0.6120772957801819\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 22000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 22001\n",
      "mean reward (100 episodes) -220.044012\n",
      "best mean reward -220.044012\n",
      "running time 62.542309\n",
      "Train_EnvstepsSoFar : 22001\n",
      "Train_AverageReturn : -220.0440123410619\n",
      "Train_BestReturn : -220.0440123410619\n",
      "TimeSinceStart : 62.54230880737305\n",
      "Training Loss : 2.2118239402770996\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 23000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 23001\n",
      "mean reward (100 episodes) -218.805767\n",
      "best mean reward -218.805767\n",
      "running time 66.737236\n",
      "Train_EnvstepsSoFar : 23001\n",
      "Train_AverageReturn : -218.80576699145723\n",
      "Train_BestReturn : -218.80576699145723\n",
      "TimeSinceStart : 66.73723578453064\n",
      "Training Loss : 3.334486961364746\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 24000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 24001\n",
      "mean reward (100 episodes) -217.863074\n",
      "best mean reward -217.863074\n",
      "running time 70.709546\n",
      "Train_EnvstepsSoFar : 24001\n",
      "Train_AverageReturn : -217.86307433951148\n",
      "Train_BestReturn : -217.86307433951148\n",
      "TimeSinceStart : 70.70954585075378\n",
      "Training Loss : 4.469270706176758\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 25000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 25001\n",
      "mean reward (100 episodes) -216.833676\n",
      "best mean reward -216.833676\n",
      "running time 74.685691\n",
      "Train_EnvstepsSoFar : 25001\n",
      "Train_AverageReturn : -216.8336761438967\n",
      "Train_BestReturn : -216.8336761438967\n",
      "TimeSinceStart : 74.68569087982178\n",
      "Training Loss : 0.8420644402503967\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 26000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 26001\n",
      "mean reward (100 episodes) -215.810069\n",
      "best mean reward -215.810069\n",
      "running time 79.005466\n",
      "Train_EnvstepsSoFar : 26001\n",
      "Train_AverageReturn : -215.81006943475506\n",
      "Train_BestReturn : -215.81006943475506\n",
      "TimeSinceStart : 79.0054657459259\n",
      "Training Loss : 0.579231858253479\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 27000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 27001\n",
      "mean reward (100 episodes) -213.132743\n",
      "best mean reward -213.132743\n",
      "running time 83.388544\n",
      "Train_EnvstepsSoFar : 27001\n",
      "Train_AverageReturn : -213.13274347594586\n",
      "Train_BestReturn : -213.13274347594586\n",
      "TimeSinceStart : 83.38854384422302\n",
      "Training Loss : 0.4884894788265228\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 28000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 28001\n",
      "mean reward (100 episodes) -212.763741\n",
      "best mean reward -212.763741\n",
      "running time 88.141335\n",
      "Train_EnvstepsSoFar : 28001\n",
      "Train_AverageReturn : -212.76374072067742\n",
      "Train_BestReturn : -212.76374072067742\n",
      "TimeSinceStart : 88.14133477210999\n",
      "Training Loss : 1.9812486171722412\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 29000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 29001\n",
      "mean reward (100 episodes) -211.682613\n",
      "best mean reward -211.682613\n",
      "running time 92.096123\n",
      "Train_EnvstepsSoFar : 29001\n",
      "Train_AverageReturn : -211.68261278640568\n",
      "Train_BestReturn : -211.68261278640568\n",
      "TimeSinceStart : 92.09612274169922\n",
      "Training Loss : 0.9214882850646973\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 30000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 30001\n",
      "mean reward (100 episodes) -211.347909\n",
      "best mean reward -211.347909\n",
      "running time 95.439081\n",
      "Train_EnvstepsSoFar : 30001\n",
      "Train_AverageReturn : -211.34790948624567\n",
      "Train_BestReturn : -211.34790948624567\n",
      "TimeSinceStart : 95.43908095359802\n",
      "Training Loss : 1.0748368501663208\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 31000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 31001\n",
      "mean reward (100 episodes) -208.592734\n",
      "best mean reward -208.592734\n",
      "running time 99.067875\n",
      "Train_EnvstepsSoFar : 31001\n",
      "Train_AverageReturn : -208.59273392055758\n",
      "Train_BestReturn : -208.59273392055758\n",
      "TimeSinceStart : 99.06787467002869\n",
      "Training Loss : 1.4594875574111938\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 32000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 32001\n",
      "mean reward (100 episodes) -205.277088\n",
      "best mean reward -205.277088\n",
      "running time 103.035951\n",
      "Train_EnvstepsSoFar : 32001\n",
      "Train_AverageReturn : -205.2770880309493\n",
      "Train_BestReturn : -205.2770880309493\n",
      "TimeSinceStart : 103.03595066070557\n",
      "Training Loss : 0.4543294310569763\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 33000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 33001\n",
      "mean reward (100 episodes) -204.977268\n",
      "best mean reward -204.977268\n",
      "running time 107.164414\n",
      "Train_EnvstepsSoFar : 33001\n",
      "Train_AverageReturn : -204.97726808975523\n",
      "Train_BestReturn : -204.97726808975523\n",
      "TimeSinceStart : 107.1644139289856\n",
      "Training Loss : 0.5799806118011475\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 34000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 34001\n",
      "mean reward (100 episodes) -204.407910\n",
      "best mean reward -204.407910\n",
      "running time 110.968677\n",
      "Train_EnvstepsSoFar : 34001\n",
      "Train_AverageReturn : -204.40791007731602\n",
      "Train_BestReturn : -204.40791007731602\n",
      "TimeSinceStart : 110.96867680549622\n",
      "Training Loss : 0.4143529534339905\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 35000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 35001\n",
      "mean reward (100 episodes) -201.027898\n",
      "best mean reward -201.027898\n",
      "running time 114.586001\n",
      "Train_EnvstepsSoFar : 35001\n",
      "Train_AverageReturn : -201.0278982254551\n",
      "Train_BestReturn : -201.0278982254551\n",
      "TimeSinceStart : 114.58600091934204\n",
      "Training Loss : 0.9359883666038513\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 36000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 36001\n",
      "mean reward (100 episodes) -201.294995\n",
      "best mean reward -201.027898\n",
      "running time 118.361597\n",
      "Train_EnvstepsSoFar : 36001\n",
      "Train_AverageReturn : -201.2949949967254\n",
      "Train_BestReturn : -201.0278982254551\n",
      "TimeSinceStart : 118.36159682273865\n",
      "Training Loss : 7.344169616699219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 37000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 37001\n",
      "mean reward (100 episodes) -195.836471\n",
      "best mean reward -195.836471\n",
      "running time 122.142384\n",
      "Train_EnvstepsSoFar : 37001\n",
      "Train_AverageReturn : -195.83647102072726\n",
      "Train_BestReturn : -195.83647102072726\n",
      "TimeSinceStart : 122.14238381385803\n",
      "Training Loss : 0.770888090133667\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 38000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 38001\n",
      "mean reward (100 episodes) -193.160980\n",
      "best mean reward -193.160980\n",
      "running time 126.095036\n",
      "Train_EnvstepsSoFar : 38001\n",
      "Train_AverageReturn : -193.1609802934323\n",
      "Train_BestReturn : -193.1609802934323\n",
      "TimeSinceStart : 126.0950357913971\n",
      "Training Loss : 1.174376130104065\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 39000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 39001\n",
      "mean reward (100 episodes) -188.605989\n",
      "best mean reward -188.605989\n",
      "running time 129.629234\n",
      "Train_EnvstepsSoFar : 39001\n",
      "Train_AverageReturn : -188.6059887323715\n",
      "Train_BestReturn : -188.6059887323715\n",
      "TimeSinceStart : 129.62923383712769\n",
      "Training Loss : 0.6754868626594543\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 40001\n",
      "mean reward (100 episodes) -186.246213\n",
      "best mean reward -186.246213\n",
      "running time 132.984886\n",
      "Train_EnvstepsSoFar : 40001\n",
      "Train_AverageReturn : -186.24621288923703\n",
      "Train_BestReturn : -186.24621288923703\n",
      "TimeSinceStart : 132.98488593101501\n",
      "Training Loss : 0.9799926280975342\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 41000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 41001\n",
      "mean reward (100 episodes) -182.761008\n",
      "best mean reward -182.761008\n",
      "running time 136.555940\n",
      "Train_EnvstepsSoFar : 41001\n",
      "Train_AverageReturn : -182.76100782819347\n",
      "Train_BestReturn : -182.76100782819347\n",
      "TimeSinceStart : 136.55593967437744\n",
      "Training Loss : 0.4671780467033386\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 42000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 42001\n",
      "mean reward (100 episodes) -180.838727\n",
      "best mean reward -180.838727\n",
      "running time 140.516295\n",
      "Train_EnvstepsSoFar : 42001\n",
      "Train_AverageReturn : -180.83872734814867\n",
      "Train_BestReturn : -180.83872734814867\n",
      "TimeSinceStart : 140.51629495620728\n",
      "Training Loss : 4.969760894775391\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 43000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 43001\n",
      "mean reward (100 episodes) -180.858059\n",
      "best mean reward -180.838727\n",
      "running time 144.414061\n",
      "Train_EnvstepsSoFar : 43001\n",
      "Train_AverageReturn : -180.85805898219732\n",
      "Train_BestReturn : -180.83872734814867\n",
      "TimeSinceStart : 144.41406083106995\n",
      "Training Loss : 2.0718722343444824\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 44000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 44001\n",
      "mean reward (100 episodes) -179.861577\n",
      "best mean reward -179.861577\n",
      "running time 148.920340\n",
      "Train_EnvstepsSoFar : 44001\n",
      "Train_AverageReturn : -179.86157671898854\n",
      "Train_BestReturn : -179.86157671898854\n",
      "TimeSinceStart : 148.92033982276917\n",
      "Training Loss : 0.4870959520339966\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 45000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 45001\n",
      "mean reward (100 episodes) -176.477986\n",
      "best mean reward -176.477986\n",
      "running time 152.893427\n",
      "Train_EnvstepsSoFar : 45001\n",
      "Train_AverageReturn : -176.4779863728858\n",
      "Train_BestReturn : -176.4779863728858\n",
      "TimeSinceStart : 152.8934268951416\n",
      "Training Loss : 0.334550678730011\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 46000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 46001\n",
      "mean reward (100 episodes) -174.487141\n",
      "best mean reward -174.487141\n",
      "running time 156.487857\n",
      "Train_EnvstepsSoFar : 46001\n",
      "Train_AverageReturn : -174.48714108758065\n",
      "Train_BestReturn : -174.48714108758065\n",
      "TimeSinceStart : 156.4878568649292\n",
      "Training Loss : 0.5835703611373901\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 47000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 47001\n",
      "mean reward (100 episodes) -170.957792\n",
      "best mean reward -170.957792\n",
      "running time 160.651236\n",
      "Train_EnvstepsSoFar : 47001\n",
      "Train_AverageReturn : -170.9577922508157\n",
      "Train_BestReturn : -170.9577922508157\n",
      "TimeSinceStart : 160.65123581886292\n",
      "Training Loss : 0.4488292336463928\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 48000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 48001\n",
      "mean reward (100 episodes) -168.846130\n",
      "best mean reward -168.846130\n",
      "running time 164.366864\n",
      "Train_EnvstepsSoFar : 48001\n",
      "Train_AverageReturn : -168.84613045600616\n",
      "Train_BestReturn : -168.84613045600616\n",
      "TimeSinceStart : 164.36686372756958\n",
      "Training Loss : 0.45833176374435425\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 49000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 49001\n",
      "mean reward (100 episodes) -165.948629\n",
      "best mean reward -165.948629\n",
      "running time 167.766575\n",
      "Train_EnvstepsSoFar : 49001\n",
      "Train_AverageReturn : -165.9486291344171\n",
      "Train_BestReturn : -165.9486291344171\n",
      "TimeSinceStart : 167.76657485961914\n",
      "Training Loss : 1.3592664003372192\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 50001\n",
      "mean reward (100 episodes) -161.833664\n",
      "best mean reward -161.833664\n",
      "running time 171.754690\n",
      "Train_EnvstepsSoFar : 50001\n",
      "Train_AverageReturn : -161.8336640345358\n",
      "Train_BestReturn : -161.8336640345358\n",
      "TimeSinceStart : 171.75468969345093\n",
      "Training Loss : 0.3620435893535614\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 51000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 51001\n",
      "mean reward (100 episodes) -158.954610\n",
      "best mean reward -158.954610\n",
      "running time 176.741582\n",
      "Train_EnvstepsSoFar : 51001\n",
      "Train_AverageReturn : -158.95461022685066\n",
      "Train_BestReturn : -158.95461022685066\n",
      "TimeSinceStart : 176.7415816783905\n",
      "Training Loss : 0.389000803232193\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 52000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 52001\n",
      "mean reward (100 episodes) -156.717674\n",
      "best mean reward -156.717674\n",
      "running time 180.407864\n",
      "Train_EnvstepsSoFar : 52001\n",
      "Train_AverageReturn : -156.7176737868372\n",
      "Train_BestReturn : -156.7176737868372\n",
      "TimeSinceStart : 180.40786385536194\n",
      "Training Loss : 1.032907485961914\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 53000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 53001\n",
      "mean reward (100 episodes) -155.986086\n",
      "best mean reward -155.986086\n",
      "running time 183.753349\n",
      "Train_EnvstepsSoFar : 53001\n",
      "Train_AverageReturn : -155.98608642757358\n",
      "Train_BestReturn : -155.98608642757358\n",
      "TimeSinceStart : 183.75334882736206\n",
      "Training Loss : 0.4172305762767792\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 54000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 54001\n",
      "mean reward (100 episodes) -154.579412\n",
      "best mean reward -154.579412\n",
      "running time 187.861896\n",
      "Train_EnvstepsSoFar : 54001\n",
      "Train_AverageReturn : -154.5794121547259\n",
      "Train_BestReturn : -154.5794121547259\n",
      "TimeSinceStart : 187.86189579963684\n",
      "Training Loss : 0.5801225304603577\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 55000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 55001\n",
      "mean reward (100 episodes) -154.326807\n",
      "best mean reward -154.326807\n",
      "running time 191.788455\n",
      "Train_EnvstepsSoFar : 55001\n",
      "Train_AverageReturn : -154.32680722747455\n",
      "Train_BestReturn : -154.32680722747455\n",
      "TimeSinceStart : 191.78845477104187\n",
      "Training Loss : 0.9234123229980469\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 56000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 56001\n",
      "mean reward (100 episodes) -147.639044\n",
      "best mean reward -147.639044\n",
      "running time 195.508891\n",
      "Train_EnvstepsSoFar : 56001\n",
      "Train_AverageReturn : -147.6390443283004\n",
      "Train_BestReturn : -147.6390443283004\n",
      "TimeSinceStart : 195.50889086723328\n",
      "Training Loss : 0.7666354775428772\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 57000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 57001\n",
      "mean reward (100 episodes) -142.843270\n",
      "best mean reward -142.843270\n",
      "running time 199.273507\n",
      "Train_EnvstepsSoFar : 57001\n",
      "Train_AverageReturn : -142.8432697914729\n",
      "Train_BestReturn : -142.8432697914729\n",
      "TimeSinceStart : 199.27350664138794\n",
      "Training Loss : 0.4037869870662689\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 58000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 58001\n",
      "mean reward (100 episodes) -141.856452\n",
      "best mean reward -141.856452\n",
      "running time 202.894256\n",
      "Train_EnvstepsSoFar : 58001\n",
      "Train_AverageReturn : -141.85645174155815\n",
      "Train_BestReturn : -141.85645174155815\n",
      "TimeSinceStart : 202.89425587654114\n",
      "Training Loss : 0.7747028470039368\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 59000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 59001\n",
      "mean reward (100 episodes) -141.472374\n",
      "best mean reward -141.472374\n",
      "running time 208.005118\n",
      "Train_EnvstepsSoFar : 59001\n",
      "Train_AverageReturn : -141.47237421740724\n",
      "Train_BestReturn : -141.47237421740724\n",
      "TimeSinceStart : 208.005117893219\n",
      "Training Loss : 0.2230793833732605\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 60001\n",
      "mean reward (100 episodes) -139.853127\n",
      "best mean reward -139.853127\n",
      "running time 211.986926\n",
      "Train_EnvstepsSoFar : 60001\n",
      "Train_AverageReturn : -139.85312655417067\n",
      "Train_BestReturn : -139.85312655417067\n",
      "TimeSinceStart : 211.9869258403778\n",
      "Training Loss : 0.2754620313644409\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 61000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 61001\n",
      "mean reward (100 episodes) -134.181240\n",
      "best mean reward -134.181240\n",
      "running time 215.790495\n",
      "Train_EnvstepsSoFar : 61001\n",
      "Train_AverageReturn : -134.18124037132492\n",
      "Train_BestReturn : -134.18124037132492\n",
      "TimeSinceStart : 215.79049491882324\n",
      "Training Loss : 0.3491460680961609\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 62000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 62001\n",
      "mean reward (100 episodes) -134.206845\n",
      "best mean reward -134.181240\n",
      "running time 219.336019\n",
      "Train_EnvstepsSoFar : 62001\n",
      "Train_AverageReturn : -134.2068452065136\n",
      "Train_BestReturn : -134.18124037132492\n",
      "TimeSinceStart : 219.33601880073547\n",
      "Training Loss : 0.2628527581691742\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 63000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 63001\n",
      "mean reward (100 episodes) -132.691069\n",
      "best mean reward -132.691069\n",
      "running time 222.623587\n",
      "Train_EnvstepsSoFar : 63001\n",
      "Train_AverageReturn : -132.6910693062535\n",
      "Train_BestReturn : -132.6910693062535\n",
      "TimeSinceStart : 222.6235866546631\n",
      "Training Loss : 0.4205043315887451\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 64000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 64001\n",
      "mean reward (100 episodes) -131.550004\n",
      "best mean reward -131.550004\n",
      "running time 226.837508\n",
      "Train_EnvstepsSoFar : 64001\n",
      "Train_AverageReturn : -131.55000374218108\n",
      "Train_BestReturn : -131.55000374218108\n",
      "TimeSinceStart : 226.83750772476196\n",
      "Training Loss : 0.2911859452724457\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 65000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 65001\n",
      "mean reward (100 episodes) -127.348927\n",
      "best mean reward -127.348927\n",
      "running time 231.255394\n",
      "Train_EnvstepsSoFar : 65001\n",
      "Train_AverageReturn : -127.34892656100945\n",
      "Train_BestReturn : -127.34892656100945\n",
      "TimeSinceStart : 231.25539374351501\n",
      "Training Loss : 0.25441092252731323\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 66000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 66001\n",
      "mean reward (100 episodes) -125.827484\n",
      "best mean reward -125.827484\n",
      "running time 234.779805\n",
      "Train_EnvstepsSoFar : 66001\n",
      "Train_AverageReturn : -125.82748413927499\n",
      "Train_BestReturn : -125.82748413927499\n",
      "TimeSinceStart : 234.77980494499207\n",
      "Training Loss : 0.22073237597942352\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 67000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 67001\n",
      "mean reward (100 episodes) -120.324493\n",
      "best mean reward -120.324493\n",
      "running time 238.431064\n",
      "Train_EnvstepsSoFar : 67001\n",
      "Train_AverageReturn : -120.32449278707298\n",
      "Train_BestReturn : -120.32449278707298\n",
      "TimeSinceStart : 238.43106365203857\n",
      "Training Loss : 0.2250927835702896\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 68000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 68001\n",
      "mean reward (100 episodes) -124.270297\n",
      "best mean reward -120.324493\n",
      "running time 243.061660\n",
      "Train_EnvstepsSoFar : 68001\n",
      "Train_AverageReturn : -124.27029657164634\n",
      "Train_BestReturn : -120.32449278707298\n",
      "TimeSinceStart : 243.06165981292725\n",
      "Training Loss : 0.3259691298007965\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 69000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 69001\n",
      "mean reward (100 episodes) -121.551814\n",
      "best mean reward -120.324493\n",
      "running time 246.847685\n",
      "Train_EnvstepsSoFar : 69001\n",
      "Train_AverageReturn : -121.55181350791194\n",
      "Train_BestReturn : -120.32449278707298\n",
      "TimeSinceStart : 246.8476848602295\n",
      "Training Loss : 0.2154228836297989\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 70001\n",
      "mean reward (100 episodes) -119.158838\n",
      "best mean reward -119.158838\n",
      "running time 251.002852\n",
      "Train_EnvstepsSoFar : 70001\n",
      "Train_AverageReturn : -119.15883791027105\n",
      "Train_BestReturn : -119.15883791027105\n",
      "TimeSinceStart : 251.00285172462463\n",
      "Training Loss : 0.27992740273475647\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 71000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 71001\n",
      "mean reward (100 episodes) -118.314160\n",
      "best mean reward -118.314160\n",
      "running time 254.384889\n",
      "Train_EnvstepsSoFar : 71001\n",
      "Train_AverageReturn : -118.31416015495837\n",
      "Train_BestReturn : -118.31416015495837\n",
      "TimeSinceStart : 254.38488864898682\n",
      "Training Loss : 0.6551766991615295\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 72000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 72001\n",
      "mean reward (100 episodes) -115.261200\n",
      "best mean reward -115.261200\n",
      "running time 258.122749\n",
      "Train_EnvstepsSoFar : 72001\n",
      "Train_AverageReturn : -115.2611995377014\n",
      "Train_BestReturn : -115.2611995377014\n",
      "TimeSinceStart : 258.1227488517761\n",
      "Training Loss : 0.7046878337860107\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 73000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 73001\n",
      "mean reward (100 episodes) -114.326683\n",
      "best mean reward -114.326683\n",
      "running time 261.961974\n",
      "Train_EnvstepsSoFar : 73001\n",
      "Train_AverageReturn : -114.32668318130665\n",
      "Train_BestReturn : -114.32668318130665\n",
      "TimeSinceStart : 261.96197390556335\n",
      "Training Loss : 0.2598627209663391\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 74000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 74001\n",
      "mean reward (100 episodes) -109.808883\n",
      "best mean reward -109.808883\n",
      "running time 265.961318\n",
      "Train_EnvstepsSoFar : 74001\n",
      "Train_AverageReturn : -109.80888250351018\n",
      "Train_BestReturn : -109.80888250351018\n",
      "TimeSinceStart : 265.96131777763367\n",
      "Training Loss : 0.19530907273292542\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 75000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 75001\n",
      "mean reward (100 episodes) -109.429174\n",
      "best mean reward -109.429174\n",
      "running time 269.331828\n",
      "Train_EnvstepsSoFar : 75001\n",
      "Train_AverageReturn : -109.42917378751223\n",
      "Train_BestReturn : -109.42917378751223\n",
      "TimeSinceStart : 269.331827878952\n",
      "Training Loss : 0.1859809160232544\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 76000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 76001\n",
      "mean reward (100 episodes) -107.165774\n",
      "best mean reward -107.165774\n",
      "running time 272.947970\n",
      "Train_EnvstepsSoFar : 76001\n",
      "Train_AverageReturn : -107.16577421176245\n",
      "Train_BestReturn : -107.16577421176245\n",
      "TimeSinceStart : 272.9479696750641\n",
      "Training Loss : 0.26021087169647217\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 77000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 77001\n",
      "mean reward (100 episodes) -102.338432\n",
      "best mean reward -102.338432\n",
      "running time 276.499471\n",
      "Train_EnvstepsSoFar : 77001\n",
      "Train_AverageReturn : -102.33843189280604\n",
      "Train_BestReturn : -102.33843189280604\n",
      "TimeSinceStart : 276.4994707107544\n",
      "Training Loss : 0.17399808764457703\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 78000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 78001\n",
      "mean reward (100 episodes) -99.373039\n",
      "best mean reward -99.373039\n",
      "running time 280.062773\n",
      "Train_EnvstepsSoFar : 78001\n",
      "Train_AverageReturn : -99.3730388977831\n",
      "Train_BestReturn : -99.3730388977831\n",
      "TimeSinceStart : 280.06277298927307\n",
      "Training Loss : 0.7138028740882874\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 79000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 79001\n",
      "mean reward (100 episodes) -99.289335\n",
      "best mean reward -99.289335\n",
      "running time 284.453585\n",
      "Train_EnvstepsSoFar : 79001\n",
      "Train_AverageReturn : -99.28933480084048\n",
      "Train_BestReturn : -99.28933480084048\n",
      "TimeSinceStart : 284.4535849094391\n",
      "Training Loss : 0.2124224305152893\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 80001\n",
      "mean reward (100 episodes) -96.418201\n",
      "best mean reward -96.418201\n",
      "running time 288.337016\n",
      "Train_EnvstepsSoFar : 80001\n",
      "Train_AverageReturn : -96.41820067701894\n",
      "Train_BestReturn : -96.41820067701894\n",
      "TimeSinceStart : 288.3370156288147\n",
      "Training Loss : 0.18734225630760193\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 81000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 81001\n",
      "mean reward (100 episodes) -92.677717\n",
      "best mean reward -92.677717\n",
      "running time 291.554480\n",
      "Train_EnvstepsSoFar : 81001\n",
      "Train_AverageReturn : -92.67771667050413\n",
      "Train_BestReturn : -92.67771667050413\n",
      "TimeSinceStart : 291.5544798374176\n",
      "Training Loss : 0.12687323987483978\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 82000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 82001\n",
      "mean reward (100 episodes) -86.063849\n",
      "best mean reward -86.063849\n",
      "running time 295.348296\n",
      "Train_EnvstepsSoFar : 82001\n",
      "Train_AverageReturn : -86.06384856759432\n",
      "Train_BestReturn : -86.06384856759432\n",
      "TimeSinceStart : 295.34829592704773\n",
      "Training Loss : 0.16790781915187836\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 83000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 83001\n",
      "mean reward (100 episodes) -84.160169\n",
      "best mean reward -84.160169\n",
      "running time 299.395368\n",
      "Train_EnvstepsSoFar : 83001\n",
      "Train_AverageReturn : -84.16016901867643\n",
      "Train_BestReturn : -84.16016901867643\n",
      "TimeSinceStart : 299.39536786079407\n",
      "Training Loss : 0.19271817803382874\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 84000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 84001\n",
      "mean reward (100 episodes) -83.433199\n",
      "best mean reward -83.433199\n",
      "running time 304.055417\n",
      "Train_EnvstepsSoFar : 84001\n",
      "Train_AverageReturn : -83.4331989694079\n",
      "Train_BestReturn : -83.4331989694079\n",
      "TimeSinceStart : 304.0554168224335\n",
      "Training Loss : 0.15849077701568604\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 85000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 85001\n",
      "mean reward (100 episodes) -78.859994\n",
      "best mean reward -78.859994\n",
      "running time 307.751929\n",
      "Train_EnvstepsSoFar : 85001\n",
      "Train_AverageReturn : -78.85999393697345\n",
      "Train_BestReturn : -78.85999393697345\n",
      "TimeSinceStart : 307.75192880630493\n",
      "Training Loss : 0.12432240694761276\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 86000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 86001\n",
      "mean reward (100 episodes) -75.725631\n",
      "best mean reward -75.725631\n",
      "running time 311.011175\n",
      "Train_EnvstepsSoFar : 86001\n",
      "Train_AverageReturn : -75.72563073654234\n",
      "Train_BestReturn : -75.72563073654234\n",
      "TimeSinceStart : 311.01117491722107\n",
      "Training Loss : 0.19225013256072998\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 87000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 87001\n",
      "mean reward (100 episodes) -74.599246\n",
      "best mean reward -74.599246\n",
      "running time 314.922451\n",
      "Train_EnvstepsSoFar : 87001\n",
      "Train_AverageReturn : -74.59924643398031\n",
      "Train_BestReturn : -74.59924643398031\n",
      "TimeSinceStart : 314.92245078086853\n",
      "Training Loss : 0.3824350833892822\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 88000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 88001\n",
      "mean reward (100 episodes) -73.903557\n",
      "best mean reward -73.903557\n",
      "running time 319.764943\n",
      "Train_EnvstepsSoFar : 88001\n",
      "Train_AverageReturn : -73.9035574733321\n",
      "Train_BestReturn : -73.9035574733321\n",
      "TimeSinceStart : 319.7649428844452\n",
      "Training Loss : 0.12370537221431732\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 89000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 89001\n",
      "mean reward (100 episodes) -71.397940\n",
      "best mean reward -71.397940\n",
      "running time 323.818013\n",
      "Train_EnvstepsSoFar : 89001\n",
      "Train_AverageReturn : -71.3979403081175\n",
      "Train_BestReturn : -71.3979403081175\n",
      "TimeSinceStart : 323.81801295280457\n",
      "Training Loss : 0.17754460871219635\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 90001\n",
      "mean reward (100 episodes) -69.211532\n",
      "best mean reward -69.211532\n",
      "running time 327.425317\n",
      "Train_EnvstepsSoFar : 90001\n",
      "Train_AverageReturn : -69.21153164594485\n",
      "Train_BestReturn : -69.21153164594485\n",
      "TimeSinceStart : 327.4253168106079\n",
      "Training Loss : 0.1575133353471756\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 91000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 91001\n",
      "mean reward (100 episodes) -65.972286\n",
      "best mean reward -65.972286\n",
      "running time 330.766695\n",
      "Train_EnvstepsSoFar : 91001\n",
      "Train_AverageReturn : -65.97228641314204\n",
      "Train_BestReturn : -65.97228641314204\n",
      "TimeSinceStart : 330.76669478416443\n",
      "Training Loss : 0.3134804964065552\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 92000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 92001\n",
      "mean reward (100 episodes) -66.307216\n",
      "best mean reward -65.972286\n",
      "running time 334.656680\n",
      "Train_EnvstepsSoFar : 92001\n",
      "Train_AverageReturn : -66.30721606792946\n",
      "Train_BestReturn : -65.97228641314204\n",
      "TimeSinceStart : 334.6566798686981\n",
      "Training Loss : 0.3808949589729309\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 93000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 93001\n",
      "mean reward (100 episodes) -61.678147\n",
      "best mean reward -61.678147\n",
      "running time 337.888052\n",
      "Train_EnvstepsSoFar : 93001\n",
      "Train_AverageReturn : -61.6781466727888\n",
      "Train_BestReturn : -61.6781466727888\n",
      "TimeSinceStart : 337.88805198669434\n",
      "Training Loss : 0.5950069427490234\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 94000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 94001\n",
      "mean reward (100 episodes) -62.675967\n",
      "best mean reward -61.678147\n",
      "running time 341.310179\n",
      "Train_EnvstepsSoFar : 94001\n",
      "Train_AverageReturn : -62.67596720913922\n",
      "Train_BestReturn : -61.6781466727888\n",
      "TimeSinceStart : 341.31017875671387\n",
      "Training Loss : 0.1475774645805359\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 95000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 95001\n",
      "mean reward (100 episodes) -58.421487\n",
      "best mean reward -58.421487\n",
      "running time 344.807649\n",
      "Train_EnvstepsSoFar : 95001\n",
      "Train_AverageReturn : -58.4214874941008\n",
      "Train_BestReturn : -58.4214874941008\n",
      "TimeSinceStart : 344.80764865875244\n",
      "Training Loss : 0.27711138129234314\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 96000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 96001\n",
      "mean reward (100 episodes) -58.615512\n",
      "best mean reward -58.421487\n",
      "running time 348.411040\n",
      "Train_EnvstepsSoFar : 96001\n",
      "Train_AverageReturn : -58.61551225422126\n",
      "Train_BestReturn : -58.4214874941008\n",
      "TimeSinceStart : 348.41103982925415\n",
      "Training Loss : 0.27903836965560913\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 97000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 97001\n",
      "mean reward (100 episodes) -54.719440\n",
      "best mean reward -54.719440\n",
      "running time 352.186190\n",
      "Train_EnvstepsSoFar : 97001\n",
      "Train_AverageReturn : -54.7194398485789\n",
      "Train_BestReturn : -54.7194398485789\n",
      "TimeSinceStart : 352.18618965148926\n",
      "Training Loss : 0.23133426904678345\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 98000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 98001\n",
      "mean reward (100 episodes) -55.246430\n",
      "best mean reward -54.719440\n",
      "running time 356.791001\n",
      "Train_EnvstepsSoFar : 98001\n",
      "Train_AverageReturn : -55.24642997758614\n",
      "Train_BestReturn : -54.7194398485789\n",
      "TimeSinceStart : 356.7910008430481\n",
      "Training Loss : 0.09787612408399582\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 99000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 99001\n",
      "mean reward (100 episodes) -53.288617\n",
      "best mean reward -53.288617\n",
      "running time 360.158593\n",
      "Train_EnvstepsSoFar : 99001\n",
      "Train_AverageReturn : -53.288616673263235\n",
      "Train_BestReturn : -53.288616673263235\n",
      "TimeSinceStart : 360.15859293937683\n",
      "Training Loss : 0.23549063503742218\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 100001\n",
      "mean reward (100 episodes) -53.091013\n",
      "best mean reward -53.091013\n",
      "running time 364.015221\n",
      "Train_EnvstepsSoFar : 100001\n",
      "Train_AverageReturn : -53.09101301517339\n",
      "Train_BestReturn : -53.09101301517339\n",
      "TimeSinceStart : 364.0152208805084\n",
      "Training Loss : 0.22312593460083008\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 101001\n",
      "mean reward (100 episodes) -54.297969\n",
      "best mean reward -53.091013\n",
      "running time 366.895455\n",
      "Train_EnvstepsSoFar : 101001\n",
      "Train_AverageReturn : -54.29796914914735\n",
      "Train_BestReturn : -53.09101301517339\n",
      "TimeSinceStart : 366.89545488357544\n",
      "Training Loss : 0.1038087010383606\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 102001\n",
      "mean reward (100 episodes) -52.637714\n",
      "best mean reward -52.637714\n",
      "running time 370.368344\n",
      "Train_EnvstepsSoFar : 102001\n",
      "Train_AverageReturn : -52.637714165264185\n",
      "Train_BestReturn : -52.637714165264185\n",
      "TimeSinceStart : 370.36834383010864\n",
      "Training Loss : 0.30527758598327637\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 103001\n",
      "mean reward (100 episodes) -51.627414\n",
      "best mean reward -51.627414\n",
      "running time 373.387129\n",
      "Train_EnvstepsSoFar : 103001\n",
      "Train_AverageReturn : -51.62741430474993\n",
      "Train_BestReturn : -51.62741430474993\n",
      "TimeSinceStart : 373.38712882995605\n",
      "Training Loss : 0.19156694412231445\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 104001\n",
      "mean reward (100 episodes) -51.287893\n",
      "best mean reward -51.287893\n",
      "running time 377.073740\n",
      "Train_EnvstepsSoFar : 104001\n",
      "Train_AverageReturn : -51.28789307842777\n",
      "Train_BestReturn : -51.28789307842777\n",
      "TimeSinceStart : 377.0737397670746\n",
      "Training Loss : 0.5617077350616455\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 105001\n",
      "mean reward (100 episodes) -50.854206\n",
      "best mean reward -50.854206\n",
      "running time 380.609668\n",
      "Train_EnvstepsSoFar : 105001\n",
      "Train_AverageReturn : -50.85420624323248\n",
      "Train_BestReturn : -50.85420624323248\n",
      "TimeSinceStart : 380.60966777801514\n",
      "Training Loss : 0.6141786575317383\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 106001\n",
      "mean reward (100 episodes) -51.360233\n",
      "best mean reward -50.854206\n",
      "running time 383.852481\n",
      "Train_EnvstepsSoFar : 106001\n",
      "Train_AverageReturn : -51.36023288822411\n",
      "Train_BestReturn : -50.85420624323248\n",
      "TimeSinceStart : 383.8524808883667\n",
      "Training Loss : 0.12001373618841171\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 107001\n",
      "mean reward (100 episodes) -51.186268\n",
      "best mean reward -50.854206\n",
      "running time 387.124987\n",
      "Train_EnvstepsSoFar : 107001\n",
      "Train_AverageReturn : -51.186267823104764\n",
      "Train_BestReturn : -50.85420624323248\n",
      "TimeSinceStart : 387.12498688697815\n",
      "Training Loss : 0.09382197260856628\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 108001\n",
      "mean reward (100 episodes) -50.021980\n",
      "best mean reward -50.021980\n",
      "running time 391.034898\n",
      "Train_EnvstepsSoFar : 108001\n",
      "Train_AverageReturn : -50.021979810979\n",
      "Train_BestReturn : -50.021979810979\n",
      "TimeSinceStart : 391.03489780426025\n",
      "Training Loss : 0.11973042786121368\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 109001\n",
      "mean reward (100 episodes) -49.543196\n",
      "best mean reward -49.543196\n",
      "running time 395.327666\n",
      "Train_EnvstepsSoFar : 109001\n",
      "Train_AverageReturn : -49.54319632020063\n",
      "Train_BestReturn : -49.54319632020063\n",
      "TimeSinceStart : 395.32766580581665\n",
      "Training Loss : 0.18930412828922272\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 110001\n",
      "mean reward (100 episodes) -49.787511\n",
      "best mean reward -49.543196\n",
      "running time 399.598878\n",
      "Train_EnvstepsSoFar : 110001\n",
      "Train_AverageReturn : -49.787510618786655\n",
      "Train_BestReturn : -49.54319632020063\n",
      "TimeSinceStart : 399.5988779067993\n",
      "Training Loss : 0.14688509702682495\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 111001\n",
      "mean reward (100 episodes) -46.935475\n",
      "best mean reward -46.935475\n",
      "running time 403.270169\n",
      "Train_EnvstepsSoFar : 111001\n",
      "Train_AverageReturn : -46.93547492269955\n",
      "Train_BestReturn : -46.93547492269955\n",
      "TimeSinceStart : 403.2701687812805\n",
      "Training Loss : 0.09610038250684738\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 112001\n",
      "mean reward (100 episodes) -46.402731\n",
      "best mean reward -46.402731\n",
      "running time 407.103711\n",
      "Train_EnvstepsSoFar : 112001\n",
      "Train_AverageReturn : -46.402730633242214\n",
      "Train_BestReturn : -46.402730633242214\n",
      "TimeSinceStart : 407.1037108898163\n",
      "Training Loss : 1.1644036769866943\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 113001\n",
      "mean reward (100 episodes) -45.137426\n",
      "best mean reward -45.137426\n",
      "running time 410.958322\n",
      "Train_EnvstepsSoFar : 113001\n",
      "Train_AverageReturn : -45.13742610206461\n",
      "Train_BestReturn : -45.13742610206461\n",
      "TimeSinceStart : 410.9583218097687\n",
      "Training Loss : 0.7426124215126038\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 114001\n",
      "mean reward (100 episodes) -40.772661\n",
      "best mean reward -40.772661\n",
      "running time 414.169818\n",
      "Train_EnvstepsSoFar : 114001\n",
      "Train_AverageReturn : -40.772661125244596\n",
      "Train_BestReturn : -40.772661125244596\n",
      "TimeSinceStart : 414.16981768608093\n",
      "Training Loss : 1.0022165775299072\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 115001\n",
      "mean reward (100 episodes) -41.889907\n",
      "best mean reward -40.772661\n",
      "running time 417.680287\n",
      "Train_EnvstepsSoFar : 115001\n",
      "Train_AverageReturn : -41.889906600332786\n",
      "Train_BestReturn : -40.772661125244596\n",
      "TimeSinceStart : 417.68028688430786\n",
      "Training Loss : 0.256264328956604\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 116001\n",
      "mean reward (100 episodes) -35.203860\n",
      "best mean reward -35.203860\n",
      "running time 420.743853\n",
      "Train_EnvstepsSoFar : 116001\n",
      "Train_AverageReturn : -35.203859594992956\n",
      "Train_BestReturn : -35.203859594992956\n",
      "TimeSinceStart : 420.74385261535645\n",
      "Training Loss : 0.18097203969955444\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 117001\n",
      "mean reward (100 episodes) -35.641953\n",
      "best mean reward -35.203860\n",
      "running time 424.673401\n",
      "Train_EnvstepsSoFar : 117001\n",
      "Train_AverageReturn : -35.64195288563747\n",
      "Train_BestReturn : -35.203859594992956\n",
      "TimeSinceStart : 424.67340087890625\n",
      "Training Loss : 0.12933552265167236\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 118001\n",
      "mean reward (100 episodes) -35.196248\n",
      "best mean reward -35.196248\n",
      "running time 428.447198\n",
      "Train_EnvstepsSoFar : 118001\n",
      "Train_AverageReturn : -35.19624800112137\n",
      "Train_BestReturn : -35.19624800112137\n",
      "TimeSinceStart : 428.44719791412354\n",
      "Training Loss : 0.37080448865890503\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 119001\n",
      "mean reward (100 episodes) -34.374748\n",
      "best mean reward -34.374748\n",
      "running time 431.895538\n",
      "Train_EnvstepsSoFar : 119001\n",
      "Train_AverageReturn : -34.37474825604369\n",
      "Train_BestReturn : -34.37474825604369\n",
      "TimeSinceStart : 431.89553785324097\n",
      "Training Loss : 0.19142356514930725\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 120001\n",
      "mean reward (100 episodes) -31.221478\n",
      "best mean reward -31.221478\n",
      "running time 435.863905\n",
      "Train_EnvstepsSoFar : 120001\n",
      "Train_AverageReturn : -31.221478390138962\n",
      "Train_BestReturn : -31.221478390138962\n",
      "TimeSinceStart : 435.86390471458435\n",
      "Training Loss : 0.17267338931560516\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 121001\n",
      "mean reward (100 episodes) -29.385033\n",
      "best mean reward -29.385033\n",
      "running time 438.900706\n",
      "Train_EnvstepsSoFar : 121001\n",
      "Train_AverageReturn : -29.38503317345494\n",
      "Train_BestReturn : -29.38503317345494\n",
      "TimeSinceStart : 438.9007058143616\n",
      "Training Loss : 1.1094623804092407\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 122001\n",
      "mean reward (100 episodes) -29.442685\n",
      "best mean reward -29.385033\n",
      "running time 442.544738\n",
      "Train_EnvstepsSoFar : 122001\n",
      "Train_AverageReturn : -29.44268486352386\n",
      "Train_BestReturn : -29.38503317345494\n",
      "TimeSinceStart : 442.54473781585693\n",
      "Training Loss : 0.17173713445663452\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 123001\n",
      "mean reward (100 episodes) -31.109871\n",
      "best mean reward -29.385033\n",
      "running time 446.025940\n",
      "Train_EnvstepsSoFar : 123001\n",
      "Train_AverageReturn : -31.109870844344353\n",
      "Train_BestReturn : -29.38503317345494\n",
      "TimeSinceStart : 446.02593994140625\n",
      "Training Loss : 0.12939196825027466\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 124001\n",
      "mean reward (100 episodes) -27.742746\n",
      "best mean reward -27.742746\n",
      "running time 449.467129\n",
      "Train_EnvstepsSoFar : 124001\n",
      "Train_AverageReturn : -27.742746145336096\n",
      "Train_BestReturn : -27.742746145336096\n",
      "TimeSinceStart : 449.4671287536621\n",
      "Training Loss : 0.6634666323661804\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 125001\n",
      "mean reward (100 episodes) -24.794017\n",
      "best mean reward -24.794017\n",
      "running time 452.486370\n",
      "Train_EnvstepsSoFar : 125001\n",
      "Train_AverageReturn : -24.794016786095074\n",
      "Train_BestReturn : -24.794016786095074\n",
      "TimeSinceStart : 452.48636960983276\n",
      "Training Loss : 1.6104508638381958\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 126001\n",
      "mean reward (100 episodes) -25.680523\n",
      "best mean reward -24.794017\n",
      "running time 456.867592\n",
      "Train_EnvstepsSoFar : 126001\n",
      "Train_AverageReturn : -25.680522500778256\n",
      "Train_BestReturn : -24.794016786095074\n",
      "TimeSinceStart : 456.86759185791016\n",
      "Training Loss : 0.16244995594024658\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 127001\n",
      "mean reward (100 episodes) -21.226605\n",
      "best mean reward -21.226605\n",
      "running time 460.042663\n",
      "Train_EnvstepsSoFar : 127001\n",
      "Train_AverageReturn : -21.226604811366688\n",
      "Train_BestReturn : -21.226604811366688\n",
      "TimeSinceStart : 460.042662858963\n",
      "Training Loss : 0.19525179266929626\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 128001\n",
      "mean reward (100 episodes) -18.429714\n",
      "best mean reward -18.429714\n",
      "running time 463.730651\n",
      "Train_EnvstepsSoFar : 128001\n",
      "Train_AverageReturn : -18.42971396150393\n",
      "Train_BestReturn : -18.42971396150393\n",
      "TimeSinceStart : 463.73065090179443\n",
      "Training Loss : 0.09631728380918503\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 129001\n",
      "mean reward (100 episodes) -16.524502\n",
      "best mean reward -16.524502\n",
      "running time 467.065231\n",
      "Train_EnvstepsSoFar : 129001\n",
      "Train_AverageReturn : -16.524501633575372\n",
      "Train_BestReturn : -16.524501633575372\n",
      "TimeSinceStart : 467.06523084640503\n",
      "Training Loss : 0.06819634884595871\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 130001\n",
      "mean reward (100 episodes) -17.064421\n",
      "best mean reward -16.524502\n",
      "running time 471.316827\n",
      "Train_EnvstepsSoFar : 130001\n",
      "Train_AverageReturn : -17.064420967297984\n",
      "Train_BestReturn : -16.524501633575372\n",
      "TimeSinceStart : 471.31682682037354\n",
      "Training Loss : 0.6171684861183167\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 131001\n",
      "mean reward (100 episodes) -19.750148\n",
      "best mean reward -16.524502\n",
      "running time 474.907575\n",
      "Train_EnvstepsSoFar : 131001\n",
      "Train_AverageReturn : -19.750148115861524\n",
      "Train_BestReturn : -16.524501633575372\n",
      "TimeSinceStart : 474.9075746536255\n",
      "Training Loss : 0.34535378217697144\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 132001\n",
      "mean reward (100 episodes) -20.712890\n",
      "best mean reward -16.524502\n",
      "running time 478.357199\n",
      "Train_EnvstepsSoFar : 132001\n",
      "Train_AverageReturn : -20.71289047743249\n",
      "Train_BestReturn : -16.524501633575372\n",
      "TimeSinceStart : 478.35719871520996\n",
      "Training Loss : 0.8116886615753174\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 133001\n",
      "mean reward (100 episodes) -16.659563\n",
      "best mean reward -16.524502\n",
      "running time 482.203014\n",
      "Train_EnvstepsSoFar : 133001\n",
      "Train_AverageReturn : -16.65956297738928\n",
      "Train_BestReturn : -16.524501633575372\n",
      "TimeSinceStart : 482.20301389694214\n",
      "Training Loss : 0.10354740917682648\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 134001\n",
      "mean reward (100 episodes) -18.586393\n",
      "best mean reward -16.524502\n",
      "running time 485.956676\n",
      "Train_EnvstepsSoFar : 134001\n",
      "Train_AverageReturn : -18.586393208638654\n",
      "Train_BestReturn : -16.524501633575372\n",
      "TimeSinceStart : 485.95667576789856\n",
      "Training Loss : 0.6944354772567749\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 135001\n",
      "mean reward (100 episodes) -16.733884\n",
      "best mean reward -16.524502\n",
      "running time 489.592245\n",
      "Train_EnvstepsSoFar : 135001\n",
      "Train_AverageReturn : -16.733883839239155\n",
      "Train_BestReturn : -16.524501633575372\n",
      "TimeSinceStart : 489.59224486351013\n",
      "Training Loss : 0.4501585066318512\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 136001\n",
      "mean reward (100 episodes) -18.756725\n",
      "best mean reward -16.524502\n",
      "running time 493.943577\n",
      "Train_EnvstepsSoFar : 136001\n",
      "Train_AverageReturn : -18.75672498842205\n",
      "Train_BestReturn : -16.524501633575372\n",
      "TimeSinceStart : 493.94357681274414\n",
      "Training Loss : 0.0892878919839859\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 137001\n",
      "mean reward (100 episodes) -15.531476\n",
      "best mean reward -15.531476\n",
      "running time 497.578325\n",
      "Train_EnvstepsSoFar : 137001\n",
      "Train_AverageReturn : -15.531476320818916\n",
      "Train_BestReturn : -15.531476320818916\n",
      "TimeSinceStart : 497.57832503318787\n",
      "Training Loss : 0.1700536012649536\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 138001\n",
      "mean reward (100 episodes) -18.381184\n",
      "best mean reward -15.531476\n",
      "running time 501.695062\n",
      "Train_EnvstepsSoFar : 138001\n",
      "Train_AverageReturn : -18.381184012134224\n",
      "Train_BestReturn : -15.531476320818916\n",
      "TimeSinceStart : 501.69506192207336\n",
      "Training Loss : 0.23229196667671204\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 139001\n",
      "mean reward (100 episodes) -15.317495\n",
      "best mean reward -15.317495\n",
      "running time 504.803907\n",
      "Train_EnvstepsSoFar : 139001\n",
      "Train_AverageReturn : -15.31749462190508\n",
      "Train_BestReturn : -15.31749462190508\n",
      "TimeSinceStart : 504.80390667915344\n",
      "Training Loss : 0.7297132611274719\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 140001\n",
      "mean reward (100 episodes) -15.316748\n",
      "best mean reward -15.316748\n",
      "running time 509.607102\n",
      "Train_EnvstepsSoFar : 140001\n",
      "Train_AverageReturn : -15.316747616629895\n",
      "Train_BestReturn : -15.316747616629895\n",
      "TimeSinceStart : 509.60710191726685\n",
      "Training Loss : 0.33518505096435547\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 141001\n",
      "mean reward (100 episodes) -17.620709\n",
      "best mean reward -15.316748\n",
      "running time 512.923426\n",
      "Train_EnvstepsSoFar : 141001\n",
      "Train_AverageReturn : -17.620708832507923\n",
      "Train_BestReturn : -15.316747616629895\n",
      "TimeSinceStart : 512.9234256744385\n",
      "Training Loss : 0.8136016726493835\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 142001\n",
      "mean reward (100 episodes) -19.600711\n",
      "best mean reward -15.316748\n",
      "running time 517.394973\n",
      "Train_EnvstepsSoFar : 142001\n",
      "Train_AverageReturn : -19.600711269946594\n",
      "Train_BestReturn : -15.316747616629895\n",
      "TimeSinceStart : 517.3949728012085\n",
      "Training Loss : 0.1495821475982666\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 143001\n",
      "mean reward (100 episodes) -16.058969\n",
      "best mean reward -15.316748\n",
      "running time 520.537292\n",
      "Train_EnvstepsSoFar : 143001\n",
      "Train_AverageReturn : -16.058968823183847\n",
      "Train_BestReturn : -15.316747616629895\n",
      "TimeSinceStart : 520.537291765213\n",
      "Training Loss : 1.1483839750289917\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 144001\n",
      "mean reward (100 episodes) -13.633873\n",
      "best mean reward -13.633873\n",
      "running time 523.880166\n",
      "Train_EnvstepsSoFar : 144001\n",
      "Train_AverageReturn : -13.633872824321578\n",
      "Train_BestReturn : -13.633872824321578\n",
      "TimeSinceStart : 523.8801658153534\n",
      "Training Loss : 0.12119452655315399\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 145001\n",
      "mean reward (100 episodes) -13.187645\n",
      "best mean reward -13.187645\n",
      "running time 527.336846\n",
      "Train_EnvstepsSoFar : 145001\n",
      "Train_AverageReturn : -13.187644564036432\n",
      "Train_BestReturn : -13.187644564036432\n",
      "TimeSinceStart : 527.3368456363678\n",
      "Training Loss : 0.593194842338562\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 146001\n",
      "mean reward (100 episodes) -14.481759\n",
      "best mean reward -13.187645\n",
      "running time 531.435863\n",
      "Train_EnvstepsSoFar : 146001\n",
      "Train_AverageReturn : -14.48175930856927\n",
      "Train_BestReturn : -13.187644564036432\n",
      "TimeSinceStart : 531.4358627796173\n",
      "Training Loss : 4.172546863555908\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 147001\n",
      "mean reward (100 episodes) -14.897099\n",
      "best mean reward -13.187645\n",
      "running time 535.721815\n",
      "Train_EnvstepsSoFar : 147001\n",
      "Train_AverageReturn : -14.897099305880339\n",
      "Train_BestReturn : -13.187644564036432\n",
      "TimeSinceStart : 535.7218146324158\n",
      "Training Loss : 0.15859028697013855\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 148001\n",
      "mean reward (100 episodes) -12.424397\n",
      "best mean reward -12.424397\n",
      "running time 539.753863\n",
      "Train_EnvstepsSoFar : 148001\n",
      "Train_AverageReturn : -12.424396594574386\n",
      "Train_BestReturn : -12.424396594574386\n",
      "TimeSinceStart : 539.7538628578186\n",
      "Training Loss : 0.08450084179639816\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 149001\n",
      "mean reward (100 episodes) -14.611779\n",
      "best mean reward -12.424397\n",
      "running time 543.279794\n",
      "Train_EnvstepsSoFar : 149001\n",
      "Train_AverageReturn : -14.611779306327408\n",
      "Train_BestReturn : -12.424396594574386\n",
      "TimeSinceStart : 543.2797937393188\n",
      "Training Loss : 0.20548787713050842\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 150001\n",
      "mean reward (100 episodes) -13.149088\n",
      "best mean reward -12.424397\n",
      "running time 546.735853\n",
      "Train_EnvstepsSoFar : 150001\n",
      "Train_AverageReturn : -13.14908817883594\n",
      "Train_BestReturn : -12.424396594574386\n",
      "TimeSinceStart : 546.7358527183533\n",
      "Training Loss : 1.1623061895370483\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 151001\n",
      "mean reward (100 episodes) -10.191854\n",
      "best mean reward -10.191854\n",
      "running time 549.978654\n",
      "Train_EnvstepsSoFar : 151001\n",
      "Train_AverageReturn : -10.191853933405486\n",
      "Train_BestReturn : -10.191853933405486\n",
      "TimeSinceStart : 549.9786539077759\n",
      "Training Loss : 0.11680860817432404\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 152001\n",
      "mean reward (100 episodes) -7.884496\n",
      "best mean reward -7.884496\n",
      "running time 553.526064\n",
      "Train_EnvstepsSoFar : 152001\n",
      "Train_AverageReturn : -7.8844956612828385\n",
      "Train_BestReturn : -7.8844956612828385\n",
      "TimeSinceStart : 553.5260639190674\n",
      "Training Loss : 0.18366768956184387\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 153001\n",
      "mean reward (100 episodes) -6.947341\n",
      "best mean reward -6.947341\n",
      "running time 556.984120\n",
      "Train_EnvstepsSoFar : 153001\n",
      "Train_AverageReturn : -6.9473412658907305\n",
      "Train_BestReturn : -6.9473412658907305\n",
      "TimeSinceStart : 556.9841196537018\n",
      "Training Loss : 0.10380423069000244\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 154001\n",
      "mean reward (100 episodes) -9.540275\n",
      "best mean reward -6.947341\n",
      "running time 561.919761\n",
      "Train_EnvstepsSoFar : 154001\n",
      "Train_AverageReturn : -9.540275393464023\n",
      "Train_BestReturn : -6.9473412658907305\n",
      "TimeSinceStart : 561.9197609424591\n",
      "Training Loss : 0.40078502893447876\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 155001\n",
      "mean reward (100 episodes) -9.722644\n",
      "best mean reward -6.947341\n",
      "running time 565.770245\n",
      "Train_EnvstepsSoFar : 155001\n",
      "Train_AverageReturn : -9.722643698291972\n",
      "Train_BestReturn : -6.9473412658907305\n",
      "TimeSinceStart : 565.7702448368073\n",
      "Training Loss : 0.10969065874814987\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 156001\n",
      "mean reward (100 episodes) -8.478115\n",
      "best mean reward -6.947341\n",
      "running time 569.694950\n",
      "Train_EnvstepsSoFar : 156001\n",
      "Train_AverageReturn : -8.478114987188185\n",
      "Train_BestReturn : -6.9473412658907305\n",
      "TimeSinceStart : 569.6949498653412\n",
      "Training Loss : 0.13217952847480774\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 157001\n",
      "mean reward (100 episodes) -5.311748\n",
      "best mean reward -5.311748\n",
      "running time 573.315838\n",
      "Train_EnvstepsSoFar : 157001\n",
      "Train_AverageReturn : -5.311748187092747\n",
      "Train_BestReturn : -5.311748187092747\n",
      "TimeSinceStart : 573.3158378601074\n",
      "Training Loss : 0.09902116656303406\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 158001\n",
      "mean reward (100 episodes) -2.817611\n",
      "best mean reward -2.817611\n",
      "running time 576.346345\n",
      "Train_EnvstepsSoFar : 158001\n",
      "Train_AverageReturn : -2.817611495900163\n",
      "Train_BestReturn : -2.817611495900163\n",
      "TimeSinceStart : 576.3463449478149\n",
      "Training Loss : 0.08617815375328064\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 159001\n",
      "mean reward (100 episodes) -1.672683\n",
      "best mean reward -1.672683\n",
      "running time 580.995678\n",
      "Train_EnvstepsSoFar : 159001\n",
      "Train_AverageReturn : -1.6726831151768515\n",
      "Train_BestReturn : -1.6726831151768515\n",
      "TimeSinceStart : 580.995677947998\n",
      "Training Loss : 0.059554874897003174\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 160001\n",
      "mean reward (100 episodes) -1.646762\n",
      "best mean reward -1.646762\n",
      "running time 584.804466\n",
      "Train_EnvstepsSoFar : 160001\n",
      "Train_AverageReturn : -1.6467617716366134\n",
      "Train_BestReturn : -1.6467617716366134\n",
      "TimeSinceStart : 584.8044657707214\n",
      "Training Loss : 0.08958636224269867\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 161001\n",
      "mean reward (100 episodes) -1.184352\n",
      "best mean reward -1.184352\n",
      "running time 588.491768\n",
      "Train_EnvstepsSoFar : 161001\n",
      "Train_AverageReturn : -1.1843519478651063\n",
      "Train_BestReturn : -1.1843519478651063\n",
      "TimeSinceStart : 588.4917676448822\n",
      "Training Loss : 2.3917527198791504\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 162001\n",
      "mean reward (100 episodes) 0.150533\n",
      "best mean reward 0.150533\n",
      "running time 593.152305\n",
      "Train_EnvstepsSoFar : 162001\n",
      "Train_AverageReturn : 0.15053305237956083\n",
      "Train_BestReturn : 0.15053305237956083\n",
      "TimeSinceStart : 593.1523048877716\n",
      "Training Loss : 0.15125957131385803\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 163001\n",
      "mean reward (100 episodes) -0.733783\n",
      "best mean reward 0.150533\n",
      "running time 597.204168\n",
      "Train_EnvstepsSoFar : 163001\n",
      "Train_AverageReturn : -0.7337826304083633\n",
      "Train_BestReturn : 0.15053305237956083\n",
      "TimeSinceStart : 597.204167842865\n",
      "Training Loss : 0.15100662410259247\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 164001\n",
      "mean reward (100 episodes) 8.054668\n",
      "best mean reward 8.054668\n",
      "running time 600.039648\n",
      "Train_EnvstepsSoFar : 164001\n",
      "Train_AverageReturn : 8.054668261784782\n",
      "Train_BestReturn : 8.054668261784782\n",
      "TimeSinceStart : 600.0396478176117\n",
      "Training Loss : 0.1400560736656189\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 165001\n",
      "mean reward (100 episodes) 11.663954\n",
      "best mean reward 11.663954\n",
      "running time 604.268070\n",
      "Train_EnvstepsSoFar : 165001\n",
      "Train_AverageReturn : 11.66395415587115\n",
      "Train_BestReturn : 11.66395415587115\n",
      "TimeSinceStart : 604.2680697441101\n",
      "Training Loss : 0.14159752428531647\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 166001\n",
      "mean reward (100 episodes) 15.256173\n",
      "best mean reward 15.256173\n",
      "running time 608.231180\n",
      "Train_EnvstepsSoFar : 166001\n",
      "Train_AverageReturn : 15.256173435375818\n",
      "Train_BestReturn : 15.256173435375818\n",
      "TimeSinceStart : 608.2311797142029\n",
      "Training Loss : 0.13883596658706665\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 167001\n",
      "mean reward (100 episodes) 16.854025\n",
      "best mean reward 16.854025\n",
      "running time 611.277378\n",
      "Train_EnvstepsSoFar : 167001\n",
      "Train_AverageReturn : 16.854024674923718\n",
      "Train_BestReturn : 16.854024674923718\n",
      "TimeSinceStart : 611.2773776054382\n",
      "Training Loss : 0.17638850212097168\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 168001\n",
      "mean reward (100 episodes) 17.639259\n",
      "best mean reward 17.639259\n",
      "running time 614.948134\n",
      "Train_EnvstepsSoFar : 168001\n",
      "Train_AverageReturn : 17.639259286227333\n",
      "Train_BestReturn : 17.639259286227333\n",
      "TimeSinceStart : 614.9481337070465\n",
      "Training Loss : 0.08181784301996231\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 169001\n",
      "mean reward (100 episodes) 19.237051\n",
      "best mean reward 19.237051\n",
      "running time 618.557392\n",
      "Train_EnvstepsSoFar : 169001\n",
      "Train_AverageReturn : 19.23705115003884\n",
      "Train_BestReturn : 19.23705115003884\n",
      "TimeSinceStart : 618.5573918819427\n",
      "Training Loss : 0.07624319940805435\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 170001\n",
      "mean reward (100 episodes) 25.675955\n",
      "best mean reward 25.675955\n",
      "running time 621.905232\n",
      "Train_EnvstepsSoFar : 170001\n",
      "Train_AverageReturn : 25.675954745144473\n",
      "Train_BestReturn : 25.675954745144473\n",
      "TimeSinceStart : 621.9052317142487\n",
      "Training Loss : 0.10032662749290466\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 171001\n",
      "mean reward (100 episodes) 30.175354\n",
      "best mean reward 30.175354\n",
      "running time 625.161237\n",
      "Train_EnvstepsSoFar : 171001\n",
      "Train_AverageReturn : 30.175353918523665\n",
      "Train_BestReturn : 30.175353918523665\n",
      "TimeSinceStart : 625.1612370014191\n",
      "Training Loss : 0.14264778792858124\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 172001\n",
      "mean reward (100 episodes) 31.381145\n",
      "best mean reward 31.381145\n",
      "running time 629.115766\n",
      "Train_EnvstepsSoFar : 172001\n",
      "Train_AverageReturn : 31.38114476194019\n",
      "Train_BestReturn : 31.38114476194019\n",
      "TimeSinceStart : 629.1157660484314\n",
      "Training Loss : 1.0936166048049927\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 173001\n",
      "mean reward (100 episodes) 38.704429\n",
      "best mean reward 38.704429\n",
      "running time 632.001638\n",
      "Train_EnvstepsSoFar : 173001\n",
      "Train_AverageReturn : 38.70442883591717\n",
      "Train_BestReturn : 38.70442883591717\n",
      "TimeSinceStart : 632.0016379356384\n",
      "Training Loss : 0.17641428112983704\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 174001\n",
      "mean reward (100 episodes) 38.544702\n",
      "best mean reward 38.704429\n",
      "running time 635.137940\n",
      "Train_EnvstepsSoFar : 174001\n",
      "Train_AverageReturn : 38.544702178336095\n",
      "Train_BestReturn : 38.70442883591717\n",
      "TimeSinceStart : 635.1379399299622\n",
      "Training Loss : 1.0925629138946533\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 175001\n",
      "mean reward (100 episodes) 37.938189\n",
      "best mean reward 38.704429\n",
      "running time 639.163167\n",
      "Train_EnvstepsSoFar : 175001\n",
      "Train_AverageReturn : 37.93818855280924\n",
      "Train_BestReturn : 38.70442883591717\n",
      "TimeSinceStart : 639.1631669998169\n",
      "Training Loss : 0.09255697578191757\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 176001\n",
      "mean reward (100 episodes) 40.962902\n",
      "best mean reward 40.962902\n",
      "running time 642.984506\n",
      "Train_EnvstepsSoFar : 176001\n",
      "Train_AverageReturn : 40.96290201562405\n",
      "Train_BestReturn : 40.96290201562405\n",
      "TimeSinceStart : 642.9845058917999\n",
      "Training Loss : 0.09360496699810028\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 177001\n",
      "mean reward (100 episodes) 40.825404\n",
      "best mean reward 40.962902\n",
      "running time 647.949181\n",
      "Train_EnvstepsSoFar : 177001\n",
      "Train_AverageReturn : 40.82540428900028\n",
      "Train_BestReturn : 40.96290201562405\n",
      "TimeSinceStart : 647.9491808414459\n",
      "Training Loss : 0.13814903795719147\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 178001\n",
      "mean reward (100 episodes) 42.069591\n",
      "best mean reward 42.069591\n",
      "running time 651.548230\n",
      "Train_EnvstepsSoFar : 178001\n",
      "Train_AverageReturn : 42.06959138528551\n",
      "Train_BestReturn : 42.06959138528551\n",
      "TimeSinceStart : 651.548229932785\n",
      "Training Loss : 0.13533543050289154\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 179001\n",
      "mean reward (100 episodes) 41.695378\n",
      "best mean reward 42.069591\n",
      "running time 654.668233\n",
      "Train_EnvstepsSoFar : 179001\n",
      "Train_AverageReturn : 41.695378105190905\n",
      "Train_BestReturn : 42.06959138528551\n",
      "TimeSinceStart : 654.6682329177856\n",
      "Training Loss : 1.2073055505752563\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 180001\n",
      "mean reward (100 episodes) 42.378191\n",
      "best mean reward 42.378191\n",
      "running time 658.773942\n",
      "Train_EnvstepsSoFar : 180001\n",
      "Train_AverageReturn : 42.37819146475237\n",
      "Train_BestReturn : 42.37819146475237\n",
      "TimeSinceStart : 658.7739419937134\n",
      "Training Loss : 1.3037084341049194\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 181001\n",
      "mean reward (100 episodes) 42.676262\n",
      "best mean reward 42.676262\n",
      "running time 662.326650\n",
      "Train_EnvstepsSoFar : 181001\n",
      "Train_AverageReturn : 42.67626201236817\n",
      "Train_BestReturn : 42.67626201236817\n",
      "TimeSinceStart : 662.3266496658325\n",
      "Training Loss : 0.15480907261371613\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 182001\n",
      "mean reward (100 episodes) 47.726087\n",
      "best mean reward 47.726087\n",
      "running time 665.315186\n",
      "Train_EnvstepsSoFar : 182001\n",
      "Train_AverageReturn : 47.72608738283201\n",
      "Train_BestReturn : 47.72608738283201\n",
      "TimeSinceStart : 665.3151857852936\n",
      "Training Loss : 0.12058776617050171\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 183001\n",
      "mean reward (100 episodes) 46.516733\n",
      "best mean reward 47.726087\n",
      "running time 668.869836\n",
      "Train_EnvstepsSoFar : 183001\n",
      "Train_AverageReturn : 46.51673276865479\n",
      "Train_BestReturn : 47.72608738283201\n",
      "TimeSinceStart : 668.8698356151581\n",
      "Training Loss : 1.5949746370315552\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 184001\n",
      "mean reward (100 episodes) 48.628950\n",
      "best mean reward 48.628950\n",
      "running time 672.243211\n",
      "Train_EnvstepsSoFar : 184001\n",
      "Train_AverageReturn : 48.62895041899947\n",
      "Train_BestReturn : 48.62895041899947\n",
      "TimeSinceStart : 672.2432110309601\n",
      "Training Loss : 3.1004064083099365\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 185001\n",
      "mean reward (100 episodes) 51.348255\n",
      "best mean reward 51.348255\n",
      "running time 676.126621\n",
      "Train_EnvstepsSoFar : 185001\n",
      "Train_AverageReturn : 51.348254656813964\n",
      "Train_BestReturn : 51.348254656813964\n",
      "TimeSinceStart : 676.1266207695007\n",
      "Training Loss : 0.1987903118133545\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 186001\n",
      "mean reward (100 episodes) 50.536755\n",
      "best mean reward 51.348255\n",
      "running time 679.935926\n",
      "Train_EnvstepsSoFar : 186001\n",
      "Train_AverageReturn : 50.53675477973785\n",
      "Train_BestReturn : 51.348254656813964\n",
      "TimeSinceStart : 679.9359259605408\n",
      "Training Loss : 0.39734241366386414\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 187001\n",
      "mean reward (100 episodes) 47.587514\n",
      "best mean reward 51.348255\n",
      "running time 683.327477\n",
      "Train_EnvstepsSoFar : 187001\n",
      "Train_AverageReturn : 47.5875140134928\n",
      "Train_BestReturn : 51.348254656813964\n",
      "TimeSinceStart : 683.327476978302\n",
      "Training Loss : 0.08017843961715698\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 188001\n",
      "mean reward (100 episodes) 48.351570\n",
      "best mean reward 51.348255\n",
      "running time 687.859765\n",
      "Train_EnvstepsSoFar : 188001\n",
      "Train_AverageReturn : 48.351570422958346\n",
      "Train_BestReturn : 51.348254656813964\n",
      "TimeSinceStart : 687.8597648143768\n",
      "Training Loss : 0.42484694719314575\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 189001\n",
      "mean reward (100 episodes) 49.201803\n",
      "best mean reward 51.348255\n",
      "running time 691.404044\n",
      "Train_EnvstepsSoFar : 189001\n",
      "Train_AverageReturn : 49.20180338859127\n",
      "Train_BestReturn : 51.348254656813964\n",
      "TimeSinceStart : 691.404043674469\n",
      "Training Loss : 0.2455858439207077\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 190001\n",
      "mean reward (100 episodes) 47.753680\n",
      "best mean reward 51.348255\n",
      "running time 695.001136\n",
      "Train_EnvstepsSoFar : 190001\n",
      "Train_AverageReturn : 47.7536800566264\n",
      "Train_BestReturn : 51.348254656813964\n",
      "TimeSinceStart : 695.0011358261108\n",
      "Training Loss : 1.1923624277114868\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 191001\n",
      "mean reward (100 episodes) 51.692954\n",
      "best mean reward 51.692954\n",
      "running time 697.726024\n",
      "Train_EnvstepsSoFar : 191001\n",
      "Train_AverageReturn : 51.69295413212052\n",
      "Train_BestReturn : 51.69295413212052\n",
      "TimeSinceStart : 697.7260239124298\n",
      "Training Loss : 0.09506005793809891\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 192001\n",
      "mean reward (100 episodes) 54.378080\n",
      "best mean reward 54.378080\n",
      "running time 701.461520\n",
      "Train_EnvstepsSoFar : 192001\n",
      "Train_AverageReturn : 54.37808045959465\n",
      "Train_BestReturn : 54.37808045959465\n",
      "TimeSinceStart : 701.4615197181702\n",
      "Training Loss : 0.2890748381614685\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 193001\n",
      "mean reward (100 episodes) 57.884391\n",
      "best mean reward 57.884391\n",
      "running time 705.162231\n",
      "Train_EnvstepsSoFar : 193001\n",
      "Train_AverageReturn : 57.88439117767311\n",
      "Train_BestReturn : 57.88439117767311\n",
      "TimeSinceStart : 705.1622307300568\n",
      "Training Loss : 3.2948408126831055\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 194001\n",
      "mean reward (100 episodes) 57.807046\n",
      "best mean reward 57.884391\n",
      "running time 709.637547\n",
      "Train_EnvstepsSoFar : 194001\n",
      "Train_AverageReturn : 57.807046158209495\n",
      "Train_BestReturn : 57.88439117767311\n",
      "TimeSinceStart : 709.6375470161438\n",
      "Training Loss : 0.36004310846328735\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 195001\n",
      "mean reward (100 episodes) 58.849151\n",
      "best mean reward 58.849151\n",
      "running time 713.405689\n",
      "Train_EnvstepsSoFar : 195001\n",
      "Train_AverageReturn : 58.849151049270695\n",
      "Train_BestReturn : 58.849151049270695\n",
      "TimeSinceStart : 713.4056887626648\n",
      "Training Loss : 0.4695468544960022\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 196001\n",
      "mean reward (100 episodes) 65.602352\n",
      "best mean reward 65.602352\n",
      "running time 716.285236\n",
      "Train_EnvstepsSoFar : 196001\n",
      "Train_AverageReturn : 65.60235209027606\n",
      "Train_BestReturn : 65.60235209027606\n",
      "TimeSinceStart : 716.2852358818054\n",
      "Training Loss : 0.16031789779663086\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 197001\n",
      "mean reward (100 episodes) 66.279256\n",
      "best mean reward 66.279256\n",
      "running time 719.124256\n",
      "Train_EnvstepsSoFar : 197001\n",
      "Train_AverageReturn : 66.27925614717701\n",
      "Train_BestReturn : 66.27925614717701\n",
      "TimeSinceStart : 719.1242558956146\n",
      "Training Loss : 0.08778664469718933\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 198001\n",
      "mean reward (100 episodes) 66.843976\n",
      "best mean reward 66.843976\n",
      "running time 722.565748\n",
      "Train_EnvstepsSoFar : 198001\n",
      "Train_AverageReturn : 66.8439760181735\n",
      "Train_BestReturn : 66.8439760181735\n",
      "TimeSinceStart : 722.5657479763031\n",
      "Training Loss : 0.7108044624328613\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 199001\n",
      "mean reward (100 episodes) 64.373533\n",
      "best mean reward 66.843976\n",
      "running time 726.027502\n",
      "Train_EnvstepsSoFar : 199001\n",
      "Train_AverageReturn : 64.37353308335678\n",
      "Train_BestReturn : 66.8439760181735\n",
      "TimeSinceStart : 726.027501821518\n",
      "Training Loss : 0.1251034289598465\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 200001\n",
      "mean reward (100 episodes) 62.619008\n",
      "best mean reward 66.843976\n",
      "running time 729.341247\n",
      "Train_EnvstepsSoFar : 200001\n",
      "Train_AverageReturn : 62.619008310623514\n",
      "Train_BestReturn : 66.8439760181735\n",
      "TimeSinceStart : 729.341246843338\n",
      "Training Loss : 0.12291577458381653\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 201001\n",
      "mean reward (100 episodes) 67.304472\n",
      "best mean reward 67.304472\n",
      "running time 732.976446\n",
      "Train_EnvstepsSoFar : 201001\n",
      "Train_AverageReturn : 67.30447218263207\n",
      "Train_BestReturn : 67.30447218263207\n",
      "TimeSinceStart : 732.9764459133148\n",
      "Training Loss : 0.14895780384540558\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 202001\n",
      "mean reward (100 episodes) 66.115066\n",
      "best mean reward 67.304472\n",
      "running time 735.901050\n",
      "Train_EnvstepsSoFar : 202001\n",
      "Train_AverageReturn : 66.11506562678984\n",
      "Train_BestReturn : 67.30447218263207\n",
      "TimeSinceStart : 735.9010498523712\n",
      "Training Loss : 0.3520869314670563\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 203001\n",
      "mean reward (100 episodes) 66.941757\n",
      "best mean reward 67.304472\n",
      "running time 739.028356\n",
      "Train_EnvstepsSoFar : 203001\n",
      "Train_AverageReturn : 66.94175692472076\n",
      "Train_BestReturn : 67.30447218263207\n",
      "TimeSinceStart : 739.0283558368683\n",
      "Training Loss : 0.16884076595306396\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 204001\n",
      "mean reward (100 episodes) 68.464491\n",
      "best mean reward 68.464491\n",
      "running time 742.305003\n",
      "Train_EnvstepsSoFar : 204001\n",
      "Train_AverageReturn : 68.4644906562904\n",
      "Train_BestReturn : 68.4644906562904\n",
      "TimeSinceStart : 742.3050029277802\n",
      "Training Loss : 0.8656246662139893\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 205001\n",
      "mean reward (100 episodes) 68.200248\n",
      "best mean reward 68.464491\n",
      "running time 746.184755\n",
      "Train_EnvstepsSoFar : 205001\n",
      "Train_AverageReturn : 68.20024784564889\n",
      "Train_BestReturn : 68.4644906562904\n",
      "TimeSinceStart : 746.1847548484802\n",
      "Training Loss : 0.12346114218235016\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 206001\n",
      "mean reward (100 episodes) 65.401280\n",
      "best mean reward 68.464491\n",
      "running time 750.358434\n",
      "Train_EnvstepsSoFar : 206001\n",
      "Train_AverageReturn : 65.4012800627727\n",
      "Train_BestReturn : 68.4644906562904\n",
      "TimeSinceStart : 750.3584337234497\n",
      "Training Loss : 1.1195229291915894\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 207001\n",
      "mean reward (100 episodes) 70.273584\n",
      "best mean reward 70.273584\n",
      "running time 753.627834\n",
      "Train_EnvstepsSoFar : 207001\n",
      "Train_AverageReturn : 70.27358438968399\n",
      "Train_BestReturn : 70.27358438968399\n",
      "TimeSinceStart : 753.6278338432312\n",
      "Training Loss : 0.8268890976905823\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 208001\n",
      "mean reward (100 episodes) 69.786143\n",
      "best mean reward 70.273584\n",
      "running time 756.674075\n",
      "Train_EnvstepsSoFar : 208001\n",
      "Train_AverageReturn : 69.7861431901684\n",
      "Train_BestReturn : 70.27358438968399\n",
      "TimeSinceStart : 756.6740748882294\n",
      "Training Loss : 0.8758167028427124\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 209001\n",
      "mean reward (100 episodes) 68.240042\n",
      "best mean reward 70.273584\n",
      "running time 759.539554\n",
      "Train_EnvstepsSoFar : 209001\n",
      "Train_AverageReturn : 68.2400423688897\n",
      "Train_BestReturn : 70.27358438968399\n",
      "TimeSinceStart : 759.539553642273\n",
      "Training Loss : 0.6745308637619019\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 210001\n",
      "mean reward (100 episodes) 70.299562\n",
      "best mean reward 70.299562\n",
      "running time 762.434955\n",
      "Train_EnvstepsSoFar : 210001\n",
      "Train_AverageReturn : 70.29956203254568\n",
      "Train_BestReturn : 70.29956203254568\n",
      "TimeSinceStart : 762.4349548816681\n",
      "Training Loss : 2.1261792182922363\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 211001\n",
      "mean reward (100 episodes) 73.254576\n",
      "best mean reward 73.254576\n",
      "running time 765.425876\n",
      "Train_EnvstepsSoFar : 211001\n",
      "Train_AverageReturn : 73.25457618122213\n",
      "Train_BestReturn : 73.25457618122213\n",
      "TimeSinceStart : 765.4258759021759\n",
      "Training Loss : 0.160627543926239\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 212001\n",
      "mean reward (100 episodes) 74.533291\n",
      "best mean reward 74.533291\n",
      "running time 768.254408\n",
      "Train_EnvstepsSoFar : 212001\n",
      "Train_AverageReturn : 74.53329138875579\n",
      "Train_BestReturn : 74.53329138875579\n",
      "TimeSinceStart : 768.2544078826904\n",
      "Training Loss : 0.22628140449523926\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 213001\n",
      "mean reward (100 episodes) 76.286098\n",
      "best mean reward 76.286098\n",
      "running time 771.103922\n",
      "Train_EnvstepsSoFar : 213001\n",
      "Train_AverageReturn : 76.28609845349386\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 771.1039218902588\n",
      "Training Loss : 0.535236656665802\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 214001\n",
      "mean reward (100 episodes) 69.708875\n",
      "best mean reward 76.286098\n",
      "running time 774.106646\n",
      "Train_EnvstepsSoFar : 214001\n",
      "Train_AverageReturn : 69.70887515202445\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 774.1066460609436\n",
      "Training Loss : 0.3363181948661804\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 215001\n",
      "mean reward (100 episodes) 71.883235\n",
      "best mean reward 76.286098\n",
      "running time 777.262978\n",
      "Train_EnvstepsSoFar : 215001\n",
      "Train_AverageReturn : 71.8832349873485\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 777.2629778385162\n",
      "Training Loss : 1.3378199338912964\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 216001\n",
      "mean reward (100 episodes) 74.605694\n",
      "best mean reward 76.286098\n",
      "running time 780.137955\n",
      "Train_EnvstepsSoFar : 216001\n",
      "Train_AverageReturn : 74.60569401675708\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 780.1379547119141\n",
      "Training Loss : 2.403230667114258\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 217001\n",
      "mean reward (100 episodes) 70.585429\n",
      "best mean reward 76.286098\n",
      "running time 783.050866\n",
      "Train_EnvstepsSoFar : 217001\n",
      "Train_AverageReturn : 70.5854294314827\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 783.0508658885956\n",
      "Training Loss : 0.2999504506587982\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 218001\n",
      "mean reward (100 episodes) 68.933612\n",
      "best mean reward 76.286098\n",
      "running time 786.058256\n",
      "Train_EnvstepsSoFar : 218001\n",
      "Train_AverageReturn : 68.93361197960772\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 786.0582559108734\n",
      "Training Loss : 0.10224970430135727\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 219001\n",
      "mean reward (100 episodes) 65.616842\n",
      "best mean reward 76.286098\n",
      "running time 788.897580\n",
      "Train_EnvstepsSoFar : 219001\n",
      "Train_AverageReturn : 65.6168415015569\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 788.8975796699524\n",
      "Training Loss : 0.19993971288204193\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 220001\n",
      "mean reward (100 episodes) 64.897681\n",
      "best mean reward 76.286098\n",
      "running time 791.997587\n",
      "Train_EnvstepsSoFar : 220001\n",
      "Train_AverageReturn : 64.89768073773132\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 791.9975869655609\n",
      "Training Loss : 0.1783498376607895\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 221001\n",
      "mean reward (100 episodes) 65.824450\n",
      "best mean reward 76.286098\n",
      "running time 794.913528\n",
      "Train_EnvstepsSoFar : 221001\n",
      "Train_AverageReturn : 65.82444994264557\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 794.9135277271271\n",
      "Training Loss : 0.5501843094825745\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 222001\n",
      "mean reward (100 episodes) 57.213024\n",
      "best mean reward 76.286098\n",
      "running time 797.925021\n",
      "Train_EnvstepsSoFar : 222001\n",
      "Train_AverageReturn : 57.213023820645596\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 797.9250209331512\n",
      "Training Loss : 0.6215663552284241\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 223001\n",
      "mean reward (100 episodes) 55.705165\n",
      "best mean reward 76.286098\n",
      "running time 800.919529\n",
      "Train_EnvstepsSoFar : 223001\n",
      "Train_AverageReturn : 55.7051650807522\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 800.9195289611816\n",
      "Training Loss : 0.23316191136837006\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 224001\n",
      "mean reward (100 episodes) 55.290049\n",
      "best mean reward 76.286098\n",
      "running time 803.941665\n",
      "Train_EnvstepsSoFar : 224001\n",
      "Train_AverageReturn : 55.29004872398507\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 803.9416646957397\n",
      "Training Loss : 0.8195841908454895\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 225001\n",
      "mean reward (100 episodes) 59.569126\n",
      "best mean reward 76.286098\n",
      "running time 807.268346\n",
      "Train_EnvstepsSoFar : 225001\n",
      "Train_AverageReturn : 59.569126280940566\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 807.2683458328247\n",
      "Training Loss : 0.9684832692146301\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 226001\n",
      "mean reward (100 episodes) 60.638742\n",
      "best mean reward 76.286098\n",
      "running time 810.287604\n",
      "Train_EnvstepsSoFar : 226001\n",
      "Train_AverageReturn : 60.638742184421325\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 810.287603855133\n",
      "Training Loss : 0.19303739070892334\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 227001\n",
      "mean reward (100 episodes) 56.824194\n",
      "best mean reward 76.286098\n",
      "running time 813.607459\n",
      "Train_EnvstepsSoFar : 227001\n",
      "Train_AverageReturn : 56.824194151617604\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 813.6074588298798\n",
      "Training Loss : 0.6519909501075745\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 228001\n",
      "mean reward (100 episodes) 55.296661\n",
      "best mean reward 76.286098\n",
      "running time 816.735922\n",
      "Train_EnvstepsSoFar : 228001\n",
      "Train_AverageReturn : 55.29666139482771\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 816.7359218597412\n",
      "Training Loss : 1.2627581357955933\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 229001\n",
      "mean reward (100 episodes) 60.086669\n",
      "best mean reward 76.286098\n",
      "running time 819.730045\n",
      "Train_EnvstepsSoFar : 229001\n",
      "Train_AverageReturn : 60.086669137959596\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 819.7300448417664\n",
      "Training Loss : 0.5965883135795593\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 230001\n",
      "mean reward (100 episodes) 57.619266\n",
      "best mean reward 76.286098\n",
      "running time 823.441624\n",
      "Train_EnvstepsSoFar : 230001\n",
      "Train_AverageReturn : 57.6192661607036\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 823.4416236877441\n",
      "Training Loss : 0.1551162600517273\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 231001\n",
      "mean reward (100 episodes) 60.240320\n",
      "best mean reward 76.286098\n",
      "running time 827.165625\n",
      "Train_EnvstepsSoFar : 231001\n",
      "Train_AverageReturn : 60.240320052057584\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 827.1656248569489\n",
      "Training Loss : 1.6408848762512207\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 232001\n",
      "mean reward (100 episodes) 59.702591\n",
      "best mean reward 76.286098\n",
      "running time 833.912592\n",
      "Train_EnvstepsSoFar : 232001\n",
      "Train_AverageReturn : 59.70259112791402\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 833.9125916957855\n",
      "Training Loss : 0.19027642905712128\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 233001\n",
      "mean reward (100 episodes) 62.769831\n",
      "best mean reward 76.286098\n",
      "running time 837.135199\n",
      "Train_EnvstepsSoFar : 233001\n",
      "Train_AverageReturn : 62.76983128446461\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 837.1351988315582\n",
      "Training Loss : 0.10842253267765045\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 234001\n",
      "mean reward (100 episodes) 61.662873\n",
      "best mean reward 76.286098\n",
      "running time 840.114464\n",
      "Train_EnvstepsSoFar : 234001\n",
      "Train_AverageReturn : 61.66287288274322\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 840.1144640445709\n",
      "Training Loss : 0.12905649840831757\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 235001\n",
      "mean reward (100 episodes) 62.896324\n",
      "best mean reward 76.286098\n",
      "running time 843.530999\n",
      "Train_EnvstepsSoFar : 235001\n",
      "Train_AverageReturn : 62.89632375862895\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 843.5309987068176\n",
      "Training Loss : 1.142366647720337\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 236001\n",
      "mean reward (100 episodes) 62.024121\n",
      "best mean reward 76.286098\n",
      "running time 846.645441\n",
      "Train_EnvstepsSoFar : 236001\n",
      "Train_AverageReturn : 62.02412115941965\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 846.6454410552979\n",
      "Training Loss : 0.18070067465305328\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 237001\n",
      "mean reward (100 episodes) 62.352091\n",
      "best mean reward 76.286098\n",
      "running time 849.668664\n",
      "Train_EnvstepsSoFar : 237001\n",
      "Train_AverageReturn : 62.35209058827317\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 849.6686637401581\n",
      "Training Loss : 0.18182216584682465\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 238001\n",
      "mean reward (100 episodes) 62.593385\n",
      "best mean reward 76.286098\n",
      "running time 852.421864\n",
      "Train_EnvstepsSoFar : 238001\n",
      "Train_AverageReturn : 62.593384506053305\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 852.4218637943268\n",
      "Training Loss : 0.4141559600830078\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 239001\n",
      "mean reward (100 episodes) 55.128927\n",
      "best mean reward 76.286098\n",
      "running time 855.198519\n",
      "Train_EnvstepsSoFar : 239001\n",
      "Train_AverageReturn : 55.12892668249336\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 855.1985187530518\n",
      "Training Loss : 0.1842801719903946\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 240001\n",
      "mean reward (100 episodes) 55.009976\n",
      "best mean reward 76.286098\n",
      "running time 858.091973\n",
      "Train_EnvstepsSoFar : 240001\n",
      "Train_AverageReturn : 55.009975866113685\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 858.0919728279114\n",
      "Training Loss : 1.1500493288040161\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 241001\n",
      "mean reward (100 episodes) 56.944458\n",
      "best mean reward 76.286098\n",
      "running time 860.812762\n",
      "Train_EnvstepsSoFar : 241001\n",
      "Train_AverageReturn : 56.94445810257561\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 860.8127617835999\n",
      "Training Loss : 1.0348554849624634\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 242001\n",
      "mean reward (100 episodes) 62.716727\n",
      "best mean reward 76.286098\n",
      "running time 863.814560\n",
      "Train_EnvstepsSoFar : 242001\n",
      "Train_AverageReturn : 62.71672740908581\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 863.8145599365234\n",
      "Training Loss : 2.399111032485962\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 243001\n",
      "mean reward (100 episodes) 62.465089\n",
      "best mean reward 76.286098\n",
      "running time 867.302506\n",
      "Train_EnvstepsSoFar : 243001\n",
      "Train_AverageReturn : 62.465088915322305\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 867.3025059700012\n",
      "Training Loss : 0.6293023824691772\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 244001\n",
      "mean reward (100 episodes) 62.762428\n",
      "best mean reward 76.286098\n",
      "running time 870.298952\n",
      "Train_EnvstepsSoFar : 244001\n",
      "Train_AverageReturn : 62.762427983081295\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 870.2989518642426\n",
      "Training Loss : 0.23329703509807587\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 245001\n",
      "mean reward (100 episodes) 64.076038\n",
      "best mean reward 76.286098\n",
      "running time 873.412059\n",
      "Train_EnvstepsSoFar : 245001\n",
      "Train_AverageReturn : 64.07603803265785\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 873.4120588302612\n",
      "Training Loss : 0.38156989216804504\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 246001\n",
      "mean reward (100 episodes) 55.214474\n",
      "best mean reward 76.286098\n",
      "running time 876.285379\n",
      "Train_EnvstepsSoFar : 246001\n",
      "Train_AverageReturn : 55.21447419054332\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 876.2853786945343\n",
      "Training Loss : 0.4797861576080322\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 247001\n",
      "mean reward (100 episodes) 59.672026\n",
      "best mean reward 76.286098\n",
      "running time 879.084049\n",
      "Train_EnvstepsSoFar : 247001\n",
      "Train_AverageReturn : 59.67202646384882\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 879.0840487480164\n",
      "Training Loss : 0.1221078485250473\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 248001\n",
      "mean reward (100 episodes) 61.302190\n",
      "best mean reward 76.286098\n",
      "running time 882.418327\n",
      "Train_EnvstepsSoFar : 248001\n",
      "Train_AverageReturn : 61.302190294503504\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 882.4183266162872\n",
      "Training Loss : 2.074981927871704\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 249001\n",
      "mean reward (100 episodes) 61.778005\n",
      "best mean reward 76.286098\n",
      "running time 885.152724\n",
      "Train_EnvstepsSoFar : 249001\n",
      "Train_AverageReturn : 61.77800518502283\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 885.1527237892151\n",
      "Training Loss : 0.30918073654174805\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 250001\n",
      "mean reward (100 episodes) 63.162256\n",
      "best mean reward 76.286098\n",
      "running time 888.832504\n",
      "Train_EnvstepsSoFar : 250001\n",
      "Train_AverageReturn : 63.16225605733715\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 888.8325037956238\n",
      "Training Loss : 0.10518398880958557\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 251001\n",
      "mean reward (100 episodes) 68.951808\n",
      "best mean reward 76.286098\n",
      "running time 891.846779\n",
      "Train_EnvstepsSoFar : 251001\n",
      "Train_AverageReturn : 68.9518075840132\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 891.8467788696289\n",
      "Training Loss : 0.2777386009693146\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 252001\n",
      "mean reward (100 episodes) 66.100403\n",
      "best mean reward 76.286098\n",
      "running time 894.970763\n",
      "Train_EnvstepsSoFar : 252001\n",
      "Train_AverageReturn : 66.1004033434143\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 894.9707629680634\n",
      "Training Loss : 0.695512056350708\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 253001\n",
      "mean reward (100 episodes) 65.434745\n",
      "best mean reward 76.286098\n",
      "running time 897.974551\n",
      "Train_EnvstepsSoFar : 253001\n",
      "Train_AverageReturn : 65.43474496408247\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 897.9745507240295\n",
      "Training Loss : 0.5215826034545898\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 254001\n",
      "mean reward (100 episodes) 68.074903\n",
      "best mean reward 76.286098\n",
      "running time 900.678490\n",
      "Train_EnvstepsSoFar : 254001\n",
      "Train_AverageReturn : 68.0749033817236\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 900.6784896850586\n",
      "Training Loss : 1.0033597946166992\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 255001\n",
      "mean reward (100 episodes) 73.352870\n",
      "best mean reward 76.286098\n",
      "running time 903.645659\n",
      "Train_EnvstepsSoFar : 255001\n",
      "Train_AverageReturn : 73.35286961706286\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 903.6456587314606\n",
      "Training Loss : 1.2119282484054565\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 256001\n",
      "mean reward (100 episodes) 68.669566\n",
      "best mean reward 76.286098\n",
      "running time 906.520521\n",
      "Train_EnvstepsSoFar : 256001\n",
      "Train_AverageReturn : 68.66956584017149\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 906.5205209255219\n",
      "Training Loss : 0.23582276701927185\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 257001\n",
      "mean reward (100 episodes) 66.377292\n",
      "best mean reward 76.286098\n",
      "running time 909.604560\n",
      "Train_EnvstepsSoFar : 257001\n",
      "Train_AverageReturn : 66.37729186727614\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 909.6045598983765\n",
      "Training Loss : 0.17973469197750092\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 258001\n",
      "mean reward (100 episodes) 66.061261\n",
      "best mean reward 76.286098\n",
      "running time 912.933857\n",
      "Train_EnvstepsSoFar : 258001\n",
      "Train_AverageReturn : 66.06126063585721\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 912.9338567256927\n",
      "Training Loss : 1.8427588939666748\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 259001\n",
      "mean reward (100 episodes) 65.409826\n",
      "best mean reward 76.286098\n",
      "running time 917.133912\n",
      "Train_EnvstepsSoFar : 259001\n",
      "Train_AverageReturn : 65.40982603975813\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 917.1339118480682\n",
      "Training Loss : 1.0408687591552734\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 260001\n",
      "mean reward (100 episodes) 59.350800\n",
      "best mean reward 76.286098\n",
      "running time 919.752750\n",
      "Train_EnvstepsSoFar : 260001\n",
      "Train_AverageReturn : 59.35079991478857\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 919.7527496814728\n",
      "Training Loss : 0.4530009925365448\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 261001\n",
      "mean reward (100 episodes) 61.814477\n",
      "best mean reward 76.286098\n",
      "running time 922.577771\n",
      "Train_EnvstepsSoFar : 261001\n",
      "Train_AverageReturn : 61.81447662926888\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 922.57777094841\n",
      "Training Loss : 0.8508968353271484\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 262001\n",
      "mean reward (100 episodes) 63.705878\n",
      "best mean reward 76.286098\n",
      "running time 925.371033\n",
      "Train_EnvstepsSoFar : 262001\n",
      "Train_AverageReturn : 63.705878168638264\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 925.3710327148438\n",
      "Training Loss : 3.9408440589904785\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 263001\n",
      "mean reward (100 episodes) 61.913326\n",
      "best mean reward 76.286098\n",
      "running time 928.618428\n",
      "Train_EnvstepsSoFar : 263001\n",
      "Train_AverageReturn : 61.913325548922515\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 928.6184277534485\n",
      "Training Loss : 1.3415571451187134\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 264001\n",
      "mean reward (100 episodes) 64.250143\n",
      "best mean reward 76.286098\n",
      "running time 931.326063\n",
      "Train_EnvstepsSoFar : 264001\n",
      "Train_AverageReturn : 64.25014266250668\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 931.3260626792908\n",
      "Training Loss : 0.28129807114601135\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 265001\n",
      "mean reward (100 episodes) 64.718135\n",
      "best mean reward 76.286098\n",
      "running time 934.483263\n",
      "Train_EnvstepsSoFar : 265001\n",
      "Train_AverageReturn : 64.7181354925644\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 934.4832630157471\n",
      "Training Loss : 0.37173858284950256\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 266001\n",
      "mean reward (100 episodes) 57.935011\n",
      "best mean reward 76.286098\n",
      "running time 937.544844\n",
      "Train_EnvstepsSoFar : 266001\n",
      "Train_AverageReturn : 57.93501122016885\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 937.544843673706\n",
      "Training Loss : 0.18637007474899292\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 267001\n",
      "mean reward (100 episodes) 50.985073\n",
      "best mean reward 76.286098\n",
      "running time 940.271261\n",
      "Train_EnvstepsSoFar : 267001\n",
      "Train_AverageReturn : 50.98507347561882\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 940.2712607383728\n",
      "Training Loss : 0.7019780874252319\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 268001\n",
      "mean reward (100 episodes) 46.469226\n",
      "best mean reward 76.286098\n",
      "running time 943.491013\n",
      "Train_EnvstepsSoFar : 268001\n",
      "Train_AverageReturn : 46.46922554013901\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 943.4910128116608\n",
      "Training Loss : 1.4240683317184448\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 269001\n",
      "mean reward (100 episodes) 43.818915\n",
      "best mean reward 76.286098\n",
      "running time 947.251905\n",
      "Train_EnvstepsSoFar : 269001\n",
      "Train_AverageReturn : 43.81891493472543\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 947.251904964447\n",
      "Training Loss : 0.5173221826553345\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 270001\n",
      "mean reward (100 episodes) 46.194001\n",
      "best mean reward 76.286098\n",
      "running time 950.049392\n",
      "Train_EnvstepsSoFar : 270001\n",
      "Train_AverageReturn : 46.19400076336247\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 950.049391746521\n",
      "Training Loss : 0.1047544777393341\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 271001\n",
      "mean reward (100 episodes) 54.203867\n",
      "best mean reward 76.286098\n",
      "running time 952.902677\n",
      "Train_EnvstepsSoFar : 271001\n",
      "Train_AverageReturn : 54.203866873954375\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 952.902676820755\n",
      "Training Loss : 1.3663114309310913\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 272001\n",
      "mean reward (100 episodes) 58.363236\n",
      "best mean reward 76.286098\n",
      "running time 955.657319\n",
      "Train_EnvstepsSoFar : 272001\n",
      "Train_AverageReturn : 58.36323558809649\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 955.6573188304901\n",
      "Training Loss : 4.873542785644531\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 273001\n",
      "mean reward (100 episodes) 57.243893\n",
      "best mean reward 76.286098\n",
      "running time 958.387137\n",
      "Train_EnvstepsSoFar : 273001\n",
      "Train_AverageReturn : 57.24389258552873\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 958.3871369361877\n",
      "Training Loss : 0.13030414283275604\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 274001\n",
      "mean reward (100 episodes) 59.557914\n",
      "best mean reward 76.286098\n",
      "running time 961.257734\n",
      "Train_EnvstepsSoFar : 274001\n",
      "Train_AverageReturn : 59.55791398345585\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 961.2577338218689\n",
      "Training Loss : 0.3007168769836426\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 275001\n",
      "mean reward (100 episodes) 61.562225\n",
      "best mean reward 76.286098\n",
      "running time 964.061673\n",
      "Train_EnvstepsSoFar : 275001\n",
      "Train_AverageReturn : 61.56222520544117\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 964.0616729259491\n",
      "Training Loss : 1.0981080532073975\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 276001\n",
      "mean reward (100 episodes) 63.188399\n",
      "best mean reward 76.286098\n",
      "running time 967.013334\n",
      "Train_EnvstepsSoFar : 276001\n",
      "Train_AverageReturn : 63.188398838023495\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 967.0133340358734\n",
      "Training Loss : 0.3219642639160156\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 277001\n",
      "mean reward (100 episodes) 63.015417\n",
      "best mean reward 76.286098\n",
      "running time 970.018956\n",
      "Train_EnvstepsSoFar : 277001\n",
      "Train_AverageReturn : 63.0154173795091\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 970.01895570755\n",
      "Training Loss : 0.3914547264575958\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 278001\n",
      "mean reward (100 episodes) 62.440357\n",
      "best mean reward 76.286098\n",
      "running time 973.194379\n",
      "Train_EnvstepsSoFar : 278001\n",
      "Train_AverageReturn : 62.440357360382016\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 973.1943788528442\n",
      "Training Loss : 0.3387192189693451\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 279001\n",
      "mean reward (100 episodes) 64.671919\n",
      "best mean reward 76.286098\n",
      "running time 976.122152\n",
      "Train_EnvstepsSoFar : 279001\n",
      "Train_AverageReturn : 64.67191852438933\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 976.122151851654\n",
      "Training Loss : 0.24251794815063477\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 280001\n",
      "mean reward (100 episodes) 68.036345\n",
      "best mean reward 76.286098\n",
      "running time 978.925293\n",
      "Train_EnvstepsSoFar : 280001\n",
      "Train_AverageReturn : 68.03634476973247\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 978.9252927303314\n",
      "Training Loss : 0.852006733417511\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 281001\n",
      "mean reward (100 episodes) 63.651920\n",
      "best mean reward 76.286098\n",
      "running time 981.734277\n",
      "Train_EnvstepsSoFar : 281001\n",
      "Train_AverageReturn : 63.651919540983066\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 981.7342767715454\n",
      "Training Loss : 0.2701377272605896\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 282001\n",
      "mean reward (100 episodes) 66.855284\n",
      "best mean reward 76.286098\n",
      "running time 984.608394\n",
      "Train_EnvstepsSoFar : 282001\n",
      "Train_AverageReturn : 66.85528388475576\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 984.608393907547\n",
      "Training Loss : 1.0584306716918945\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 283001\n",
      "mean reward (100 episodes) 65.799206\n",
      "best mean reward 76.286098\n",
      "running time 987.564997\n",
      "Train_EnvstepsSoFar : 283001\n",
      "Train_AverageReturn : 65.79920623411415\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 987.5649967193604\n",
      "Training Loss : 0.2778506875038147\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 284001\n",
      "mean reward (100 episodes) 64.002060\n",
      "best mean reward 76.286098\n",
      "running time 991.058432\n",
      "Train_EnvstepsSoFar : 284001\n",
      "Train_AverageReturn : 64.00206047651248\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 991.0584318637848\n",
      "Training Loss : 2.4696125984191895\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 285001\n",
      "mean reward (100 episodes) 67.412449\n",
      "best mean reward 76.286098\n",
      "running time 993.871445\n",
      "Train_EnvstepsSoFar : 285001\n",
      "Train_AverageReturn : 67.41244900078598\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 993.871444940567\n",
      "Training Loss : 2.3517332077026367\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 286001\n",
      "mean reward (100 episodes) 67.523015\n",
      "best mean reward 76.286098\n",
      "running time 996.622602\n",
      "Train_EnvstepsSoFar : 286001\n",
      "Train_AverageReturn : 67.52301530882842\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 996.6226019859314\n",
      "Training Loss : 5.720130920410156\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 287001\n",
      "mean reward (100 episodes) 67.709231\n",
      "best mean reward 76.286098\n",
      "running time 999.382744\n",
      "Train_EnvstepsSoFar : 287001\n",
      "Train_AverageReturn : 67.70923089518689\n",
      "Train_BestReturn : 76.28609845349386\n",
      "TimeSinceStart : 999.3827438354492\n",
      "Training Loss : 0.5344803929328918\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 288001\n",
      "mean reward (100 episodes) 79.536479\n",
      "best mean reward 79.536479\n",
      "running time 1002.005922\n",
      "Train_EnvstepsSoFar : 288001\n",
      "Train_AverageReturn : 79.53647894457093\n",
      "Train_BestReturn : 79.53647894457093\n",
      "TimeSinceStart : 1002.0059216022491\n",
      "Training Loss : 0.5526820421218872\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 289001\n",
      "mean reward (100 episodes) 84.344945\n",
      "best mean reward 84.344945\n",
      "running time 1004.762644\n",
      "Train_EnvstepsSoFar : 289001\n",
      "Train_AverageReturn : 84.3449449309792\n",
      "Train_BestReturn : 84.3449449309792\n",
      "TimeSinceStart : 1004.7626438140869\n",
      "Training Loss : 0.20753848552703857\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 290001\n",
      "mean reward (100 episodes) 83.537879\n",
      "best mean reward 84.344945\n",
      "running time 1008.080891\n",
      "Train_EnvstepsSoFar : 290001\n",
      "Train_AverageReturn : 83.53787924851802\n",
      "Train_BestReturn : 84.3449449309792\n",
      "TimeSinceStart : 1008.0808908939362\n",
      "Training Loss : 0.3046206831932068\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 291001\n",
      "mean reward (100 episodes) 81.144534\n",
      "best mean reward 84.344945\n",
      "running time 1010.714600\n",
      "Train_EnvstepsSoFar : 291001\n",
      "Train_AverageReturn : 81.14453355812631\n",
      "Train_BestReturn : 84.3449449309792\n",
      "TimeSinceStart : 1010.7145998477936\n",
      "Training Loss : 2.993788003921509\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 292001\n",
      "mean reward (100 episodes) 83.013055\n",
      "best mean reward 84.344945\n",
      "running time 1014.192168\n",
      "Train_EnvstepsSoFar : 292001\n",
      "Train_AverageReturn : 83.01305540092548\n",
      "Train_BestReturn : 84.3449449309792\n",
      "TimeSinceStart : 1014.1921677589417\n",
      "Training Loss : 0.5786720514297485\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 293001\n",
      "mean reward (100 episodes) 85.209896\n",
      "best mean reward 85.209896\n",
      "running time 1017.456113\n",
      "Train_EnvstepsSoFar : 293001\n",
      "Train_AverageReturn : 85.20989635314052\n",
      "Train_BestReturn : 85.20989635314052\n",
      "TimeSinceStart : 1017.4561128616333\n",
      "Training Loss : 3.1346569061279297\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 294001\n",
      "mean reward (100 episodes) 84.933605\n",
      "best mean reward 85.209896\n",
      "running time 1020.099529\n",
      "Train_EnvstepsSoFar : 294001\n",
      "Train_AverageReturn : 84.93360460050364\n",
      "Train_BestReturn : 85.20989635314052\n",
      "TimeSinceStart : 1020.0995287895203\n",
      "Training Loss : 0.6104851961135864\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 295001\n",
      "mean reward (100 episodes) 85.270027\n",
      "best mean reward 85.270027\n",
      "running time 1022.875342\n",
      "Train_EnvstepsSoFar : 295001\n",
      "Train_AverageReturn : 85.27002658403163\n",
      "Train_BestReturn : 85.27002658403163\n",
      "TimeSinceStart : 1022.8753416538239\n",
      "Training Loss : 0.42917701601982117\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 296001\n",
      "mean reward (100 episodes) 82.548620\n",
      "best mean reward 85.270027\n",
      "running time 1025.977805\n",
      "Train_EnvstepsSoFar : 296001\n",
      "Train_AverageReturn : 82.54861977827531\n",
      "Train_BestReturn : 85.27002658403163\n",
      "TimeSinceStart : 1025.9778046607971\n",
      "Training Loss : 1.8090574741363525\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 297001\n",
      "mean reward (100 episodes) 75.001784\n",
      "best mean reward 85.270027\n",
      "running time 1029.541185\n",
      "Train_EnvstepsSoFar : 297001\n",
      "Train_AverageReturn : 75.00178377022246\n",
      "Train_BestReturn : 85.27002658403163\n",
      "TimeSinceStart : 1029.5411849021912\n",
      "Training Loss : 0.3329353928565979\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 298001\n",
      "mean reward (100 episodes) 79.962903\n",
      "best mean reward 85.270027\n",
      "running time 1032.165850\n",
      "Train_EnvstepsSoFar : 298001\n",
      "Train_AverageReturn : 79.96290311704435\n",
      "Train_BestReturn : 85.27002658403163\n",
      "TimeSinceStart : 1032.165849685669\n",
      "Training Loss : 3.3179588317871094\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 299001\n",
      "mean reward (100 episodes) 80.048221\n",
      "best mean reward 85.270027\n",
      "running time 1034.852878\n",
      "Train_EnvstepsSoFar : 299001\n",
      "Train_AverageReturn : 80.04822132229698\n",
      "Train_BestReturn : 85.27002658403163\n",
      "TimeSinceStart : 1034.852877855301\n",
      "Training Loss : 6.582283020019531\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 300001\n",
      "mean reward (100 episodes) 82.655791\n",
      "best mean reward 85.270027\n",
      "running time 1038.987832\n",
      "Train_EnvstepsSoFar : 300001\n",
      "Train_AverageReturn : 82.6557908423884\n",
      "Train_BestReturn : 85.27002658403163\n",
      "TimeSinceStart : 1038.9878318309784\n",
      "Training Loss : 2.6569018363952637\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 301001\n",
      "mean reward (100 episodes) 87.664460\n",
      "best mean reward 87.664460\n",
      "running time 1041.671747\n",
      "Train_EnvstepsSoFar : 301001\n",
      "Train_AverageReturn : 87.66446017761444\n",
      "Train_BestReturn : 87.66446017761444\n",
      "TimeSinceStart : 1041.6717467308044\n",
      "Training Loss : 1.051537275314331\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 302001\n",
      "mean reward (100 episodes) 83.058915\n",
      "best mean reward 87.664460\n",
      "running time 1045.088359\n",
      "Train_EnvstepsSoFar : 302001\n",
      "Train_AverageReturn : 83.05891503919915\n",
      "Train_BestReturn : 87.66446017761444\n",
      "TimeSinceStart : 1045.0883588790894\n",
      "Training Loss : 0.8070176839828491\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 303001\n",
      "mean reward (100 episodes) 80.617444\n",
      "best mean reward 87.664460\n",
      "running time 1047.940372\n",
      "Train_EnvstepsSoFar : 303001\n",
      "Train_AverageReturn : 80.6174441038012\n",
      "Train_BestReturn : 87.66446017761444\n",
      "TimeSinceStart : 1047.9403719902039\n",
      "Training Loss : 3.8416919708251953\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 304001\n",
      "mean reward (100 episodes) 79.289861\n",
      "best mean reward 87.664460\n",
      "running time 1052.749195\n",
      "Train_EnvstepsSoFar : 304001\n",
      "Train_AverageReturn : 79.28986087906958\n",
      "Train_BestReturn : 87.66446017761444\n",
      "TimeSinceStart : 1052.7491948604584\n",
      "Training Loss : 3.0402886867523193\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 305001\n",
      "mean reward (100 episodes) 83.036126\n",
      "best mean reward 87.664460\n",
      "running time 1055.392846\n",
      "Train_EnvstepsSoFar : 305001\n",
      "Train_AverageReturn : 83.03612568412916\n",
      "Train_BestReturn : 87.66446017761444\n",
      "TimeSinceStart : 1055.3928458690643\n",
      "Training Loss : 0.420303612947464\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 306001\n",
      "mean reward (100 episodes) 75.326597\n",
      "best mean reward 87.664460\n",
      "running time 1058.014239\n",
      "Train_EnvstepsSoFar : 306001\n",
      "Train_AverageReturn : 75.32659653764561\n",
      "Train_BestReturn : 87.66446017761444\n",
      "TimeSinceStart : 1058.014238834381\n",
      "Training Loss : 0.46634864807128906\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 307001\n",
      "mean reward (100 episodes) 72.222283\n",
      "best mean reward 87.664460\n",
      "running time 1061.180919\n",
      "Train_EnvstepsSoFar : 307001\n",
      "Train_AverageReturn : 72.22228263484497\n",
      "Train_BestReturn : 87.66446017761444\n",
      "TimeSinceStart : 1061.1809186935425\n",
      "Training Loss : 3.5000712871551514\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 308001\n",
      "mean reward (100 episodes) 73.251807\n",
      "best mean reward 87.664460\n",
      "running time 1064.226457\n",
      "Train_EnvstepsSoFar : 308001\n",
      "Train_AverageReturn : 73.25180679471198\n",
      "Train_BestReturn : 87.66446017761444\n",
      "TimeSinceStart : 1064.2264566421509\n",
      "Training Loss : 0.14721888303756714\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 309001\n",
      "mean reward (100 episodes) 70.760875\n",
      "best mean reward 87.664460\n",
      "running time 1067.857121\n",
      "Train_EnvstepsSoFar : 309001\n",
      "Train_AverageReturn : 70.7608748280797\n",
      "Train_BestReturn : 87.66446017761444\n",
      "TimeSinceStart : 1067.8571207523346\n",
      "Training Loss : 0.9355103969573975\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 310001\n",
      "mean reward (100 episodes) 69.626234\n",
      "best mean reward 87.664460\n",
      "running time 1071.085177\n",
      "Train_EnvstepsSoFar : 310001\n",
      "Train_AverageReturn : 69.62623388701628\n",
      "Train_BestReturn : 87.66446017761444\n",
      "TimeSinceStart : 1071.085176706314\n",
      "Training Loss : 1.402902364730835\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 311001\n",
      "mean reward (100 episodes) 70.495524\n",
      "best mean reward 87.664460\n",
      "running time 1074.626967\n",
      "Train_EnvstepsSoFar : 311001\n",
      "Train_AverageReturn : 70.49552410835528\n",
      "Train_BestReturn : 87.66446017761444\n",
      "TimeSinceStart : 1074.626966714859\n",
      "Training Loss : 0.12724857032299042\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 312001\n",
      "mean reward (100 episodes) 70.978340\n",
      "best mean reward 87.664460\n",
      "running time 1077.405771\n",
      "Train_EnvstepsSoFar : 312001\n",
      "Train_AverageReturn : 70.97834024626147\n",
      "Train_BestReturn : 87.66446017761444\n",
      "TimeSinceStart : 1077.4057710170746\n",
      "Training Loss : 0.10888875275850296\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 313001\n",
      "mean reward (100 episodes) 71.866437\n",
      "best mean reward 87.664460\n",
      "running time 1080.253869\n",
      "Train_EnvstepsSoFar : 313001\n",
      "Train_AverageReturn : 71.8664373418806\n",
      "Train_BestReturn : 87.66446017761444\n",
      "TimeSinceStart : 1080.253868818283\n",
      "Training Loss : 0.37460947036743164\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 314001\n",
      "mean reward (100 episodes) 72.367298\n",
      "best mean reward 87.664460\n",
      "running time 1082.865906\n",
      "Train_EnvstepsSoFar : 314001\n",
      "Train_AverageReturn : 72.36729780236948\n",
      "Train_BestReturn : 87.66446017761444\n",
      "TimeSinceStart : 1082.8659057617188\n",
      "Training Loss : 0.8909797072410583\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 315001\n",
      "mean reward (100 episodes) 70.867348\n",
      "best mean reward 87.664460\n",
      "running time 1085.508656\n",
      "Train_EnvstepsSoFar : 315001\n",
      "Train_AverageReturn : 70.86734762933781\n",
      "Train_BestReturn : 87.66446017761444\n",
      "TimeSinceStart : 1085.5086557865143\n",
      "Training Loss : 2.3384735584259033\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 316001\n",
      "mean reward (100 episodes) 74.529155\n",
      "best mean reward 87.664460\n",
      "running time 1088.289705\n",
      "Train_EnvstepsSoFar : 316001\n",
      "Train_AverageReturn : 74.52915483213438\n",
      "Train_BestReturn : 87.66446017761444\n",
      "TimeSinceStart : 1088.289704799652\n",
      "Training Loss : 1.705710768699646\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 317001\n",
      "mean reward (100 episodes) 74.109449\n",
      "best mean reward 87.664460\n",
      "running time 1092.509842\n",
      "Train_EnvstepsSoFar : 317001\n",
      "Train_AverageReturn : 74.1094492808689\n",
      "Train_BestReturn : 87.66446017761444\n",
      "TimeSinceStart : 1092.5098419189453\n",
      "Training Loss : 0.33806025981903076\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 318001\n",
      "mean reward (100 episodes) 71.611233\n",
      "best mean reward 87.664460\n",
      "running time 1095.156023\n",
      "Train_EnvstepsSoFar : 318001\n",
      "Train_AverageReturn : 71.61123301872406\n",
      "Train_BestReturn : 87.66446017761444\n",
      "TimeSinceStart : 1095.1560227870941\n",
      "Training Loss : 0.2335396707057953\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 319001\n",
      "mean reward (100 episodes) 72.369363\n",
      "best mean reward 87.664460\n",
      "running time 1097.838829\n",
      "Train_EnvstepsSoFar : 319001\n",
      "Train_AverageReturn : 72.36936320467039\n",
      "Train_BestReturn : 87.66446017761444\n",
      "TimeSinceStart : 1097.8388288021088\n",
      "Training Loss : 0.2168695032596588\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 320001\n",
      "mean reward (100 episodes) 72.987807\n",
      "best mean reward 87.664460\n",
      "running time 1100.587885\n",
      "Train_EnvstepsSoFar : 320001\n",
      "Train_AverageReturn : 72.98780670139871\n",
      "Train_BestReturn : 87.66446017761444\n",
      "TimeSinceStart : 1100.5878846645355\n",
      "Training Loss : 0.3786940574645996\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 321001\n",
      "mean reward (100 episodes) 75.355879\n",
      "best mean reward 87.664460\n",
      "running time 1103.559349\n",
      "Train_EnvstepsSoFar : 321001\n",
      "Train_AverageReturn : 75.35587903226\n",
      "Train_BestReturn : 87.66446017761444\n",
      "TimeSinceStart : 1103.55934882164\n",
      "Training Loss : 1.3891438245773315\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 322001\n",
      "mean reward (100 episodes) 75.508876\n",
      "best mean reward 87.664460\n",
      "running time 1106.900338\n",
      "Train_EnvstepsSoFar : 322001\n",
      "Train_AverageReturn : 75.5088759662032\n",
      "Train_BestReturn : 87.66446017761444\n",
      "TimeSinceStart : 1106.900337934494\n",
      "Training Loss : 2.0554606914520264\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 323001\n",
      "mean reward (100 episodes) 80.887058\n",
      "best mean reward 87.664460\n",
      "running time 1109.583442\n",
      "Train_EnvstepsSoFar : 323001\n",
      "Train_AverageReturn : 80.88705793082559\n",
      "Train_BestReturn : 87.66446017761444\n",
      "TimeSinceStart : 1109.583441734314\n",
      "Training Loss : 3.1047487258911133\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 324001\n",
      "mean reward (100 episodes) 83.124949\n",
      "best mean reward 87.664460\n",
      "running time 1112.261129\n",
      "Train_EnvstepsSoFar : 324001\n",
      "Train_AverageReturn : 83.12494857133335\n",
      "Train_BestReturn : 87.66446017761444\n",
      "TimeSinceStart : 1112.2611289024353\n",
      "Training Loss : 0.5830877423286438\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 325001\n",
      "mean reward (100 episodes) 89.310046\n",
      "best mean reward 89.310046\n",
      "running time 1114.954711\n",
      "Train_EnvstepsSoFar : 325001\n",
      "Train_AverageReturn : 89.31004561653567\n",
      "Train_BestReturn : 89.31004561653567\n",
      "TimeSinceStart : 1114.9547107219696\n",
      "Training Loss : 1.9056756496429443\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 326001\n",
      "mean reward (100 episodes) 95.448305\n",
      "best mean reward 95.448305\n",
      "running time 1117.739417\n",
      "Train_EnvstepsSoFar : 326001\n",
      "Train_AverageReturn : 95.4483046601805\n",
      "Train_BestReturn : 95.4483046601805\n",
      "TimeSinceStart : 1117.7394168376923\n",
      "Training Loss : 0.3522580862045288\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 327001\n",
      "mean reward (100 episodes) 96.630523\n",
      "best mean reward 96.630523\n",
      "running time 1120.436917\n",
      "Train_EnvstepsSoFar : 327001\n",
      "Train_AverageReturn : 96.6305226721904\n",
      "Train_BestReturn : 96.6305226721904\n",
      "TimeSinceStart : 1120.4369168281555\n",
      "Training Loss : 0.5093268156051636\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 328001\n",
      "mean reward (100 episodes) 96.125033\n",
      "best mean reward 96.630523\n",
      "running time 1123.221418\n",
      "Train_EnvstepsSoFar : 328001\n",
      "Train_AverageReturn : 96.12503304800893\n",
      "Train_BestReturn : 96.6305226721904\n",
      "TimeSinceStart : 1123.2214179039001\n",
      "Training Loss : 0.15501214563846588\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 329001\n",
      "mean reward (100 episodes) 92.504845\n",
      "best mean reward 96.630523\n",
      "running time 1126.795130\n",
      "Train_EnvstepsSoFar : 329001\n",
      "Train_AverageReturn : 92.50484461917435\n",
      "Train_BestReturn : 96.6305226721904\n",
      "TimeSinceStart : 1126.795129776001\n",
      "Training Loss : 0.14006128907203674\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 330001\n",
      "mean reward (100 episodes) 92.427527\n",
      "best mean reward 96.630523\n",
      "running time 1129.620100\n",
      "Train_EnvstepsSoFar : 330001\n",
      "Train_AverageReturn : 92.42752710809914\n",
      "Train_BestReturn : 96.6305226721904\n",
      "TimeSinceStart : 1129.6200997829437\n",
      "Training Loss : 3.167604446411133\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 331001\n",
      "mean reward (100 episodes) 92.358879\n",
      "best mean reward 96.630523\n",
      "running time 1132.257907\n",
      "Train_EnvstepsSoFar : 331001\n",
      "Train_AverageReturn : 92.35887881196548\n",
      "Train_BestReturn : 96.6305226721904\n",
      "TimeSinceStart : 1132.2579069137573\n",
      "Training Loss : 3.8627965450286865\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 332001\n",
      "mean reward (100 episodes) 91.926576\n",
      "best mean reward 96.630523\n",
      "running time 1135.196271\n",
      "Train_EnvstepsSoFar : 332001\n",
      "Train_AverageReturn : 91.92657641981017\n",
      "Train_BestReturn : 96.6305226721904\n",
      "TimeSinceStart : 1135.1962707042694\n",
      "Training Loss : 0.3968738317489624\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 333001\n",
      "mean reward (100 episodes) 88.962480\n",
      "best mean reward 96.630523\n",
      "running time 1137.954599\n",
      "Train_EnvstepsSoFar : 333001\n",
      "Train_AverageReturn : 88.9624801431576\n",
      "Train_BestReturn : 96.6305226721904\n",
      "TimeSinceStart : 1137.954598903656\n",
      "Training Loss : 2.643357753753662\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 334001\n",
      "mean reward (100 episodes) 87.555631\n",
      "best mean reward 96.630523\n",
      "running time 1140.701817\n",
      "Train_EnvstepsSoFar : 334001\n",
      "Train_AverageReturn : 87.55563065641736\n",
      "Train_BestReturn : 96.6305226721904\n",
      "TimeSinceStart : 1140.7018167972565\n",
      "Training Loss : 2.4130427837371826\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 335001\n",
      "mean reward (100 episodes) 92.305037\n",
      "best mean reward 96.630523\n",
      "running time 1143.335101\n",
      "Train_EnvstepsSoFar : 335001\n",
      "Train_AverageReturn : 92.30503742701345\n",
      "Train_BestReturn : 96.6305226721904\n",
      "TimeSinceStart : 1143.335100889206\n",
      "Training Loss : 0.41343921422958374\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 336001\n",
      "mean reward (100 episodes) 86.403223\n",
      "best mean reward 96.630523\n",
      "running time 1146.233996\n",
      "Train_EnvstepsSoFar : 336001\n",
      "Train_AverageReturn : 86.40322307757118\n",
      "Train_BestReturn : 96.6305226721904\n",
      "TimeSinceStart : 1146.2339956760406\n",
      "Training Loss : 0.44580787420272827\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 337001\n",
      "mean reward (100 episodes) 85.649047\n",
      "best mean reward 96.630523\n",
      "running time 1148.959514\n",
      "Train_EnvstepsSoFar : 337001\n",
      "Train_AverageReturn : 85.64904708999995\n",
      "Train_BestReturn : 96.6305226721904\n",
      "TimeSinceStart : 1148.9595139026642\n",
      "Training Loss : 1.4483450651168823\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 338001\n",
      "mean reward (100 episodes) 90.684417\n",
      "best mean reward 96.630523\n",
      "running time 1151.667888\n",
      "Train_EnvstepsSoFar : 338001\n",
      "Train_AverageReturn : 90.68441677910279\n",
      "Train_BestReturn : 96.6305226721904\n",
      "TimeSinceStart : 1151.667887687683\n",
      "Training Loss : 0.9695553779602051\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 339001\n",
      "mean reward (100 episodes) 88.421881\n",
      "best mean reward 96.630523\n",
      "running time 1154.348970\n",
      "Train_EnvstepsSoFar : 339001\n",
      "Train_AverageReturn : 88.42188050506059\n",
      "Train_BestReturn : 96.6305226721904\n",
      "TimeSinceStart : 1154.3489699363708\n",
      "Training Loss : 1.2790255546569824\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 340001\n",
      "mean reward (100 episodes) 82.269761\n",
      "best mean reward 96.630523\n",
      "running time 1157.035158\n",
      "Train_EnvstepsSoFar : 340001\n",
      "Train_AverageReturn : 82.26976142838222\n",
      "Train_BestReturn : 96.6305226721904\n",
      "TimeSinceStart : 1157.03515791893\n",
      "Training Loss : 0.10695938020944595\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 341001\n",
      "mean reward (100 episodes) 79.324887\n",
      "best mean reward 96.630523\n",
      "running time 1160.303882\n",
      "Train_EnvstepsSoFar : 341001\n",
      "Train_AverageReturn : 79.32488706306277\n",
      "Train_BestReturn : 96.6305226721904\n",
      "TimeSinceStart : 1160.3038818836212\n",
      "Training Loss : 0.5207993984222412\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 342001\n",
      "mean reward (100 episodes) 72.865751\n",
      "best mean reward 96.630523\n",
      "running time 1163.055021\n",
      "Train_EnvstepsSoFar : 342001\n",
      "Train_AverageReturn : 72.86575083518699\n",
      "Train_BestReturn : 96.6305226721904\n",
      "TimeSinceStart : 1163.0550208091736\n",
      "Training Loss : 1.8024483919143677\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 343001\n",
      "mean reward (100 episodes) 66.838819\n",
      "best mean reward 96.630523\n",
      "running time 1165.789360\n",
      "Train_EnvstepsSoFar : 343001\n",
      "Train_AverageReturn : 66.83881871978645\n",
      "Train_BestReturn : 96.6305226721904\n",
      "TimeSinceStart : 1165.7893598079681\n",
      "Training Loss : 0.18918436765670776\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 344001\n",
      "mean reward (100 episodes) 69.672791\n",
      "best mean reward 96.630523\n",
      "running time 1168.483146\n",
      "Train_EnvstepsSoFar : 344001\n",
      "Train_AverageReturn : 69.67279104005647\n",
      "Train_BestReturn : 96.6305226721904\n",
      "TimeSinceStart : 1168.4831459522247\n",
      "Training Loss : 0.42428308725357056\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 345001\n",
      "mean reward (100 episodes) 68.936030\n",
      "best mean reward 96.630523\n",
      "running time 1171.130306\n",
      "Train_EnvstepsSoFar : 345001\n",
      "Train_AverageReturn : 68.93602970927468\n",
      "Train_BestReturn : 96.6305226721904\n",
      "TimeSinceStart : 1171.1303057670593\n",
      "Training Loss : 0.7866537570953369\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 346001\n",
      "mean reward (100 episodes) 72.699691\n",
      "best mean reward 96.630523\n",
      "running time 1173.821855\n",
      "Train_EnvstepsSoFar : 346001\n",
      "Train_AverageReturn : 72.69969106908235\n",
      "Train_BestReturn : 96.6305226721904\n",
      "TimeSinceStart : 1173.8218548297882\n",
      "Training Loss : 0.4092033803462982\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 347001\n",
      "mean reward (100 episodes) 77.302383\n",
      "best mean reward 96.630523\n",
      "running time 1176.487766\n",
      "Train_EnvstepsSoFar : 347001\n",
      "Train_AverageReturn : 77.30238331980229\n",
      "Train_BestReturn : 96.6305226721904\n",
      "TimeSinceStart : 1176.4877660274506\n",
      "Training Loss : 5.53439998626709\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 348001\n",
      "mean reward (100 episodes) 83.024891\n",
      "best mean reward 96.630523\n",
      "running time 1179.316868\n",
      "Train_EnvstepsSoFar : 348001\n",
      "Train_AverageReturn : 83.02489064766282\n",
      "Train_BestReturn : 96.6305226721904\n",
      "TimeSinceStart : 1179.3168678283691\n",
      "Training Loss : 2.890556573867798\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 349001\n",
      "mean reward (100 episodes) 83.207965\n",
      "best mean reward 96.630523\n",
      "running time 1182.001713\n",
      "Train_EnvstepsSoFar : 349001\n",
      "Train_AverageReturn : 83.20796461090205\n",
      "Train_BestReturn : 96.6305226721904\n",
      "TimeSinceStart : 1182.0017127990723\n",
      "Training Loss : 0.5191481113433838\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 350001\n",
      "mean reward (100 episodes) 90.493082\n",
      "best mean reward 96.630523\n",
      "running time 1184.642806\n",
      "Train_EnvstepsSoFar : 350001\n",
      "Train_AverageReturn : 90.4930819381136\n",
      "Train_BestReturn : 96.6305226721904\n",
      "TimeSinceStart : 1184.6428060531616\n",
      "Training Loss : 2.7833340167999268\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 351001\n",
      "mean reward (100 episodes) 91.833316\n",
      "best mean reward 96.630523\n",
      "running time 1187.923027\n",
      "Train_EnvstepsSoFar : 351001\n",
      "Train_AverageReturn : 91.83331566190617\n",
      "Train_BestReturn : 96.6305226721904\n",
      "TimeSinceStart : 1187.9230268001556\n",
      "Training Loss : 0.3641493022441864\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 352001\n",
      "mean reward (100 episodes) 97.312298\n",
      "best mean reward 97.312298\n",
      "running time 1190.561128\n",
      "Train_EnvstepsSoFar : 352001\n",
      "Train_AverageReturn : 97.31229822966128\n",
      "Train_BestReturn : 97.31229822966128\n",
      "TimeSinceStart : 1190.5611279010773\n",
      "Training Loss : 2.5179789066314697\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 353001\n",
      "mean reward (100 episodes) 97.579789\n",
      "best mean reward 97.579789\n",
      "running time 1193.700082\n",
      "Train_EnvstepsSoFar : 353001\n",
      "Train_AverageReturn : 97.57978929565496\n",
      "Train_BestReturn : 97.57978929565496\n",
      "TimeSinceStart : 1193.7000818252563\n",
      "Training Loss : 0.6343699097633362\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 354001\n",
      "mean reward (100 episodes) 92.853186\n",
      "best mean reward 97.579789\n",
      "running time 1196.698602\n",
      "Train_EnvstepsSoFar : 354001\n",
      "Train_AverageReturn : 92.85318595946755\n",
      "Train_BestReturn : 97.57978929565496\n",
      "TimeSinceStart : 1196.6986019611359\n",
      "Training Loss : 1.3688325881958008\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 355001\n",
      "mean reward (100 episodes) 101.032210\n",
      "best mean reward 101.032210\n",
      "running time 1199.289118\n",
      "Train_EnvstepsSoFar : 355001\n",
      "Train_AverageReturn : 101.03220996522576\n",
      "Train_BestReturn : 101.03220996522576\n",
      "TimeSinceStart : 1199.2891178131104\n",
      "Training Loss : 0.24439184367656708\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 356001\n",
      "mean reward (100 episodes) 103.192468\n",
      "best mean reward 103.192468\n",
      "running time 1202.379753\n",
      "Train_EnvstepsSoFar : 356001\n",
      "Train_AverageReturn : 103.19246839475998\n",
      "Train_BestReturn : 103.19246839475998\n",
      "TimeSinceStart : 1202.3797528743744\n",
      "Training Loss : 3.3925271034240723\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 357001\n",
      "mean reward (100 episodes) 112.205248\n",
      "best mean reward 112.205248\n",
      "running time 1205.158684\n",
      "Train_EnvstepsSoFar : 357001\n",
      "Train_AverageReturn : 112.20524847222507\n",
      "Train_BestReturn : 112.20524847222507\n",
      "TimeSinceStart : 1205.1586837768555\n",
      "Training Loss : 0.6186461448669434\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 358001\n",
      "mean reward (100 episodes) 122.096089\n",
      "best mean reward 122.096089\n",
      "running time 1207.841739\n",
      "Train_EnvstepsSoFar : 358001\n",
      "Train_AverageReturn : 122.09608852765095\n",
      "Train_BestReturn : 122.09608852765095\n",
      "TimeSinceStart : 1207.8417389392853\n",
      "Training Loss : 0.3841857612133026\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 359001\n",
      "mean reward (100 episodes) 121.865339\n",
      "best mean reward 122.096089\n",
      "running time 1211.251656\n",
      "Train_EnvstepsSoFar : 359001\n",
      "Train_AverageReturn : 121.86533871803624\n",
      "Train_BestReturn : 122.09608852765095\n",
      "TimeSinceStart : 1211.2516558170319\n",
      "Training Loss : 0.13316607475280762\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 360001\n",
      "mean reward (100 episodes) 126.170956\n",
      "best mean reward 126.170956\n",
      "running time 1215.189047\n",
      "Train_EnvstepsSoFar : 360001\n",
      "Train_AverageReturn : 126.17095637996313\n",
      "Train_BestReturn : 126.17095637996313\n",
      "TimeSinceStart : 1215.1890466213226\n",
      "Training Loss : 0.12542282044887543\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 361001\n",
      "mean reward (100 episodes) 126.599722\n",
      "best mean reward 126.599722\n",
      "running time 1217.843615\n",
      "Train_EnvstepsSoFar : 361001\n",
      "Train_AverageReturn : 126.59972216739737\n",
      "Train_BestReturn : 126.59972216739737\n",
      "TimeSinceStart : 1217.8436148166656\n",
      "Training Loss : 7.908818244934082\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 362001\n",
      "mean reward (100 episodes) 132.383927\n",
      "best mean reward 132.383927\n",
      "running time 1220.541155\n",
      "Train_EnvstepsSoFar : 362001\n",
      "Train_AverageReturn : 132.38392729781737\n",
      "Train_BestReturn : 132.38392729781737\n",
      "TimeSinceStart : 1220.5411548614502\n",
      "Training Loss : 0.3732853829860687\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 363001\n",
      "mean reward (100 episodes) 133.416415\n",
      "best mean reward 133.416415\n",
      "running time 1223.201240\n",
      "Train_EnvstepsSoFar : 363001\n",
      "Train_AverageReturn : 133.41641540908043\n",
      "Train_BestReturn : 133.41641540908043\n",
      "TimeSinceStart : 1223.201239824295\n",
      "Training Loss : 0.22996950149536133\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 364001\n",
      "mean reward (100 episodes) 134.844274\n",
      "best mean reward 134.844274\n",
      "running time 1225.851068\n",
      "Train_EnvstepsSoFar : 364001\n",
      "Train_AverageReturn : 134.8442739672983\n",
      "Train_BestReturn : 134.8442739672983\n",
      "TimeSinceStart : 1225.8510677814484\n",
      "Training Loss : 2.2195687294006348\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 365001\n",
      "mean reward (100 episodes) 134.886486\n",
      "best mean reward 134.886486\n",
      "running time 1228.531989\n",
      "Train_EnvstepsSoFar : 365001\n",
      "Train_AverageReturn : 134.88648617400221\n",
      "Train_BestReturn : 134.88648617400221\n",
      "TimeSinceStart : 1228.5319888591766\n",
      "Training Loss : 0.3714422285556793\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 366001\n",
      "mean reward (100 episodes) 138.072463\n",
      "best mean reward 138.072463\n",
      "running time 1231.310334\n",
      "Train_EnvstepsSoFar : 366001\n",
      "Train_AverageReturn : 138.0724634357421\n",
      "Train_BestReturn : 138.0724634357421\n",
      "TimeSinceStart : 1231.3103339672089\n",
      "Training Loss : 0.9736014008522034\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 367001\n",
      "mean reward (100 episodes) 140.442024\n",
      "best mean reward 140.442024\n",
      "running time 1234.014262\n",
      "Train_EnvstepsSoFar : 367001\n",
      "Train_AverageReturn : 140.44202414409835\n",
      "Train_BestReturn : 140.44202414409835\n",
      "TimeSinceStart : 1234.0142619609833\n",
      "Training Loss : 0.1635926216840744\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 368001\n",
      "mean reward (100 episodes) 140.995970\n",
      "best mean reward 140.995970\n",
      "running time 1236.900949\n",
      "Train_EnvstepsSoFar : 368001\n",
      "Train_AverageReturn : 140.99597022082153\n",
      "Train_BestReturn : 140.99597022082153\n",
      "TimeSinceStart : 1236.9009490013123\n",
      "Training Loss : 1.719901442527771\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 369001\n",
      "mean reward (100 episodes) 141.809419\n",
      "best mean reward 141.809419\n",
      "running time 1239.599416\n",
      "Train_EnvstepsSoFar : 369001\n",
      "Train_AverageReturn : 141.80941877070111\n",
      "Train_BestReturn : 141.80941877070111\n",
      "TimeSinceStart : 1239.5994157791138\n",
      "Training Loss : 0.24151694774627686\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 370001\n",
      "mean reward (100 episodes) 148.507527\n",
      "best mean reward 148.507527\n",
      "running time 1242.293037\n",
      "Train_EnvstepsSoFar : 370001\n",
      "Train_AverageReturn : 148.50752667326506\n",
      "Train_BestReturn : 148.50752667326506\n",
      "TimeSinceStart : 1242.293036699295\n",
      "Training Loss : 0.38050273060798645\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 371001\n",
      "mean reward (100 episodes) 145.822370\n",
      "best mean reward 148.507527\n",
      "running time 1244.931204\n",
      "Train_EnvstepsSoFar : 371001\n",
      "Train_AverageReturn : 145.82237035360748\n",
      "Train_BestReturn : 148.50752667326506\n",
      "TimeSinceStart : 1244.931203842163\n",
      "Training Loss : 3.6143016815185547\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 372001\n",
      "mean reward (100 episodes) 149.629795\n",
      "best mean reward 149.629795\n",
      "running time 1247.574742\n",
      "Train_EnvstepsSoFar : 372001\n",
      "Train_AverageReturn : 149.62979473671126\n",
      "Train_BestReturn : 149.62979473671126\n",
      "TimeSinceStart : 1247.5747418403625\n",
      "Training Loss : 0.15996882319450378\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 373001\n",
      "mean reward (100 episodes) 148.659911\n",
      "best mean reward 149.629795\n",
      "running time 1250.232781\n",
      "Train_EnvstepsSoFar : 373001\n",
      "Train_AverageReturn : 148.65991122125908\n",
      "Train_BestReturn : 149.62979473671126\n",
      "TimeSinceStart : 1250.2327806949615\n",
      "Training Loss : 1.8306374549865723\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 374001\n",
      "mean reward (100 episodes) 150.366489\n",
      "best mean reward 150.366489\n",
      "running time 1253.234924\n",
      "Train_EnvstepsSoFar : 374001\n",
      "Train_AverageReturn : 150.36648915463388\n",
      "Train_BestReturn : 150.36648915463388\n",
      "TimeSinceStart : 1253.2349236011505\n",
      "Training Loss : 3.365379810333252\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 375001\n",
      "mean reward (100 episodes) 148.772625\n",
      "best mean reward 150.366489\n",
      "running time 1255.866809\n",
      "Train_EnvstepsSoFar : 375001\n",
      "Train_AverageReturn : 148.77262510696104\n",
      "Train_BestReturn : 150.36648915463388\n",
      "TimeSinceStart : 1255.8668086528778\n",
      "Training Loss : 0.1746419370174408\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 376001\n",
      "mean reward (100 episodes) 152.546859\n",
      "best mean reward 152.546859\n",
      "running time 1258.520291\n",
      "Train_EnvstepsSoFar : 376001\n",
      "Train_AverageReturn : 152.54685948046267\n",
      "Train_BestReturn : 152.54685948046267\n",
      "TimeSinceStart : 1258.520290851593\n",
      "Training Loss : 0.18701019883155823\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 377001\n",
      "mean reward (100 episodes) 151.510204\n",
      "best mean reward 152.546859\n",
      "running time 1261.271550\n",
      "Train_EnvstepsSoFar : 377001\n",
      "Train_AverageReturn : 151.51020446632813\n",
      "Train_BestReturn : 152.54685948046267\n",
      "TimeSinceStart : 1261.2715499401093\n",
      "Training Loss : 0.6694074273109436\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 378001\n",
      "mean reward (100 episodes) 153.582931\n",
      "best mean reward 153.582931\n",
      "running time 1263.954222\n",
      "Train_EnvstepsSoFar : 378001\n",
      "Train_AverageReturn : 153.58293085907854\n",
      "Train_BestReturn : 153.58293085907854\n",
      "TimeSinceStart : 1263.9542217254639\n",
      "Training Loss : 4.907017230987549\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 379001\n",
      "mean reward (100 episodes) 148.711180\n",
      "best mean reward 153.582931\n",
      "running time 1266.603958\n",
      "Train_EnvstepsSoFar : 379001\n",
      "Train_AverageReturn : 148.71118044649245\n",
      "Train_BestReturn : 153.58293085907854\n",
      "TimeSinceStart : 1266.6039578914642\n",
      "Training Loss : 0.7368885278701782\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 380001\n",
      "mean reward (100 episodes) 147.566069\n",
      "best mean reward 153.582931\n",
      "running time 1269.242459\n",
      "Train_EnvstepsSoFar : 380001\n",
      "Train_AverageReturn : 147.56606900280676\n",
      "Train_BestReturn : 153.58293085907854\n",
      "TimeSinceStart : 1269.242458820343\n",
      "Training Loss : 1.0750986337661743\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 381001\n",
      "mean reward (100 episodes) 150.784530\n",
      "best mean reward 153.582931\n",
      "running time 1272.005273\n",
      "Train_EnvstepsSoFar : 381001\n",
      "Train_AverageReturn : 150.78452962732518\n",
      "Train_BestReturn : 153.58293085907854\n",
      "TimeSinceStart : 1272.0052728652954\n",
      "Training Loss : 0.3837242126464844\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 382001\n",
      "mean reward (100 episodes) 149.902979\n",
      "best mean reward 153.582931\n",
      "running time 1274.698294\n",
      "Train_EnvstepsSoFar : 382001\n",
      "Train_AverageReturn : 149.90297901410918\n",
      "Train_BestReturn : 153.58293085907854\n",
      "TimeSinceStart : 1274.6982939243317\n",
      "Training Loss : 1.271094560623169\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 383001\n",
      "mean reward (100 episodes) 149.342907\n",
      "best mean reward 153.582931\n",
      "running time 1277.450341\n",
      "Train_EnvstepsSoFar : 383001\n",
      "Train_AverageReturn : 149.34290713218817\n",
      "Train_BestReturn : 153.58293085907854\n",
      "TimeSinceStart : 1277.4503407478333\n",
      "Training Loss : 0.6533610820770264\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 384001\n",
      "mean reward (100 episodes) 150.258396\n",
      "best mean reward 153.582931\n",
      "running time 1280.116611\n",
      "Train_EnvstepsSoFar : 384001\n",
      "Train_AverageReturn : 150.25839608757155\n",
      "Train_BestReturn : 153.58293085907854\n",
      "TimeSinceStart : 1280.1166107654572\n",
      "Training Loss : 0.2097117006778717\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 385001\n",
      "mean reward (100 episodes) 148.568927\n",
      "best mean reward 153.582931\n",
      "running time 1282.790002\n",
      "Train_EnvstepsSoFar : 385001\n",
      "Train_AverageReturn : 148.56892721731637\n",
      "Train_BestReturn : 153.58293085907854\n",
      "TimeSinceStart : 1282.7900018692017\n",
      "Training Loss : 1.2461141347885132\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 386001\n",
      "mean reward (100 episodes) 146.557041\n",
      "best mean reward 153.582931\n",
      "running time 1285.719258\n",
      "Train_EnvstepsSoFar : 386001\n",
      "Train_AverageReturn : 146.55704132661805\n",
      "Train_BestReturn : 153.58293085907854\n",
      "TimeSinceStart : 1285.7192578315735\n",
      "Training Loss : 0.2355031967163086\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 387001\n",
      "mean reward (100 episodes) 146.763162\n",
      "best mean reward 153.582931\n",
      "running time 1288.491547\n",
      "Train_EnvstepsSoFar : 387001\n",
      "Train_AverageReturn : 146.76316206182813\n",
      "Train_BestReturn : 153.58293085907854\n",
      "TimeSinceStart : 1288.491546869278\n",
      "Training Loss : 1.3121756315231323\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 388001\n",
      "mean reward (100 episodes) 145.849082\n",
      "best mean reward 153.582931\n",
      "running time 1291.200745\n",
      "Train_EnvstepsSoFar : 388001\n",
      "Train_AverageReturn : 145.84908215353244\n",
      "Train_BestReturn : 153.58293085907854\n",
      "TimeSinceStart : 1291.2007448673248\n",
      "Training Loss : 0.9968553185462952\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 389001\n",
      "mean reward (100 episodes) 147.165499\n",
      "best mean reward 153.582931\n",
      "running time 1293.912772\n",
      "Train_EnvstepsSoFar : 389001\n",
      "Train_AverageReturn : 147.16549929666795\n",
      "Train_BestReturn : 153.58293085907854\n",
      "TimeSinceStart : 1293.9127717018127\n",
      "Training Loss : 4.9443817138671875\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 390001\n",
      "mean reward (100 episodes) 148.801664\n",
      "best mean reward 153.582931\n",
      "running time 1296.629298\n",
      "Train_EnvstepsSoFar : 390001\n",
      "Train_AverageReturn : 148.8016640769924\n",
      "Train_BestReturn : 153.58293085907854\n",
      "TimeSinceStart : 1296.6292977333069\n",
      "Training Loss : 0.30312925577163696\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 391001\n",
      "mean reward (100 episodes) 151.770152\n",
      "best mean reward 153.582931\n",
      "running time 1299.611576\n",
      "Train_EnvstepsSoFar : 391001\n",
      "Train_AverageReturn : 151.77015157932402\n",
      "Train_BestReturn : 153.58293085907854\n",
      "TimeSinceStart : 1299.6115758419037\n",
      "Training Loss : 0.24653136730194092\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 392001\n",
      "mean reward (100 episodes) 153.186658\n",
      "best mean reward 153.582931\n",
      "running time 1302.411113\n",
      "Train_EnvstepsSoFar : 392001\n",
      "Train_AverageReturn : 153.18665825846676\n",
      "Train_BestReturn : 153.58293085907854\n",
      "TimeSinceStart : 1302.4111127853394\n",
      "Training Loss : 1.9586507081985474\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 393001\n",
      "mean reward (100 episodes) 148.962092\n",
      "best mean reward 153.582931\n",
      "running time 1305.114338\n",
      "Train_EnvstepsSoFar : 393001\n",
      "Train_AverageReturn : 148.9620919776538\n",
      "Train_BestReturn : 153.58293085907854\n",
      "TimeSinceStart : 1305.114337682724\n",
      "Training Loss : 0.808180034160614\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 394001\n",
      "mean reward (100 episodes) 149.053948\n",
      "best mean reward 153.582931\n",
      "running time 1307.819202\n",
      "Train_EnvstepsSoFar : 394001\n",
      "Train_AverageReturn : 149.05394759743174\n",
      "Train_BestReturn : 153.58293085907854\n",
      "TimeSinceStart : 1307.81920170784\n",
      "Training Loss : 1.9106438159942627\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 395001\n",
      "mean reward (100 episodes) 150.670035\n",
      "best mean reward 153.582931\n",
      "running time 1310.583965\n",
      "Train_EnvstepsSoFar : 395001\n",
      "Train_AverageReturn : 150.67003533521262\n",
      "Train_BestReturn : 153.58293085907854\n",
      "TimeSinceStart : 1310.5839648246765\n",
      "Training Loss : 0.3033093512058258\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 396001\n",
      "mean reward (100 episodes) 151.036882\n",
      "best mean reward 153.582931\n",
      "running time 1313.238064\n",
      "Train_EnvstepsSoFar : 396001\n",
      "Train_AverageReturn : 151.0368824355841\n",
      "Train_BestReturn : 153.58293085907854\n",
      "TimeSinceStart : 1313.2380638122559\n",
      "Training Loss : 0.6790340542793274\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 397001\n",
      "mean reward (100 episodes) 150.904676\n",
      "best mean reward 153.582931\n",
      "running time 1316.322190\n",
      "Train_EnvstepsSoFar : 397001\n",
      "Train_AverageReturn : 150.90467599974673\n",
      "Train_BestReturn : 153.58293085907854\n",
      "TimeSinceStart : 1316.3221898078918\n",
      "Training Loss : 2.1146535873413086\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 398001\n",
      "mean reward (100 episodes) 150.870905\n",
      "best mean reward 153.582931\n",
      "running time 1318.968773\n",
      "Train_EnvstepsSoFar : 398001\n",
      "Train_AverageReturn : 150.87090489768707\n",
      "Train_BestReturn : 153.58293085907854\n",
      "TimeSinceStart : 1318.9687728881836\n",
      "Training Loss : 0.09734848141670227\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 399001\n",
      "mean reward (100 episodes) 152.388899\n",
      "best mean reward 153.582931\n",
      "running time 1321.611358\n",
      "Train_EnvstepsSoFar : 399001\n",
      "Train_AverageReturn : 152.38889854016597\n",
      "Train_BestReturn : 153.58293085907854\n",
      "TimeSinceStart : 1321.6113579273224\n",
      "Training Loss : 0.14503394067287445\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 400001\n",
      "mean reward (100 episodes) 149.429850\n",
      "best mean reward 153.582931\n",
      "running time 1324.386118\n",
      "Train_EnvstepsSoFar : 400001\n",
      "Train_AverageReturn : 149.42984975509063\n",
      "Train_BestReturn : 153.58293085907854\n",
      "TimeSinceStart : 1324.3861179351807\n",
      "Training Loss : 0.261008620262146\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 401001\n",
      "mean reward (100 episodes) 153.183495\n",
      "best mean reward 153.582931\n",
      "running time 1327.027465\n",
      "Train_EnvstepsSoFar : 401001\n",
      "Train_AverageReturn : 153.1834948104372\n",
      "Train_BestReturn : 153.58293085907854\n",
      "TimeSinceStart : 1327.0274648666382\n",
      "Training Loss : 1.0000654458999634\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 402001\n",
      "mean reward (100 episodes) 155.323119\n",
      "best mean reward 155.323119\n",
      "running time 1329.693271\n",
      "Train_EnvstepsSoFar : 402001\n",
      "Train_AverageReturn : 155.3231187915049\n",
      "Train_BestReturn : 155.3231187915049\n",
      "TimeSinceStart : 1329.6932706832886\n",
      "Training Loss : 0.24441415071487427\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 403001\n",
      "mean reward (100 episodes) 157.671879\n",
      "best mean reward 157.671879\n",
      "running time 1332.321713\n",
      "Train_EnvstepsSoFar : 403001\n",
      "Train_AverageReturn : 157.6718792991972\n",
      "Train_BestReturn : 157.6718792991972\n",
      "TimeSinceStart : 1332.321712732315\n",
      "Training Loss : 0.2329007387161255\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 404001\n",
      "mean reward (100 episodes) 157.903032\n",
      "best mean reward 157.903032\n",
      "running time 1335.064050\n",
      "Train_EnvstepsSoFar : 404001\n",
      "Train_AverageReturn : 157.9030320058246\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1335.0640499591827\n",
      "Training Loss : 1.3543435335159302\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 405001\n",
      "mean reward (100 episodes) 154.457649\n",
      "best mean reward 157.903032\n",
      "running time 1337.703725\n",
      "Train_EnvstepsSoFar : 405001\n",
      "Train_AverageReturn : 154.45764880562808\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1337.7037246227264\n",
      "Training Loss : 0.2007836252450943\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 406001\n",
      "mean reward (100 episodes) 153.510232\n",
      "best mean reward 157.903032\n",
      "running time 1340.369823\n",
      "Train_EnvstepsSoFar : 406001\n",
      "Train_AverageReturn : 153.51023201736564\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1340.3698229789734\n",
      "Training Loss : 0.25000083446502686\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 407001\n",
      "mean reward (100 episodes) 151.017878\n",
      "best mean reward 157.903032\n",
      "running time 1343.194374\n",
      "Train_EnvstepsSoFar : 407001\n",
      "Train_AverageReturn : 151.01787839227458\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1343.194373846054\n",
      "Training Loss : 0.36763477325439453\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 408001\n",
      "mean reward (100 episodes) 154.149129\n",
      "best mean reward 157.903032\n",
      "running time 1345.860375\n",
      "Train_EnvstepsSoFar : 408001\n",
      "Train_AverageReturn : 154.14912895269407\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1345.8603746891022\n",
      "Training Loss : 0.39018499851226807\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 409001\n",
      "mean reward (100 episodes) 150.671057\n",
      "best mean reward 157.903032\n",
      "running time 1348.544316\n",
      "Train_EnvstepsSoFar : 409001\n",
      "Train_AverageReturn : 150.6710571222876\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1348.544315814972\n",
      "Training Loss : 0.78148353099823\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 410001\n",
      "mean reward (100 episodes) 151.948892\n",
      "best mean reward 157.903032\n",
      "running time 1351.269162\n",
      "Train_EnvstepsSoFar : 410001\n",
      "Train_AverageReturn : 151.94889204914858\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1351.269161939621\n",
      "Training Loss : 0.3258347809314728\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 411001\n",
      "mean reward (100 episodes) 145.926796\n",
      "best mean reward 157.903032\n",
      "running time 1354.049358\n",
      "Train_EnvstepsSoFar : 411001\n",
      "Train_AverageReturn : 145.9267958237227\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1354.0493576526642\n",
      "Training Loss : 0.13014741241931915\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 412001\n",
      "mean reward (100 episodes) 143.600168\n",
      "best mean reward 157.903032\n",
      "running time 1356.800024\n",
      "Train_EnvstepsSoFar : 412001\n",
      "Train_AverageReturn : 143.60016846668898\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1356.8000237941742\n",
      "Training Loss : 0.2441984862089157\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 413001\n",
      "mean reward (100 episodes) 138.392600\n",
      "best mean reward 157.903032\n",
      "running time 1359.485060\n",
      "Train_EnvstepsSoFar : 413001\n",
      "Train_AverageReturn : 138.39259973619878\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1359.4850597381592\n",
      "Training Loss : 1.1032164096832275\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 414001\n",
      "mean reward (100 episodes) 140.488177\n",
      "best mean reward 157.903032\n",
      "running time 1362.163687\n",
      "Train_EnvstepsSoFar : 414001\n",
      "Train_AverageReturn : 140.48817660667356\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1362.1636867523193\n",
      "Training Loss : 1.4887738227844238\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 415001\n",
      "mean reward (100 episodes) 137.003045\n",
      "best mean reward 157.903032\n",
      "running time 1364.825515\n",
      "Train_EnvstepsSoFar : 415001\n",
      "Train_AverageReturn : 137.00304546638048\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1364.8255150318146\n",
      "Training Loss : 0.14790479838848114\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 416001\n",
      "mean reward (100 episodes) 139.360669\n",
      "best mean reward 157.903032\n",
      "running time 1367.450046\n",
      "Train_EnvstepsSoFar : 416001\n",
      "Train_AverageReturn : 139.36066940369565\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1367.450045824051\n",
      "Training Loss : 1.0236212015151978\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 417001\n",
      "mean reward (100 episodes) 134.765916\n",
      "best mean reward 157.903032\n",
      "running time 1370.092211\n",
      "Train_EnvstepsSoFar : 417001\n",
      "Train_AverageReturn : 134.76591568105522\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1370.0922107696533\n",
      "Training Loss : 0.16699911653995514\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 418001\n",
      "mean reward (100 episodes) 133.557571\n",
      "best mean reward 157.903032\n",
      "running time 1372.752534\n",
      "Train_EnvstepsSoFar : 418001\n",
      "Train_AverageReturn : 133.55757077344052\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1372.7525339126587\n",
      "Training Loss : 1.6977425813674927\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 419001\n",
      "mean reward (100 episodes) 136.516695\n",
      "best mean reward 157.903032\n",
      "running time 1375.378197\n",
      "Train_EnvstepsSoFar : 419001\n",
      "Train_AverageReturn : 136.51669515171764\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1375.3781969547272\n",
      "Training Loss : 0.39247894287109375\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 420001\n",
      "mean reward (100 episodes) 127.901949\n",
      "best mean reward 157.903032\n",
      "running time 1378.163972\n",
      "Train_EnvstepsSoFar : 420001\n",
      "Train_AverageReturn : 127.90194854047249\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1378.1639716625214\n",
      "Training Loss : 0.24086278676986694\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 421001\n",
      "mean reward (100 episodes) 125.926643\n",
      "best mean reward 157.903032\n",
      "running time 1380.759241\n",
      "Train_EnvstepsSoFar : 421001\n",
      "Train_AverageReturn : 125.92664325604873\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1380.7592408657074\n",
      "Training Loss : 0.3778022825717926\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 422001\n",
      "mean reward (100 episodes) 128.637702\n",
      "best mean reward 157.903032\n",
      "running time 1383.396088\n",
      "Train_EnvstepsSoFar : 422001\n",
      "Train_AverageReturn : 128.63770169083574\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1383.3960876464844\n",
      "Training Loss : 3.954033851623535\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 423001\n",
      "mean reward (100 episodes) 126.841186\n",
      "best mean reward 157.903032\n",
      "running time 1386.089439\n",
      "Train_EnvstepsSoFar : 423001\n",
      "Train_AverageReturn : 126.84118576060006\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1386.0894389152527\n",
      "Training Loss : 0.5808238387107849\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 424001\n",
      "mean reward (100 episodes) 128.989300\n",
      "best mean reward 157.903032\n",
      "running time 1388.755458\n",
      "Train_EnvstepsSoFar : 424001\n",
      "Train_AverageReturn : 128.989300376006\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1388.7554576396942\n",
      "Training Loss : 0.1466679573059082\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 425001\n",
      "mean reward (100 episodes) 134.446262\n",
      "best mean reward 157.903032\n",
      "running time 1391.429643\n",
      "Train_EnvstepsSoFar : 425001\n",
      "Train_AverageReturn : 134.4462615102543\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1391.4296429157257\n",
      "Training Loss : 0.6185731887817383\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 426001\n",
      "mean reward (100 episodes) 136.157905\n",
      "best mean reward 157.903032\n",
      "running time 1394.146157\n",
      "Train_EnvstepsSoFar : 426001\n",
      "Train_AverageReturn : 136.15790463884295\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1394.1461567878723\n",
      "Training Loss : 0.08475694060325623\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 427001\n",
      "mean reward (100 episodes) 135.605221\n",
      "best mean reward 157.903032\n",
      "running time 1396.804406\n",
      "Train_EnvstepsSoFar : 427001\n",
      "Train_AverageReturn : 135.60522075430478\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1396.804405927658\n",
      "Training Loss : 0.23697957396507263\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 428001\n",
      "mean reward (100 episodes) 129.507107\n",
      "best mean reward 157.903032\n",
      "running time 1399.561552\n",
      "Train_EnvstepsSoFar : 428001\n",
      "Train_AverageReturn : 129.50710674486055\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1399.561551809311\n",
      "Training Loss : 1.69284987449646\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 429001\n",
      "mean reward (100 episodes) 125.754671\n",
      "best mean reward 157.903032\n",
      "running time 1402.354364\n",
      "Train_EnvstepsSoFar : 429001\n",
      "Train_AverageReturn : 125.75467140635976\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1402.3543639183044\n",
      "Training Loss : 2.213430643081665\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 430001\n",
      "mean reward (100 episodes) 121.822506\n",
      "best mean reward 157.903032\n",
      "running time 1405.168878\n",
      "Train_EnvstepsSoFar : 430001\n",
      "Train_AverageReturn : 121.82250636550093\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1405.1688778400421\n",
      "Training Loss : 0.7387622594833374\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 431001\n",
      "mean reward (100 episodes) 125.373486\n",
      "best mean reward 157.903032\n",
      "running time 1407.845112\n",
      "Train_EnvstepsSoFar : 431001\n",
      "Train_AverageReturn : 125.37348615478278\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1407.8451118469238\n",
      "Training Loss : 0.16315017640590668\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 432001\n",
      "mean reward (100 episodes) 123.072991\n",
      "best mean reward 157.903032\n",
      "running time 1410.539079\n",
      "Train_EnvstepsSoFar : 432001\n",
      "Train_AverageReturn : 123.07299069685342\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1410.539078950882\n",
      "Training Loss : 0.25943490862846375\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 433001\n",
      "mean reward (100 episodes) 123.234615\n",
      "best mean reward 157.903032\n",
      "running time 1413.224551\n",
      "Train_EnvstepsSoFar : 433001\n",
      "Train_AverageReturn : 123.23461526006221\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1413.2245507240295\n",
      "Training Loss : 0.2645760178565979\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 434001\n",
      "mean reward (100 episodes) 128.395801\n",
      "best mean reward 157.903032\n",
      "running time 1416.224727\n",
      "Train_EnvstepsSoFar : 434001\n",
      "Train_AverageReturn : 128.3958014301679\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1416.224726676941\n",
      "Training Loss : 0.7877106666564941\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 435001\n",
      "mean reward (100 episodes) 129.342961\n",
      "best mean reward 157.903032\n",
      "running time 1419.176953\n",
      "Train_EnvstepsSoFar : 435001\n",
      "Train_AverageReturn : 129.34296105624222\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1419.1769528388977\n",
      "Training Loss : 0.38579121232032776\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 436000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 436001\n",
      "mean reward (100 episodes) 135.025154\n",
      "best mean reward 157.903032\n",
      "running time 1422.232057\n",
      "Train_EnvstepsSoFar : 436001\n",
      "Train_AverageReturn : 135.0251539041294\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1422.2320568561554\n",
      "Training Loss : 0.2556675970554352\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 437001\n",
      "mean reward (100 episodes) 132.191592\n",
      "best mean reward 157.903032\n",
      "running time 1425.484601\n",
      "Train_EnvstepsSoFar : 437001\n",
      "Train_AverageReturn : 132.1915920296521\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1425.4846007823944\n",
      "Training Loss : 2.85265851020813\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 438001\n",
      "mean reward (100 episodes) 133.055092\n",
      "best mean reward 157.903032\n",
      "running time 1428.649257\n",
      "Train_EnvstepsSoFar : 438001\n",
      "Train_AverageReturn : 133.05509174517337\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1428.6492569446564\n",
      "Training Loss : 1.5323423147201538\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 439001\n",
      "mean reward (100 episodes) 134.272760\n",
      "best mean reward 157.903032\n",
      "running time 1431.391995\n",
      "Train_EnvstepsSoFar : 439001\n",
      "Train_AverageReturn : 134.27276047823707\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1431.391994714737\n",
      "Training Loss : 0.12557090818881989\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 440001\n",
      "mean reward (100 episodes) 138.134579\n",
      "best mean reward 157.903032\n",
      "running time 1434.073404\n",
      "Train_EnvstepsSoFar : 440001\n",
      "Train_AverageReturn : 138.13457934698832\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1434.0734038352966\n",
      "Training Loss : 1.1885945796966553\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 441001\n",
      "mean reward (100 episodes) 138.897125\n",
      "best mean reward 157.903032\n",
      "running time 1436.951812\n",
      "Train_EnvstepsSoFar : 441001\n",
      "Train_AverageReturn : 138.8971254615808\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1436.9518117904663\n",
      "Training Loss : 0.23675128817558289\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 442001\n",
      "mean reward (100 episodes) 137.822264\n",
      "best mean reward 157.903032\n",
      "running time 1439.594938\n",
      "Train_EnvstepsSoFar : 442001\n",
      "Train_AverageReturn : 137.82226408059944\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1439.594937801361\n",
      "Training Loss : 0.37000659108161926\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 443001\n",
      "mean reward (100 episodes) 138.726761\n",
      "best mean reward 157.903032\n",
      "running time 1442.215109\n",
      "Train_EnvstepsSoFar : 443001\n",
      "Train_AverageReturn : 138.72676070949402\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1442.21510887146\n",
      "Training Loss : 0.16580909490585327\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 444001\n",
      "mean reward (100 episodes) 137.726256\n",
      "best mean reward 157.903032\n",
      "running time 1445.092142\n",
      "Train_EnvstepsSoFar : 444001\n",
      "Train_AverageReturn : 137.72625600804614\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1445.092141866684\n",
      "Training Loss : 0.23825186491012573\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 445001\n",
      "mean reward (100 episodes) 145.405306\n",
      "best mean reward 157.903032\n",
      "running time 1448.235805\n",
      "Train_EnvstepsSoFar : 445001\n",
      "Train_AverageReturn : 145.40530631021718\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1448.2358047962189\n",
      "Training Loss : 1.4135452508926392\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 446001\n",
      "mean reward (100 episodes) 145.405567\n",
      "best mean reward 157.903032\n",
      "running time 1451.000100\n",
      "Train_EnvstepsSoFar : 446001\n",
      "Train_AverageReturn : 145.4055673686057\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1451.000099658966\n",
      "Training Loss : 1.561687707901001\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 447001\n",
      "mean reward (100 episodes) 141.927833\n",
      "best mean reward 157.903032\n",
      "running time 1453.821255\n",
      "Train_EnvstepsSoFar : 447001\n",
      "Train_AverageReturn : 141.92783279693617\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1453.8212547302246\n",
      "Training Loss : 0.2187948077917099\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 448001\n",
      "mean reward (100 episodes) 132.702112\n",
      "best mean reward 157.903032\n",
      "running time 1456.476050\n",
      "Train_EnvstepsSoFar : 448001\n",
      "Train_AverageReturn : 132.70211209512797\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1456.476049900055\n",
      "Training Loss : 2.1452598571777344\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 449001\n",
      "mean reward (100 episodes) 127.204237\n",
      "best mean reward 157.903032\n",
      "running time 1459.308045\n",
      "Train_EnvstepsSoFar : 449001\n",
      "Train_AverageReturn : 127.2042368576104\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1459.3080446720123\n",
      "Training Loss : 0.1929527372121811\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 450001\n",
      "mean reward (100 episodes) 125.800696\n",
      "best mean reward 157.903032\n",
      "running time 1461.950175\n",
      "Train_EnvstepsSoFar : 450001\n",
      "Train_AverageReturn : 125.80069589847103\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1461.9501748085022\n",
      "Training Loss : 0.1794559806585312\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 451001\n",
      "mean reward (100 episodes) 123.466151\n",
      "best mean reward 157.903032\n",
      "running time 1464.644028\n",
      "Train_EnvstepsSoFar : 451001\n",
      "Train_AverageReturn : 123.46615073078121\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1464.6440279483795\n",
      "Training Loss : 1.4061235189437866\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 452001\n",
      "mean reward (100 episodes) 124.924756\n",
      "best mean reward 157.903032\n",
      "running time 1468.497196\n",
      "Train_EnvstepsSoFar : 452001\n",
      "Train_AverageReturn : 124.92475648597909\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1468.4971957206726\n",
      "Training Loss : 0.20250801742076874\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 453001\n",
      "mean reward (100 episodes) 123.346333\n",
      "best mean reward 157.903032\n",
      "running time 1472.051444\n",
      "Train_EnvstepsSoFar : 453001\n",
      "Train_AverageReturn : 123.34633315705733\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1472.0514438152313\n",
      "Training Loss : 2.055964469909668\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 454001\n",
      "mean reward (100 episodes) 119.791978\n",
      "best mean reward 157.903032\n",
      "running time 1474.710469\n",
      "Train_EnvstepsSoFar : 454001\n",
      "Train_AverageReturn : 119.79197804294934\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1474.7104687690735\n",
      "Training Loss : 2.6593329906463623\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 455001\n",
      "mean reward (100 episodes) 122.788890\n",
      "best mean reward 157.903032\n",
      "running time 1477.653794\n",
      "Train_EnvstepsSoFar : 455001\n",
      "Train_AverageReturn : 122.78888983046164\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1477.653793811798\n",
      "Training Loss : 0.5639436841011047\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 456001\n",
      "mean reward (100 episodes) 111.098092\n",
      "best mean reward 157.903032\n",
      "running time 1480.336446\n",
      "Train_EnvstepsSoFar : 456001\n",
      "Train_AverageReturn : 111.09809245883967\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1480.3364458084106\n",
      "Training Loss : 1.4792919158935547\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 457001\n",
      "mean reward (100 episodes) 105.430965\n",
      "best mean reward 157.903032\n",
      "running time 1483.032308\n",
      "Train_EnvstepsSoFar : 457001\n",
      "Train_AverageReturn : 105.43096457441965\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1483.0323078632355\n",
      "Training Loss : 1.560190200805664\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 458001\n",
      "mean reward (100 episodes) 101.928350\n",
      "best mean reward 157.903032\n",
      "running time 1486.829354\n",
      "Train_EnvstepsSoFar : 458001\n",
      "Train_AverageReturn : 101.92834967088156\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1486.8293538093567\n",
      "Training Loss : 0.3578290641307831\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 459001\n",
      "mean reward (100 episodes) 99.045655\n",
      "best mean reward 157.903032\n",
      "running time 1490.187397\n",
      "Train_EnvstepsSoFar : 459001\n",
      "Train_AverageReturn : 99.04565466402944\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1490.1873967647552\n",
      "Training Loss : 1.3752890825271606\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 460001\n",
      "mean reward (100 episodes) 99.358901\n",
      "best mean reward 157.903032\n",
      "running time 1493.049832\n",
      "Train_EnvstepsSoFar : 460001\n",
      "Train_AverageReturn : 99.3589011969718\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1493.049831867218\n",
      "Training Loss : 1.6120104789733887\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 461001\n",
      "mean reward (100 episodes) 104.290427\n",
      "best mean reward 157.903032\n",
      "running time 1495.657800\n",
      "Train_EnvstepsSoFar : 461001\n",
      "Train_AverageReturn : 104.29042662164578\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1495.6577999591827\n",
      "Training Loss : 1.9981361627578735\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 462001\n",
      "mean reward (100 episodes) 105.572692\n",
      "best mean reward 157.903032\n",
      "running time 1498.376257\n",
      "Train_EnvstepsSoFar : 462001\n",
      "Train_AverageReturn : 105.57269193782113\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1498.376256942749\n",
      "Training Loss : 0.19459745287895203\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 463001\n",
      "mean reward (100 episodes) 104.145365\n",
      "best mean reward 157.903032\n",
      "running time 1501.058962\n",
      "Train_EnvstepsSoFar : 463001\n",
      "Train_AverageReturn : 104.14536474539281\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1501.0589618682861\n",
      "Training Loss : 3.033341646194458\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 464001\n",
      "mean reward (100 episodes) 109.636988\n",
      "best mean reward 157.903032\n",
      "running time 1503.754835\n",
      "Train_EnvstepsSoFar : 464001\n",
      "Train_AverageReturn : 109.63698830087789\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1503.7548348903656\n",
      "Training Loss : 1.4771207571029663\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 465001\n",
      "mean reward (100 episodes) 118.768070\n",
      "best mean reward 157.903032\n",
      "running time 1506.546678\n",
      "Train_EnvstepsSoFar : 465001\n",
      "Train_AverageReturn : 118.76807000625158\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1506.546677827835\n",
      "Training Loss : 1.7336690425872803\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 466001\n",
      "mean reward (100 episodes) 128.586706\n",
      "best mean reward 157.903032\n",
      "running time 1509.247067\n",
      "Train_EnvstepsSoFar : 466001\n",
      "Train_AverageReturn : 128.58670631090297\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1509.2470667362213\n",
      "Training Loss : 0.7692418694496155\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 467001\n",
      "mean reward (100 episodes) 132.133954\n",
      "best mean reward 157.903032\n",
      "running time 1511.871668\n",
      "Train_EnvstepsSoFar : 467001\n",
      "Train_AverageReturn : 132.13395426168262\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1511.8716678619385\n",
      "Training Loss : 3.274822235107422\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 468001\n",
      "mean reward (100 episodes) 128.953872\n",
      "best mean reward 157.903032\n",
      "running time 1514.789586\n",
      "Train_EnvstepsSoFar : 468001\n",
      "Train_AverageReturn : 128.9538716556188\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1514.7895858287811\n",
      "Training Loss : 0.296926885843277\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 469001\n",
      "mean reward (100 episodes) 133.702639\n",
      "best mean reward 157.903032\n",
      "running time 1517.615423\n",
      "Train_EnvstepsSoFar : 469001\n",
      "Train_AverageReturn : 133.7026392313336\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1517.615422964096\n",
      "Training Loss : 0.24933110177516937\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 470001\n",
      "mean reward (100 episodes) 137.584504\n",
      "best mean reward 157.903032\n",
      "running time 1520.334064\n",
      "Train_EnvstepsSoFar : 470001\n",
      "Train_AverageReturn : 137.58450433866557\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1520.3340637683868\n",
      "Training Loss : 1.143932580947876\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 471001\n",
      "mean reward (100 episodes) 134.306606\n",
      "best mean reward 157.903032\n",
      "running time 1523.008235\n",
      "Train_EnvstepsSoFar : 471001\n",
      "Train_AverageReturn : 134.30660569876636\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1523.0082349777222\n",
      "Training Loss : 2.7748019695281982\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 472001\n",
      "mean reward (100 episodes) 141.107175\n",
      "best mean reward 157.903032\n",
      "running time 1525.719099\n",
      "Train_EnvstepsSoFar : 472001\n",
      "Train_AverageReturn : 141.1071749174333\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1525.7190988063812\n",
      "Training Loss : 2.599553346633911\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 473001\n",
      "mean reward (100 episodes) 143.582199\n",
      "best mean reward 157.903032\n",
      "running time 1528.474528\n",
      "Train_EnvstepsSoFar : 473001\n",
      "Train_AverageReturn : 143.58219899684514\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1528.474527835846\n",
      "Training Loss : 0.30686911940574646\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 474001\n",
      "mean reward (100 episodes) 143.572087\n",
      "best mean reward 157.903032\n",
      "running time 1531.182076\n",
      "Train_EnvstepsSoFar : 474001\n",
      "Train_AverageReturn : 143.5720871652561\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1531.1820757389069\n",
      "Training Loss : 0.9953964948654175\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 475001\n",
      "mean reward (100 episodes) 141.727849\n",
      "best mean reward 157.903032\n",
      "running time 1533.903367\n",
      "Train_EnvstepsSoFar : 475001\n",
      "Train_AverageReturn : 141.72784863015818\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1533.903366804123\n",
      "Training Loss : 0.2781893312931061\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 476001\n",
      "mean reward (100 episodes) 140.706516\n",
      "best mean reward 157.903032\n",
      "running time 1536.796465\n",
      "Train_EnvstepsSoFar : 476001\n",
      "Train_AverageReturn : 140.70651615881312\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1536.796464920044\n",
      "Training Loss : 1.0272853374481201\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 477001\n",
      "mean reward (100 episodes) 135.702756\n",
      "best mean reward 157.903032\n",
      "running time 1539.472509\n",
      "Train_EnvstepsSoFar : 477001\n",
      "Train_AverageReturn : 135.70275644149834\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1539.4725089073181\n",
      "Training Loss : 4.390565395355225\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 478001\n",
      "mean reward (100 episodes) 136.063705\n",
      "best mean reward 157.903032\n",
      "running time 1542.532786\n",
      "Train_EnvstepsSoFar : 478001\n",
      "Train_AverageReturn : 136.0637049875669\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1542.5327858924866\n",
      "Training Loss : 1.7801181077957153\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 479001\n",
      "mean reward (100 episodes) 135.106448\n",
      "best mean reward 157.903032\n",
      "running time 1546.532562\n",
      "Train_EnvstepsSoFar : 479001\n",
      "Train_AverageReturn : 135.106447842228\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1546.5325617790222\n",
      "Training Loss : 0.5043876767158508\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 480001\n",
      "mean reward (100 episodes) 138.437128\n",
      "best mean reward 157.903032\n",
      "running time 1549.206067\n",
      "Train_EnvstepsSoFar : 480001\n",
      "Train_AverageReturn : 138.43712755191748\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1549.2060668468475\n",
      "Training Loss : 2.6679508686065674\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 481001\n",
      "mean reward (100 episodes) 143.631918\n",
      "best mean reward 157.903032\n",
      "running time 1551.892132\n",
      "Train_EnvstepsSoFar : 481001\n",
      "Train_AverageReturn : 143.63191849749063\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1551.89213180542\n",
      "Training Loss : 1.8719688653945923\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 482001\n",
      "mean reward (100 episodes) 135.356680\n",
      "best mean reward 157.903032\n",
      "running time 1554.677918\n",
      "Train_EnvstepsSoFar : 482001\n",
      "Train_AverageReturn : 135.35667985906096\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1554.677917957306\n",
      "Training Loss : 0.3848840296268463\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 483001\n",
      "mean reward (100 episodes) 135.059507\n",
      "best mean reward 157.903032\n",
      "running time 1557.468412\n",
      "Train_EnvstepsSoFar : 483001\n",
      "Train_AverageReturn : 135.05950715594264\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1557.4684119224548\n",
      "Training Loss : 7.0603179931640625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 484001\n",
      "mean reward (100 episodes) 139.051061\n",
      "best mean reward 157.903032\n",
      "running time 1560.090743\n",
      "Train_EnvstepsSoFar : 484001\n",
      "Train_AverageReturn : 139.05106060938422\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1560.0907428264618\n",
      "Training Loss : 0.527625322341919\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 485001\n",
      "mean reward (100 episodes) 139.063482\n",
      "best mean reward 157.903032\n",
      "running time 1562.975058\n",
      "Train_EnvstepsSoFar : 485001\n",
      "Train_AverageReturn : 139.06348195321368\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1562.9750578403473\n",
      "Training Loss : 0.9313745498657227\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 486001\n",
      "mean reward (100 episodes) 137.446877\n",
      "best mean reward 157.903032\n",
      "running time 1565.836928\n",
      "Train_EnvstepsSoFar : 486001\n",
      "Train_AverageReturn : 137.446877069507\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1565.8369278907776\n",
      "Training Loss : 7.2237114906311035\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 487001\n",
      "mean reward (100 episodes) 149.949055\n",
      "best mean reward 157.903032\n",
      "running time 1568.603978\n",
      "Train_EnvstepsSoFar : 487001\n",
      "Train_AverageReturn : 149.94905539592878\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1568.6039779186249\n",
      "Training Loss : 0.3094615340232849\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 488001\n",
      "mean reward (100 episodes) 148.633317\n",
      "best mean reward 157.903032\n",
      "running time 1571.410952\n",
      "Train_EnvstepsSoFar : 488001\n",
      "Train_AverageReturn : 148.63331745541836\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1571.4109518527985\n",
      "Training Loss : 0.602627694606781\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 489001\n",
      "mean reward (100 episodes) 147.164626\n",
      "best mean reward 157.903032\n",
      "running time 1574.064977\n",
      "Train_EnvstepsSoFar : 489001\n",
      "Train_AverageReturn : 147.1646255493952\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1574.0649769306183\n",
      "Training Loss : 2.3176727294921875\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 490001\n",
      "mean reward (100 episodes) 145.260016\n",
      "best mean reward 157.903032\n",
      "running time 1576.869653\n",
      "Train_EnvstepsSoFar : 490001\n",
      "Train_AverageReturn : 145.2600163855794\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1576.8696529865265\n",
      "Training Loss : 0.2676627039909363\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 491001\n",
      "mean reward (100 episodes) 149.614827\n",
      "best mean reward 157.903032\n",
      "running time 1579.512069\n",
      "Train_EnvstepsSoFar : 491001\n",
      "Train_AverageReturn : 149.61482715136864\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1579.5120687484741\n",
      "Training Loss : 0.5455753803253174\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 492001\n",
      "mean reward (100 episodes) 148.839253\n",
      "best mean reward 157.903032\n",
      "running time 1582.167103\n",
      "Train_EnvstepsSoFar : 492001\n",
      "Train_AverageReturn : 148.83925326609608\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1582.1671028137207\n",
      "Training Loss : 7.701116561889648\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 493001\n",
      "mean reward (100 episodes) 157.529837\n",
      "best mean reward 157.903032\n",
      "running time 1584.980689\n",
      "Train_EnvstepsSoFar : 493001\n",
      "Train_AverageReturn : 157.52983726979656\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1584.9806888103485\n",
      "Training Loss : 2.1182079315185547\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 494001\n",
      "mean reward (100 episodes) 151.644134\n",
      "best mean reward 157.903032\n",
      "running time 1587.699058\n",
      "Train_EnvstepsSoFar : 494001\n",
      "Train_AverageReturn : 151.64413446679524\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1587.699057817459\n",
      "Training Loss : 0.47214552760124207\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 495001\n",
      "mean reward (100 episodes) 154.806859\n",
      "best mean reward 157.903032\n",
      "running time 1590.318186\n",
      "Train_EnvstepsSoFar : 495001\n",
      "Train_AverageReturn : 154.80685929705024\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1590.3181858062744\n",
      "Training Loss : 0.2518959939479828\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 496001\n",
      "mean reward (100 episodes) 151.332994\n",
      "best mean reward 157.903032\n",
      "running time 1593.013401\n",
      "Train_EnvstepsSoFar : 496001\n",
      "Train_AverageReturn : 151.33299435667337\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1593.0134007930756\n",
      "Training Loss : 1.1000596284866333\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 497001\n",
      "mean reward (100 episodes) 152.697692\n",
      "best mean reward 157.903032\n",
      "running time 1595.664162\n",
      "Train_EnvstepsSoFar : 497001\n",
      "Train_AverageReturn : 152.69769247937117\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1595.664161682129\n",
      "Training Loss : 0.38058945536613464\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 498001\n",
      "mean reward (100 episodes) 154.672489\n",
      "best mean reward 157.903032\n",
      "running time 1598.521404\n",
      "Train_EnvstepsSoFar : 498001\n",
      "Train_AverageReturn : 154.6724885038657\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1598.5214037895203\n",
      "Training Loss : 0.8725965619087219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 499000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 499001\n",
      "mean reward (100 episodes) 154.825662\n",
      "best mean reward 157.903032\n",
      "running time 1601.197697\n",
      "Train_EnvstepsSoFar : 499001\n",
      "Train_AverageReturn : 154.82566152861847\n",
      "Train_BestReturn : 157.9030320058246\n",
      "TimeSinceStart : 1601.1976969242096\n",
      "Training Loss : 0.867414116859436\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Running DQN experiment with seed 2\n",
      "########################\n",
      "logging outputs to  logs/dqn/LunarLander/vanilla_dqn/seed2\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "LunarLander-v3\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1\n",
      "mean reward (100 episodes) nan\n",
      "best mean reward -inf\n",
      "running time 0.000777\n",
      "Train_EnvstepsSoFar : 1\n",
      "TimeSinceStart : 0.0007772445678710938\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1001\n",
      "mean reward (100 episodes) -302.396906\n",
      "best mean reward -inf\n",
      "running time 0.581444\n",
      "Train_EnvstepsSoFar : 1001\n",
      "Train_AverageReturn : -302.3969055199246\n",
      "TimeSinceStart : 0.581444263458252\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2001\n",
      "mean reward (100 episodes) -290.194227\n",
      "best mean reward -inf\n",
      "running time 3.002391\n",
      "Train_EnvstepsSoFar : 2001\n",
      "Train_AverageReturn : -290.1942271627718\n",
      "TimeSinceStart : 3.0023910999298096\n",
      "Training Loss : 3.3053860664367676\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3001\n",
      "mean reward (100 episodes) -298.413160\n",
      "best mean reward -inf\n",
      "running time 5.460215\n",
      "Train_EnvstepsSoFar : 3001\n",
      "Train_AverageReturn : -298.41315997746767\n",
      "TimeSinceStart : 5.460215091705322\n",
      "Training Loss : 3.4131252765655518\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4001\n",
      "mean reward (100 episodes) -303.274264\n",
      "best mean reward -inf\n",
      "running time 7.924805\n",
      "Train_EnvstepsSoFar : 4001\n",
      "Train_AverageReturn : -303.27426366964784\n",
      "TimeSinceStart : 7.924805164337158\n",
      "Training Loss : 3.2468197345733643\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5001\n",
      "mean reward (100 episodes) -283.322683\n",
      "best mean reward -inf\n",
      "running time 10.416163\n",
      "Train_EnvstepsSoFar : 5001\n",
      "Train_AverageReturn : -283.3226827125519\n",
      "TimeSinceStart : 10.416163206100464\n",
      "Training Loss : 2.0919833183288574\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6001\n",
      "mean reward (100 episodes) -276.161746\n",
      "best mean reward -inf\n",
      "running time 12.914108\n",
      "Train_EnvstepsSoFar : 6001\n",
      "Train_AverageReturn : -276.16174638998456\n",
      "TimeSinceStart : 12.914108276367188\n",
      "Training Loss : 5.321133136749268\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7001\n",
      "mean reward (100 episodes) -270.515369\n",
      "best mean reward -inf\n",
      "running time 15.489411\n",
      "Train_EnvstepsSoFar : 7001\n",
      "Train_AverageReturn : -270.5153687994749\n",
      "TimeSinceStart : 15.489411115646362\n",
      "Training Loss : 0.5283859372138977\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8001\n",
      "mean reward (100 episodes) -258.556031\n",
      "best mean reward -inf\n",
      "running time 18.436508\n",
      "Train_EnvstepsSoFar : 8001\n",
      "Train_AverageReturn : -258.5560310711746\n",
      "TimeSinceStart : 18.436508178710938\n",
      "Training Loss : 0.38191282749176025\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9001\n",
      "mean reward (100 episodes) -258.795598\n",
      "best mean reward -inf\n",
      "running time 20.962109\n",
      "Train_EnvstepsSoFar : 9001\n",
      "Train_AverageReturn : -258.7955979440475\n",
      "TimeSinceStart : 20.962109327316284\n",
      "Training Loss : 0.2815515398979187\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10001\n",
      "mean reward (100 episodes) -251.787546\n",
      "best mean reward -inf\n",
      "running time 23.508371\n",
      "Train_EnvstepsSoFar : 10001\n",
      "Train_AverageReturn : -251.78754587422074\n",
      "TimeSinceStart : 23.508371353149414\n",
      "Training Loss : 0.6299819350242615\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 11000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 11001\n",
      "mean reward (100 episodes) -249.551658\n",
      "best mean reward -inf\n",
      "running time 26.131571\n",
      "Train_EnvstepsSoFar : 11001\n",
      "Train_AverageReturn : -249.5516582263604\n",
      "TimeSinceStart : 26.131571292877197\n",
      "Training Loss : 0.38066884875297546\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 12000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 12001\n",
      "mean reward (100 episodes) -250.656014\n",
      "best mean reward -inf\n",
      "running time 28.689655\n",
      "Train_EnvstepsSoFar : 12001\n",
      "Train_AverageReturn : -250.6560143895402\n",
      "TimeSinceStart : 28.6896550655365\n",
      "Training Loss : 0.3989343047142029\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 13000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 13001\n",
      "mean reward (100 episodes) -243.143337\n",
      "best mean reward -inf\n",
      "running time 31.310855\n",
      "Train_EnvstepsSoFar : 13001\n",
      "Train_AverageReturn : -243.14333748545266\n",
      "TimeSinceStart : 31.31085515022278\n",
      "Training Loss : 0.24305537343025208\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 14000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 14001\n",
      "mean reward (100 episodes) -239.036714\n",
      "best mean reward -inf\n",
      "running time 33.978938\n",
      "Train_EnvstepsSoFar : 14001\n",
      "Train_AverageReturn : -239.03671408213813\n",
      "TimeSinceStart : 33.97893810272217\n",
      "Training Loss : 3.459628105163574\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 15000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 15001\n",
      "mean reward (100 episodes) -235.639982\n",
      "best mean reward -inf\n",
      "running time 36.670428\n",
      "Train_EnvstepsSoFar : 15001\n",
      "Train_AverageReturn : -235.63998235493042\n",
      "TimeSinceStart : 36.67042827606201\n",
      "Training Loss : 3.888798475265503\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 16000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 16001\n",
      "mean reward (100 episodes) -233.718345\n",
      "best mean reward -233.718345\n",
      "running time 39.355478\n",
      "Train_EnvstepsSoFar : 16001\n",
      "Train_AverageReturn : -233.7183445782002\n",
      "Train_BestReturn : -233.7183445782002\n",
      "TimeSinceStart : 39.355478048324585\n",
      "Training Loss : 1.3734312057495117\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 17000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 17001\n",
      "mean reward (100 episodes) -226.358278\n",
      "best mean reward -226.358278\n",
      "running time 42.084275\n",
      "Train_EnvstepsSoFar : 17001\n",
      "Train_AverageReturn : -226.35827838493668\n",
      "Train_BestReturn : -226.35827838493668\n",
      "TimeSinceStart : 42.084275245666504\n",
      "Training Loss : 1.1176793575286865\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 18000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 18001\n",
      "mean reward (100 episodes) -225.258180\n",
      "best mean reward -225.258180\n",
      "running time 45.137380\n",
      "Train_EnvstepsSoFar : 18001\n",
      "Train_AverageReturn : -225.25817993534275\n",
      "Train_BestReturn : -225.25817993534275\n",
      "TimeSinceStart : 45.13738012313843\n",
      "Training Loss : 0.382061630487442\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 19000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 19001\n",
      "mean reward (100 episodes) -220.596433\n",
      "best mean reward -220.596433\n",
      "running time 47.912939\n",
      "Train_EnvstepsSoFar : 19001\n",
      "Train_AverageReturn : -220.59643285722018\n",
      "Train_BestReturn : -220.59643285722018\n",
      "TimeSinceStart : 47.91293907165527\n",
      "Training Loss : 0.61069256067276\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 20001\n",
      "mean reward (100 episodes) -218.163592\n",
      "best mean reward -218.163592\n",
      "running time 52.063344\n",
      "Train_EnvstepsSoFar : 20001\n",
      "Train_AverageReturn : -218.1635923055553\n",
      "Train_BestReturn : -218.1635923055553\n",
      "TimeSinceStart : 52.0633442401886\n",
      "Training Loss : 2.14355731010437\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 21000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 21001\n",
      "mean reward (100 episodes) -216.321252\n",
      "best mean reward -216.321252\n",
      "running time 56.211427\n",
      "Train_EnvstepsSoFar : 21001\n",
      "Train_AverageReturn : -216.32125179617532\n",
      "Train_BestReturn : -216.32125179617532\n",
      "TimeSinceStart : 56.211427211761475\n",
      "Training Loss : 0.7472623586654663\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 22000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 22001\n",
      "mean reward (100 episodes) -214.935276\n",
      "best mean reward -214.935276\n",
      "running time 60.267148\n",
      "Train_EnvstepsSoFar : 22001\n",
      "Train_AverageReturn : -214.93527552545484\n",
      "Train_BestReturn : -214.93527552545484\n",
      "TimeSinceStart : 60.2671480178833\n",
      "Training Loss : 2.7300333976745605\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 23000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 23001\n",
      "mean reward (100 episodes) -212.752020\n",
      "best mean reward -212.752020\n",
      "running time 64.410927\n",
      "Train_EnvstepsSoFar : 23001\n",
      "Train_AverageReturn : -212.7520200042847\n",
      "Train_BestReturn : -212.7520200042847\n",
      "TimeSinceStart : 64.41092705726624\n",
      "Training Loss : 0.2850390672683716\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 24000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 24001\n",
      "mean reward (100 episodes) -211.129374\n",
      "best mean reward -211.129374\n",
      "running time 68.526695\n",
      "Train_EnvstepsSoFar : 24001\n",
      "Train_AverageReturn : -211.12937371449505\n",
      "Train_BestReturn : -211.12937371449505\n",
      "TimeSinceStart : 68.52669525146484\n",
      "Training Loss : 0.2926347553730011\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 25000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 25001\n",
      "mean reward (100 episodes) -210.024643\n",
      "best mean reward -210.024643\n",
      "running time 72.548187\n",
      "Train_EnvstepsSoFar : 25001\n",
      "Train_AverageReturn : -210.02464337477065\n",
      "Train_BestReturn : -210.02464337477065\n",
      "TimeSinceStart : 72.54818725585938\n",
      "Training Loss : 2.4287800788879395\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 26000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 26001\n",
      "mean reward (100 episodes) -207.444283\n",
      "best mean reward -207.444283\n",
      "running time 76.510445\n",
      "Train_EnvstepsSoFar : 26001\n",
      "Train_AverageReturn : -207.44428344829967\n",
      "Train_BestReturn : -207.44428344829967\n",
      "TimeSinceStart : 76.51044511795044\n",
      "Training Loss : 2.2151832580566406\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 27000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 27001\n",
      "mean reward (100 episodes) -206.490760\n",
      "best mean reward -206.490760\n",
      "running time 80.096225\n",
      "Train_EnvstepsSoFar : 27001\n",
      "Train_AverageReturn : -206.490760034843\n",
      "Train_BestReturn : -206.490760034843\n",
      "TimeSinceStart : 80.09622502326965\n",
      "Training Loss : 0.46819818019866943\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 28000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 28001\n",
      "mean reward (100 episodes) -204.073625\n",
      "best mean reward -204.073625\n",
      "running time 83.955442\n",
      "Train_EnvstepsSoFar : 28001\n",
      "Train_AverageReturn : -204.073624606781\n",
      "Train_BestReturn : -204.073624606781\n",
      "TimeSinceStart : 83.95544219017029\n",
      "Training Loss : 0.4578268527984619\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 29000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 29001\n",
      "mean reward (100 episodes) -201.886544\n",
      "best mean reward -201.886544\n",
      "running time 87.512258\n",
      "Train_EnvstepsSoFar : 29001\n",
      "Train_AverageReturn : -201.88654402690273\n",
      "Train_BestReturn : -201.88654402690273\n",
      "TimeSinceStart : 87.51225805282593\n",
      "Training Loss : 1.0791950225830078\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 30001\n",
      "mean reward (100 episodes) -199.787173\n",
      "best mean reward -199.787173\n",
      "running time 91.192532\n",
      "Train_EnvstepsSoFar : 30001\n",
      "Train_AverageReturn : -199.78717256383948\n",
      "Train_BestReturn : -199.78717256383948\n",
      "TimeSinceStart : 91.1925323009491\n",
      "Training Loss : 0.32971614599227905\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 31000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 31001\n",
      "mean reward (100 episodes) -195.874562\n",
      "best mean reward -195.874562\n",
      "running time 94.892847\n",
      "Train_EnvstepsSoFar : 31001\n",
      "Train_AverageReturn : -195.8745615845815\n",
      "Train_BestReturn : -195.8745615845815\n",
      "TimeSinceStart : 94.89284706115723\n",
      "Training Loss : 0.4264880120754242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 32000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 32001\n",
      "mean reward (100 episodes) -196.421669\n",
      "best mean reward -195.874562\n",
      "running time 98.917130\n",
      "Train_EnvstepsSoFar : 32001\n",
      "Train_AverageReturn : -196.42166914359797\n",
      "Train_BestReturn : -195.8745615845815\n",
      "TimeSinceStart : 98.9171302318573\n",
      "Training Loss : 0.3782340884208679\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 33000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 33001\n",
      "mean reward (100 episodes) -192.311595\n",
      "best mean reward -192.311595\n",
      "running time 103.291600\n",
      "Train_EnvstepsSoFar : 33001\n",
      "Train_AverageReturn : -192.3115954853713\n",
      "Train_BestReturn : -192.3115954853713\n",
      "TimeSinceStart : 103.29160022735596\n",
      "Training Loss : 0.3783520460128784\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 34000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 34001\n",
      "mean reward (100 episodes) -191.898857\n",
      "best mean reward -191.898857\n",
      "running time 107.470527\n",
      "Train_EnvstepsSoFar : 34001\n",
      "Train_AverageReturn : -191.89885654460838\n",
      "Train_BestReturn : -191.89885654460838\n",
      "TimeSinceStart : 107.4705274105072\n",
      "Training Loss : 0.40212348103523254\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 35000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 35001\n",
      "mean reward (100 episodes) -190.384332\n",
      "best mean reward -190.384332\n",
      "running time 111.442716\n",
      "Train_EnvstepsSoFar : 35001\n",
      "Train_AverageReturn : -190.38433163298964\n",
      "Train_BestReturn : -190.38433163298964\n",
      "TimeSinceStart : 111.44271612167358\n",
      "Training Loss : 0.43242666125297546\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 36000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 36001\n",
      "mean reward (100 episodes) -190.859439\n",
      "best mean reward -190.384332\n",
      "running time 115.791739\n",
      "Train_EnvstepsSoFar : 36001\n",
      "Train_AverageReturn : -190.85943856819125\n",
      "Train_BestReturn : -190.38433163298964\n",
      "TimeSinceStart : 115.79173922538757\n",
      "Training Loss : 1.268868327140808\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 37000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 37001\n",
      "mean reward (100 episodes) -188.503938\n",
      "best mean reward -188.503938\n",
      "running time 119.893769\n",
      "Train_EnvstepsSoFar : 37001\n",
      "Train_AverageReturn : -188.50393833048304\n",
      "Train_BestReturn : -188.50393833048304\n",
      "TimeSinceStart : 119.89376926422119\n",
      "Training Loss : 0.4137640595436096\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 38000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 38001\n",
      "mean reward (100 episodes) -185.243770\n",
      "best mean reward -185.243770\n",
      "running time 124.238079\n",
      "Train_EnvstepsSoFar : 38001\n",
      "Train_AverageReturn : -185.24377019181694\n",
      "Train_BestReturn : -185.24377019181694\n",
      "TimeSinceStart : 124.2380793094635\n",
      "Training Loss : 0.43232885003089905\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 39000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 39001\n",
      "mean reward (100 episodes) -185.468351\n",
      "best mean reward -185.243770\n",
      "running time 128.440406\n",
      "Train_EnvstepsSoFar : 39001\n",
      "Train_AverageReturn : -185.4683511029147\n",
      "Train_BestReturn : -185.24377019181694\n",
      "TimeSinceStart : 128.44040608406067\n",
      "Training Loss : 1.2212713956832886\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 40001\n",
      "mean reward (100 episodes) -182.281577\n",
      "best mean reward -182.281577\n",
      "running time 132.633012\n",
      "Train_EnvstepsSoFar : 40001\n",
      "Train_AverageReturn : -182.28157722632324\n",
      "Train_BestReturn : -182.28157722632324\n",
      "TimeSinceStart : 132.6330122947693\n",
      "Training Loss : 0.4034845530986786\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 41000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 41001\n",
      "mean reward (100 episodes) -182.743266\n",
      "best mean reward -182.281577\n",
      "running time 136.322858\n",
      "Train_EnvstepsSoFar : 41001\n",
      "Train_AverageReturn : -182.74326581069298\n",
      "Train_BestReturn : -182.28157722632324\n",
      "TimeSinceStart : 136.32285809516907\n",
      "Training Loss : 0.9511851668357849\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 42000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 42001\n",
      "mean reward (100 episodes) -184.660763\n",
      "best mean reward -182.281577\n",
      "running time 139.550649\n",
      "Train_EnvstepsSoFar : 42001\n",
      "Train_AverageReturn : -184.66076338664004\n",
      "Train_BestReturn : -182.28157722632324\n",
      "TimeSinceStart : 139.55064916610718\n",
      "Training Loss : 0.4581087529659271\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 43000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 43001\n",
      "mean reward (100 episodes) -183.222842\n",
      "best mean reward -182.281577\n",
      "running time 143.253348\n",
      "Train_EnvstepsSoFar : 43001\n",
      "Train_AverageReturn : -183.2228423131911\n",
      "Train_BestReturn : -182.28157722632324\n",
      "TimeSinceStart : 143.25334811210632\n",
      "Training Loss : 0.5175029039382935\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 44000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 44001\n",
      "mean reward (100 episodes) -182.459552\n",
      "best mean reward -182.281577\n",
      "running time 146.888342\n",
      "Train_EnvstepsSoFar : 44001\n",
      "Train_AverageReturn : -182.45955238001795\n",
      "Train_BestReturn : -182.28157722632324\n",
      "TimeSinceStart : 146.8883421421051\n",
      "Training Loss : 0.33642974495887756\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 45000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 45001\n",
      "mean reward (100 episodes) -179.505866\n",
      "best mean reward -179.505866\n",
      "running time 151.618460\n",
      "Train_EnvstepsSoFar : 45001\n",
      "Train_AverageReturn : -179.50586579872433\n",
      "Train_BestReturn : -179.50586579872433\n",
      "TimeSinceStart : 151.61846017837524\n",
      "Training Loss : 0.2901488244533539\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 46000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 46001\n",
      "mean reward (100 episodes) -176.864171\n",
      "best mean reward -176.864171\n",
      "running time 155.283469\n",
      "Train_EnvstepsSoFar : 46001\n",
      "Train_AverageReturn : -176.8641708567184\n",
      "Train_BestReturn : -176.8641708567184\n",
      "TimeSinceStart : 155.28346920013428\n",
      "Training Loss : 0.29386472702026367\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 47000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 47001\n",
      "mean reward (100 episodes) -174.574816\n",
      "best mean reward -174.574816\n",
      "running time 158.997934\n",
      "Train_EnvstepsSoFar : 47001\n",
      "Train_AverageReturn : -174.57481591495886\n",
      "Train_BestReturn : -174.57481591495886\n",
      "TimeSinceStart : 158.99793410301208\n",
      "Training Loss : 0.3440930247306824\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 48000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 48001\n",
      "mean reward (100 episodes) -174.874989\n",
      "best mean reward -174.574816\n",
      "running time 162.677056\n",
      "Train_EnvstepsSoFar : 48001\n",
      "Train_AverageReturn : -174.87498901209895\n",
      "Train_BestReturn : -174.57481591495886\n",
      "TimeSinceStart : 162.67705607414246\n",
      "Training Loss : 0.43686163425445557\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 49000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 49001\n",
      "mean reward (100 episodes) -174.453959\n",
      "best mean reward -174.453959\n",
      "running time 167.391695\n",
      "Train_EnvstepsSoFar : 49001\n",
      "Train_AverageReturn : -174.453958581402\n",
      "Train_BestReturn : -174.453958581402\n",
      "TimeSinceStart : 167.3916952610016\n",
      "Training Loss : 0.36173638701438904\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 50001\n",
      "mean reward (100 episodes) -173.417967\n",
      "best mean reward -173.417967\n",
      "running time 170.690857\n",
      "Train_EnvstepsSoFar : 50001\n",
      "Train_AverageReturn : -173.4179674685689\n",
      "Train_BestReturn : -173.4179674685689\n",
      "TimeSinceStart : 170.69085717201233\n",
      "Training Loss : 0.3554154336452484\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 51000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 51001\n",
      "mean reward (100 episodes) -172.206633\n",
      "best mean reward -172.206633\n",
      "running time 174.355266\n",
      "Train_EnvstepsSoFar : 51001\n",
      "Train_AverageReturn : -172.2066327925352\n",
      "Train_BestReturn : -172.2066327925352\n",
      "TimeSinceStart : 174.35526609420776\n",
      "Training Loss : 0.5548858046531677\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 52000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 52001\n",
      "mean reward (100 episodes) -171.735765\n",
      "best mean reward -171.735765\n",
      "running time 178.749870\n",
      "Train_EnvstepsSoFar : 52001\n",
      "Train_AverageReturn : -171.73576469898188\n",
      "Train_BestReturn : -171.73576469898188\n",
      "TimeSinceStart : 178.7498700618744\n",
      "Training Loss : 0.35165709257125854\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 53000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 53001\n",
      "mean reward (100 episodes) -169.787025\n",
      "best mean reward -169.787025\n",
      "running time 182.671383\n",
      "Train_EnvstepsSoFar : 53001\n",
      "Train_AverageReturn : -169.78702543766315\n",
      "Train_BestReturn : -169.78702543766315\n",
      "TimeSinceStart : 182.6713831424713\n",
      "Training Loss : 0.3422594964504242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 54000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 54001\n",
      "mean reward (100 episodes) -168.804139\n",
      "best mean reward -168.804139\n",
      "running time 186.500756\n",
      "Train_EnvstepsSoFar : 54001\n",
      "Train_AverageReturn : -168.80413938928928\n",
      "Train_BestReturn : -168.80413938928928\n",
      "TimeSinceStart : 186.5007562637329\n",
      "Training Loss : 0.26540181040763855\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 55000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 55001\n",
      "mean reward (100 episodes) -171.505179\n",
      "best mean reward -168.804139\n",
      "running time 189.903905\n",
      "Train_EnvstepsSoFar : 55001\n",
      "Train_AverageReturn : -171.50517854819176\n",
      "Train_BestReturn : -168.80413938928928\n",
      "TimeSinceStart : 189.90390515327454\n",
      "Training Loss : 0.35208868980407715\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 56000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 56001\n",
      "mean reward (100 episodes) -171.672360\n",
      "best mean reward -168.804139\n",
      "running time 194.308165\n",
      "Train_EnvstepsSoFar : 56001\n",
      "Train_AverageReturn : -171.67236042416087\n",
      "Train_BestReturn : -168.80413938928928\n",
      "TimeSinceStart : 194.30816507339478\n",
      "Training Loss : 0.27756622433662415\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 57000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 57001\n",
      "mean reward (100 episodes) -170.263256\n",
      "best mean reward -168.804139\n",
      "running time 197.942803\n",
      "Train_EnvstepsSoFar : 57001\n",
      "Train_AverageReturn : -170.26325648594798\n",
      "Train_BestReturn : -168.80413938928928\n",
      "TimeSinceStart : 197.94280338287354\n",
      "Training Loss : 0.340167373418808\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 58000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 58001\n",
      "mean reward (100 episodes) -169.543343\n",
      "best mean reward -168.804139\n",
      "running time 202.347852\n",
      "Train_EnvstepsSoFar : 58001\n",
      "Train_AverageReturn : -169.54334254267943\n",
      "Train_BestReturn : -168.80413938928928\n",
      "TimeSinceStart : 202.34785199165344\n",
      "Training Loss : 2.3380959033966064\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 59000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 59001\n",
      "mean reward (100 episodes) -167.604376\n",
      "best mean reward -167.604376\n",
      "running time 206.176482\n",
      "Train_EnvstepsSoFar : 59001\n",
      "Train_AverageReturn : -167.6043760186627\n",
      "Train_BestReturn : -167.6043760186627\n",
      "TimeSinceStart : 206.17648220062256\n",
      "Training Loss : 0.35121288895606995\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 60001\n",
      "mean reward (100 episodes) -165.441558\n",
      "best mean reward -165.441558\n",
      "running time 210.231830\n",
      "Train_EnvstepsSoFar : 60001\n",
      "Train_AverageReturn : -165.44155750467516\n",
      "Train_BestReturn : -165.44155750467516\n",
      "TimeSinceStart : 210.23183012008667\n",
      "Training Loss : 0.4255813956260681\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 61000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 61001\n",
      "mean reward (100 episodes) -165.294261\n",
      "best mean reward -165.294261\n",
      "running time 214.191412\n",
      "Train_EnvstepsSoFar : 61001\n",
      "Train_AverageReturn : -165.29426116795653\n",
      "Train_BestReturn : -165.29426116795653\n",
      "TimeSinceStart : 214.19141221046448\n",
      "Training Loss : 0.4092591106891632\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 62000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 62001\n",
      "mean reward (100 episodes) -166.472838\n",
      "best mean reward -165.294261\n",
      "running time 218.067038\n",
      "Train_EnvstepsSoFar : 62001\n",
      "Train_AverageReturn : -166.4728376987371\n",
      "Train_BestReturn : -165.29426116795653\n",
      "TimeSinceStart : 218.0670382976532\n",
      "Training Loss : 0.2649959325790405\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 63000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 63001\n",
      "mean reward (100 episodes) -162.163068\n",
      "best mean reward -162.163068\n",
      "running time 221.320347\n",
      "Train_EnvstepsSoFar : 63001\n",
      "Train_AverageReturn : -162.16306832953546\n",
      "Train_BestReturn : -162.16306832953546\n",
      "TimeSinceStart : 221.32034730911255\n",
      "Training Loss : 0.34959346055984497\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 64000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 64001\n",
      "mean reward (100 episodes) -160.692175\n",
      "best mean reward -160.692175\n",
      "running time 224.821048\n",
      "Train_EnvstepsSoFar : 64001\n",
      "Train_AverageReturn : -160.6921745090112\n",
      "Train_BestReturn : -160.6921745090112\n",
      "TimeSinceStart : 224.82104802131653\n",
      "Training Loss : 0.18969537317752838\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 65000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 65001\n",
      "mean reward (100 episodes) -161.731684\n",
      "best mean reward -160.692175\n",
      "running time 228.338080\n",
      "Train_EnvstepsSoFar : 65001\n",
      "Train_AverageReturn : -161.7316843065754\n",
      "Train_BestReturn : -160.6921745090112\n",
      "TimeSinceStart : 228.33808016777039\n",
      "Training Loss : 0.2692853808403015\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 66000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 66001\n",
      "mean reward (100 episodes) -159.557065\n",
      "best mean reward -159.557065\n",
      "running time 232.054518\n",
      "Train_EnvstepsSoFar : 66001\n",
      "Train_AverageReturn : -159.55706467743764\n",
      "Train_BestReturn : -159.55706467743764\n",
      "TimeSinceStart : 232.05451798439026\n",
      "Training Loss : 0.595026969909668\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 67000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 67001\n",
      "mean reward (100 episodes) -161.825803\n",
      "best mean reward -159.557065\n",
      "running time 235.462387\n",
      "Train_EnvstepsSoFar : 67001\n",
      "Train_AverageReturn : -161.82580290041543\n",
      "Train_BestReturn : -159.55706467743764\n",
      "TimeSinceStart : 235.46238732337952\n",
      "Training Loss : 0.2748608887195587\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 68000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 68001\n",
      "mean reward (100 episodes) -162.234880\n",
      "best mean reward -159.557065\n",
      "running time 239.242604\n",
      "Train_EnvstepsSoFar : 68001\n",
      "Train_AverageReturn : -162.23487994108606\n",
      "Train_BestReturn : -159.55706467743764\n",
      "TimeSinceStart : 239.2426040172577\n",
      "Training Loss : 0.2770267724990845\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 69000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 69001\n",
      "mean reward (100 episodes) -158.780660\n",
      "best mean reward -158.780660\n",
      "running time 242.597364\n",
      "Train_EnvstepsSoFar : 69001\n",
      "Train_AverageReturn : -158.78065951928514\n",
      "Train_BestReturn : -158.78065951928514\n",
      "TimeSinceStart : 242.5973641872406\n",
      "Training Loss : 0.1958189308643341\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 70001\n",
      "mean reward (100 episodes) -154.323779\n",
      "best mean reward -154.323779\n",
      "running time 247.241980\n",
      "Train_EnvstepsSoFar : 70001\n",
      "Train_AverageReturn : -154.3237785898004\n",
      "Train_BestReturn : -154.3237785898004\n",
      "TimeSinceStart : 247.24198031425476\n",
      "Training Loss : 0.35372135043144226\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 71000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 71001\n",
      "mean reward (100 episodes) -153.251252\n",
      "best mean reward -153.251252\n",
      "running time 250.213422\n",
      "Train_EnvstepsSoFar : 71001\n",
      "Train_AverageReturn : -153.25125171456614\n",
      "Train_BestReturn : -153.25125171456614\n",
      "TimeSinceStart : 250.21342206001282\n",
      "Training Loss : 0.22099865972995758\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 72000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 72001\n",
      "mean reward (100 episodes) -153.978463\n",
      "best mean reward -153.251252\n",
      "running time 254.209688\n",
      "Train_EnvstepsSoFar : 72001\n",
      "Train_AverageReturn : -153.97846315229918\n",
      "Train_BestReturn : -153.25125171456614\n",
      "TimeSinceStart : 254.2096881866455\n",
      "Training Loss : 0.2639860510826111\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 73000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 73001\n",
      "mean reward (100 episodes) -152.477308\n",
      "best mean reward -152.477308\n",
      "running time 258.043923\n",
      "Train_EnvstepsSoFar : 73001\n",
      "Train_AverageReturn : -152.47730782234092\n",
      "Train_BestReturn : -152.47730782234092\n",
      "TimeSinceStart : 258.0439233779907\n",
      "Training Loss : 0.17673923075199127\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 74000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 74001\n",
      "mean reward (100 episodes) -150.090388\n",
      "best mean reward -150.090388\n",
      "running time 262.227260\n",
      "Train_EnvstepsSoFar : 74001\n",
      "Train_AverageReturn : -150.09038771185362\n",
      "Train_BestReturn : -150.09038771185362\n",
      "TimeSinceStart : 262.22726035118103\n",
      "Training Loss : 0.18151529133319855\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 75000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 75001\n",
      "mean reward (100 episodes) -148.153428\n",
      "best mean reward -148.153428\n",
      "running time 265.678011\n",
      "Train_EnvstepsSoFar : 75001\n",
      "Train_AverageReturn : -148.15342786508066\n",
      "Train_BestReturn : -148.15342786508066\n",
      "TimeSinceStart : 265.67801117897034\n",
      "Training Loss : 0.1454472839832306\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 76000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 76001\n",
      "mean reward (100 episodes) -150.369568\n",
      "best mean reward -148.153428\n",
      "running time 270.070160\n",
      "Train_EnvstepsSoFar : 76001\n",
      "Train_AverageReturn : -150.36956838978955\n",
      "Train_BestReturn : -148.15342786508066\n",
      "TimeSinceStart : 270.07016015052795\n",
      "Training Loss : 0.16346164047718048\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 77000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 77001\n",
      "mean reward (100 episodes) -150.116340\n",
      "best mean reward -148.153428\n",
      "running time 274.127051\n",
      "Train_EnvstepsSoFar : 77001\n",
      "Train_AverageReturn : -150.11634023138126\n",
      "Train_BestReturn : -148.15342786508066\n",
      "TimeSinceStart : 274.127051115036\n",
      "Training Loss : 0.13249947130680084\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 78000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 78001\n",
      "mean reward (100 episodes) -145.784975\n",
      "best mean reward -145.784975\n",
      "running time 277.859155\n",
      "Train_EnvstepsSoFar : 78001\n",
      "Train_AverageReturn : -145.78497454825632\n",
      "Train_BestReturn : -145.78497454825632\n",
      "TimeSinceStart : 277.85915517807007\n",
      "Training Loss : 0.22739431262016296\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 79000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 79001\n",
      "mean reward (100 episodes) -148.330269\n",
      "best mean reward -145.784975\n",
      "running time 281.995651\n",
      "Train_EnvstepsSoFar : 79001\n",
      "Train_AverageReturn : -148.33026922405332\n",
      "Train_BestReturn : -145.78497454825632\n",
      "TimeSinceStart : 281.9956512451172\n",
      "Training Loss : 0.1578381508588791\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 80001\n",
      "mean reward (100 episodes) -148.030982\n",
      "best mean reward -145.784975\n",
      "running time 285.464441\n",
      "Train_EnvstepsSoFar : 80001\n",
      "Train_AverageReturn : -148.0309816328244\n",
      "Train_BestReturn : -145.78497454825632\n",
      "TimeSinceStart : 285.4644410610199\n",
      "Training Loss : 0.36880794167518616\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 81000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 81001\n",
      "mean reward (100 episodes) -147.311064\n",
      "best mean reward -145.784975\n",
      "running time 289.747618\n",
      "Train_EnvstepsSoFar : 81001\n",
      "Train_AverageReturn : -147.3110637917049\n",
      "Train_BestReturn : -145.78497454825632\n",
      "TimeSinceStart : 289.7476181983948\n",
      "Training Loss : 0.16765381395816803\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 82000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 82001\n",
      "mean reward (100 episodes) -147.423234\n",
      "best mean reward -145.784975\n",
      "running time 293.419487\n",
      "Train_EnvstepsSoFar : 82001\n",
      "Train_AverageReturn : -147.42323399989158\n",
      "Train_BestReturn : -145.78497454825632\n",
      "TimeSinceStart : 293.4194872379303\n",
      "Training Loss : 0.23850257694721222\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 83000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 83001\n",
      "mean reward (100 episodes) -147.540027\n",
      "best mean reward -145.784975\n",
      "running time 297.437255\n",
      "Train_EnvstepsSoFar : 83001\n",
      "Train_AverageReturn : -147.54002739722347\n",
      "Train_BestReturn : -145.78497454825632\n",
      "TimeSinceStart : 297.43725514411926\n",
      "Training Loss : 0.2132004350423813\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 84000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 84001\n",
      "mean reward (100 episodes) -143.611545\n",
      "best mean reward -143.611545\n",
      "running time 301.534412\n",
      "Train_EnvstepsSoFar : 84001\n",
      "Train_AverageReturn : -143.61154475279054\n",
      "Train_BestReturn : -143.61154475279054\n",
      "TimeSinceStart : 301.5344121456146\n",
      "Training Loss : 0.23848086595535278\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 85000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 85001\n",
      "mean reward (100 episodes) -143.746094\n",
      "best mean reward -143.611545\n",
      "running time 305.884880\n",
      "Train_EnvstepsSoFar : 85001\n",
      "Train_AverageReturn : -143.74609412494897\n",
      "Train_BestReturn : -143.61154475279054\n",
      "TimeSinceStart : 305.88488030433655\n",
      "Training Loss : 0.21276557445526123\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 86000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 86001\n",
      "mean reward (100 episodes) -143.816223\n",
      "best mean reward -143.611545\n",
      "running time 309.330207\n",
      "Train_EnvstepsSoFar : 86001\n",
      "Train_AverageReturn : -143.81622309234797\n",
      "Train_BestReturn : -143.61154475279054\n",
      "TimeSinceStart : 309.3302073478699\n",
      "Training Loss : 0.17401999235153198\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 87000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 87001\n",
      "mean reward (100 episodes) -141.990510\n",
      "best mean reward -141.990510\n",
      "running time 313.890369\n",
      "Train_EnvstepsSoFar : 87001\n",
      "Train_AverageReturn : -141.99050971459124\n",
      "Train_BestReturn : -141.99050971459124\n",
      "TimeSinceStart : 313.8903691768646\n",
      "Training Loss : 0.20876967906951904\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 88000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 88001\n",
      "mean reward (100 episodes) -139.776670\n",
      "best mean reward -139.776670\n",
      "running time 318.000775\n",
      "Train_EnvstepsSoFar : 88001\n",
      "Train_AverageReturn : -139.77666989331897\n",
      "Train_BestReturn : -139.77666989331897\n",
      "TimeSinceStart : 318.00077533721924\n",
      "Training Loss : 0.5790945887565613\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 89000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 89001\n",
      "mean reward (100 episodes) -139.878388\n",
      "best mean reward -139.776670\n",
      "running time 321.568588\n",
      "Train_EnvstepsSoFar : 89001\n",
      "Train_AverageReturn : -139.87838841779302\n",
      "Train_BestReturn : -139.77666989331897\n",
      "TimeSinceStart : 321.56858801841736\n",
      "Training Loss : 0.2934097647666931\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 90001\n",
      "mean reward (100 episodes) -137.179659\n",
      "best mean reward -137.179659\n",
      "running time 324.895831\n",
      "Train_EnvstepsSoFar : 90001\n",
      "Train_AverageReturn : -137.17965878408057\n",
      "Train_BestReturn : -137.17965878408057\n",
      "TimeSinceStart : 324.89583110809326\n",
      "Training Loss : 0.14359202980995178\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 91000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 91001\n",
      "mean reward (100 episodes) -137.350168\n",
      "best mean reward -137.179659\n",
      "running time 328.386098\n",
      "Train_EnvstepsSoFar : 91001\n",
      "Train_AverageReturn : -137.350167539801\n",
      "Train_BestReturn : -137.17965878408057\n",
      "TimeSinceStart : 328.3860981464386\n",
      "Training Loss : 0.19966858625411987\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 92000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 92001\n",
      "mean reward (100 episodes) -137.040263\n",
      "best mean reward -137.040263\n",
      "running time 332.946190\n",
      "Train_EnvstepsSoFar : 92001\n",
      "Train_AverageReturn : -137.04026336543018\n",
      "Train_BestReturn : -137.04026336543018\n",
      "TimeSinceStart : 332.9461901187897\n",
      "Training Loss : 0.16444066166877747\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 93000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 93001\n",
      "mean reward (100 episodes) -138.720333\n",
      "best mean reward -137.040263\n",
      "running time 336.205194\n",
      "Train_EnvstepsSoFar : 93001\n",
      "Train_AverageReturn : -138.72033306658778\n",
      "Train_BestReturn : -137.04026336543018\n",
      "TimeSinceStart : 336.205194234848\n",
      "Training Loss : 0.24085962772369385\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 94000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 94001\n",
      "mean reward (100 episodes) -137.903795\n",
      "best mean reward -137.040263\n",
      "running time 340.424140\n",
      "Train_EnvstepsSoFar : 94001\n",
      "Train_AverageReturn : -137.90379465891618\n",
      "Train_BestReturn : -137.04026336543018\n",
      "TimeSinceStart : 340.42414021492004\n",
      "Training Loss : 0.2541288435459137\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 95000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 95001\n",
      "mean reward (100 episodes) -139.909549\n",
      "best mean reward -137.040263\n",
      "running time 344.397732\n",
      "Train_EnvstepsSoFar : 95001\n",
      "Train_AverageReturn : -139.90954928267763\n",
      "Train_BestReturn : -137.04026336543018\n",
      "TimeSinceStart : 344.397732257843\n",
      "Training Loss : 0.18976010382175446\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 96000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 96001\n",
      "mean reward (100 episodes) -137.080317\n",
      "best mean reward -137.040263\n",
      "running time 348.440647\n",
      "Train_EnvstepsSoFar : 96001\n",
      "Train_AverageReturn : -137.08031748063777\n",
      "Train_BestReturn : -137.04026336543018\n",
      "TimeSinceStart : 348.44064712524414\n",
      "Training Loss : 0.15052063763141632\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 97000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 97001\n",
      "mean reward (100 episodes) -134.699894\n",
      "best mean reward -134.699894\n",
      "running time 353.145879\n",
      "Train_EnvstepsSoFar : 97001\n",
      "Train_AverageReturn : -134.69989442300346\n",
      "Train_BestReturn : -134.69989442300346\n",
      "TimeSinceStart : 353.14587903022766\n",
      "Training Loss : 0.24895568192005157\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 98000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 98001\n",
      "mean reward (100 episodes) -134.959790\n",
      "best mean reward -134.699894\n",
      "running time 357.830692\n",
      "Train_EnvstepsSoFar : 98001\n",
      "Train_AverageReturn : -134.9597897384944\n",
      "Train_BestReturn : -134.69989442300346\n",
      "TimeSinceStart : 357.8306920528412\n",
      "Training Loss : 0.19053146243095398\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 99000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 99001\n",
      "mean reward (100 episodes) -127.957167\n",
      "best mean reward -127.957167\n",
      "running time 360.894508\n",
      "Train_EnvstepsSoFar : 99001\n",
      "Train_AverageReturn : -127.95716722433497\n",
      "Train_BestReturn : -127.95716722433497\n",
      "TimeSinceStart : 360.8945081233978\n",
      "Training Loss : 0.2479477822780609\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 100001\n",
      "mean reward (100 episodes) -128.858255\n",
      "best mean reward -127.957167\n",
      "running time 364.810489\n",
      "Train_EnvstepsSoFar : 100001\n",
      "Train_AverageReturn : -128.8582554282749\n",
      "Train_BestReturn : -127.95716722433497\n",
      "TimeSinceStart : 364.81048917770386\n",
      "Training Loss : 0.12291235476732254\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 101001\n",
      "mean reward (100 episodes) -132.245784\n",
      "best mean reward -127.957167\n",
      "running time 367.949476\n",
      "Train_EnvstepsSoFar : 101001\n",
      "Train_AverageReturn : -132.2457842085548\n",
      "Train_BestReturn : -127.95716722433497\n",
      "TimeSinceStart : 367.94947624206543\n",
      "Training Loss : 0.11384303867816925\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 102001\n",
      "mean reward (100 episodes) -131.203939\n",
      "best mean reward -127.957167\n",
      "running time 371.033648\n",
      "Train_EnvstepsSoFar : 102001\n",
      "Train_AverageReturn : -131.20393898911792\n",
      "Train_BestReturn : -127.95716722433497\n",
      "TimeSinceStart : 371.0336482524872\n",
      "Training Loss : 0.18051666021347046\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 103001\n",
      "mean reward (100 episodes) -130.496025\n",
      "best mean reward -127.957167\n",
      "running time 375.214231\n",
      "Train_EnvstepsSoFar : 103001\n",
      "Train_AverageReturn : -130.49602473528898\n",
      "Train_BestReturn : -127.95716722433497\n",
      "TimeSinceStart : 375.2142310142517\n",
      "Training Loss : 0.1902359575033188\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 104001\n",
      "mean reward (100 episodes) -129.952685\n",
      "best mean reward -127.957167\n",
      "running time 379.083960\n",
      "Train_EnvstepsSoFar : 104001\n",
      "Train_AverageReturn : -129.95268483944338\n",
      "Train_BestReturn : -127.95716722433497\n",
      "TimeSinceStart : 379.08396005630493\n",
      "Training Loss : 0.18913254141807556\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 105001\n",
      "mean reward (100 episodes) -129.569255\n",
      "best mean reward -127.957167\n",
      "running time 383.156589\n",
      "Train_EnvstepsSoFar : 105001\n",
      "Train_AverageReturn : -129.56925497569483\n",
      "Train_BestReturn : -127.95716722433497\n",
      "TimeSinceStart : 383.1565890312195\n",
      "Training Loss : 0.5404479503631592\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 106001\n",
      "mean reward (100 episodes) -129.025733\n",
      "best mean reward -127.957167\n",
      "running time 386.714438\n",
      "Train_EnvstepsSoFar : 106001\n",
      "Train_AverageReturn : -129.02573340029394\n",
      "Train_BestReturn : -127.95716722433497\n",
      "TimeSinceStart : 386.71443819999695\n",
      "Training Loss : 0.1603335738182068\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 107001\n",
      "mean reward (100 episodes) -127.508157\n",
      "best mean reward -127.508157\n",
      "running time 390.537239\n",
      "Train_EnvstepsSoFar : 107001\n",
      "Train_AverageReturn : -127.50815738934718\n",
      "Train_BestReturn : -127.50815738934718\n",
      "TimeSinceStart : 390.53723907470703\n",
      "Training Loss : 2.759232521057129\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 108001\n",
      "mean reward (100 episodes) -124.920493\n",
      "best mean reward -124.920493\n",
      "running time 394.248020\n",
      "Train_EnvstepsSoFar : 108001\n",
      "Train_AverageReturn : -124.92049334078521\n",
      "Train_BestReturn : -124.92049334078521\n",
      "TimeSinceStart : 394.2480204105377\n",
      "Training Loss : 0.17301879823207855\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 109001\n",
      "mean reward (100 episodes) -125.038298\n",
      "best mean reward -124.920493\n",
      "running time 397.542628\n",
      "Train_EnvstepsSoFar : 109001\n",
      "Train_AverageReturn : -125.03829755394237\n",
      "Train_BestReturn : -124.92049334078521\n",
      "TimeSinceStart : 397.54262804985046\n",
      "Training Loss : 0.22931116819381714\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 110001\n",
      "mean reward (100 episodes) -124.309893\n",
      "best mean reward -124.309893\n",
      "running time 402.242307\n",
      "Train_EnvstepsSoFar : 110001\n",
      "Train_AverageReturn : -124.30989331128272\n",
      "Train_BestReturn : -124.30989331128272\n",
      "TimeSinceStart : 402.2423071861267\n",
      "Training Loss : 0.25928688049316406\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 111001\n",
      "mean reward (100 episodes) -125.100179\n",
      "best mean reward -124.309893\n",
      "running time 405.639699\n",
      "Train_EnvstepsSoFar : 111001\n",
      "Train_AverageReturn : -125.10017940868178\n",
      "Train_BestReturn : -124.30989331128272\n",
      "TimeSinceStart : 405.63969922065735\n",
      "Training Loss : 0.1214909702539444\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 112001\n",
      "mean reward (100 episodes) -124.811016\n",
      "best mean reward -124.309893\n",
      "running time 409.189021\n",
      "Train_EnvstepsSoFar : 112001\n",
      "Train_AverageReturn : -124.81101577203694\n",
      "Train_BestReturn : -124.30989331128272\n",
      "TimeSinceStart : 409.18902134895325\n",
      "Training Loss : 0.11243242025375366\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 113001\n",
      "mean reward (100 episodes) -124.245891\n",
      "best mean reward -124.245891\n",
      "running time 412.982537\n",
      "Train_EnvstepsSoFar : 113001\n",
      "Train_AverageReturn : -124.24589072942953\n",
      "Train_BestReturn : -124.24589072942953\n",
      "TimeSinceStart : 412.9825372695923\n",
      "Training Loss : 0.20260469615459442\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 114001\n",
      "mean reward (100 episodes) -121.757664\n",
      "best mean reward -121.757664\n",
      "running time 416.514624\n",
      "Train_EnvstepsSoFar : 114001\n",
      "Train_AverageReturn : -121.75766401325298\n",
      "Train_BestReturn : -121.75766401325298\n",
      "TimeSinceStart : 416.51462411880493\n",
      "Training Loss : 0.21825674176216125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 115001\n",
      "mean reward (100 episodes) -120.909526\n",
      "best mean reward -120.909526\n",
      "running time 420.784268\n",
      "Train_EnvstepsSoFar : 115001\n",
      "Train_AverageReturn : -120.90952647744986\n",
      "Train_BestReturn : -120.90952647744986\n",
      "TimeSinceStart : 420.78426814079285\n",
      "Training Loss : 0.11646311730146408\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 116001\n",
      "mean reward (100 episodes) -119.987257\n",
      "best mean reward -119.987257\n",
      "running time 425.064172\n",
      "Train_EnvstepsSoFar : 116001\n",
      "Train_AverageReturn : -119.98725737620401\n",
      "Train_BestReturn : -119.98725737620401\n",
      "TimeSinceStart : 425.06417202949524\n",
      "Training Loss : 0.15558850765228271\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 117001\n",
      "mean reward (100 episodes) -118.859562\n",
      "best mean reward -118.859562\n",
      "running time 429.050764\n",
      "Train_EnvstepsSoFar : 117001\n",
      "Train_AverageReturn : -118.85956237898075\n",
      "Train_BestReturn : -118.85956237898075\n",
      "TimeSinceStart : 429.0507640838623\n",
      "Training Loss : 0.2118598222732544\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 118001\n",
      "mean reward (100 episodes) -116.355455\n",
      "best mean reward -116.355455\n",
      "running time 433.776174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 118001\n",
      "Train_AverageReturn : -116.35545531798631\n",
      "Train_BestReturn : -116.35545531798631\n",
      "TimeSinceStart : 433.7761740684509\n",
      "Training Loss : 0.26184317469596863\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 119001\n",
      "mean reward (100 episodes) -115.034593\n",
      "best mean reward -115.034593\n",
      "running time 437.651545\n",
      "Train_EnvstepsSoFar : 119001\n",
      "Train_AverageReturn : -115.03459292908177\n",
      "Train_BestReturn : -115.03459292908177\n",
      "TimeSinceStart : 437.65154504776\n",
      "Training Loss : 0.1298454850912094\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 120001\n",
      "mean reward (100 episodes) -112.362062\n",
      "best mean reward -112.362062\n",
      "running time 441.400427\n",
      "Train_EnvstepsSoFar : 120001\n",
      "Train_AverageReturn : -112.36206243324395\n",
      "Train_BestReturn : -112.36206243324395\n",
      "TimeSinceStart : 441.4004271030426\n",
      "Training Loss : 0.2226596474647522\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 121001\n",
      "mean reward (100 episodes) -110.486031\n",
      "best mean reward -110.486031\n",
      "running time 445.088606\n",
      "Train_EnvstepsSoFar : 121001\n",
      "Train_AverageReturn : -110.48603061419354\n",
      "Train_BestReturn : -110.48603061419354\n",
      "TimeSinceStart : 445.08860635757446\n",
      "Training Loss : 0.16117161512374878\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 122001\n",
      "mean reward (100 episodes) -108.312956\n",
      "best mean reward -108.312956\n",
      "running time 448.800499\n",
      "Train_EnvstepsSoFar : 122001\n",
      "Train_AverageReturn : -108.31295647801852\n",
      "Train_BestReturn : -108.31295647801852\n",
      "TimeSinceStart : 448.8004992008209\n",
      "Training Loss : 1.0279061794281006\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 123001\n",
      "mean reward (100 episodes) -107.517785\n",
      "best mean reward -107.517785\n",
      "running time 452.848783\n",
      "Train_EnvstepsSoFar : 123001\n",
      "Train_AverageReturn : -107.51778511317731\n",
      "Train_BestReturn : -107.51778511317731\n",
      "TimeSinceStart : 452.8487832546234\n",
      "Training Loss : 0.09687523543834686\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 124001\n",
      "mean reward (100 episodes) -106.708985\n",
      "best mean reward -106.708985\n",
      "running time 456.905299\n",
      "Train_EnvstepsSoFar : 124001\n",
      "Train_AverageReturn : -106.70898537388518\n",
      "Train_BestReturn : -106.70898537388518\n",
      "TimeSinceStart : 456.90529918670654\n",
      "Training Loss : 0.48690807819366455\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 125001\n",
      "mean reward (100 episodes) -101.763690\n",
      "best mean reward -101.763690\n",
      "running time 460.242529\n",
      "Train_EnvstepsSoFar : 125001\n",
      "Train_AverageReturn : -101.76369026778956\n",
      "Train_BestReturn : -101.76369026778956\n",
      "TimeSinceStart : 460.24252915382385\n",
      "Training Loss : 0.1762186884880066\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 126001\n",
      "mean reward (100 episodes) -100.157788\n",
      "best mean reward -100.157788\n",
      "running time 463.832168\n",
      "Train_EnvstepsSoFar : 126001\n",
      "Train_AverageReturn : -100.15778757257809\n",
      "Train_BestReturn : -100.15778757257809\n",
      "TimeSinceStart : 463.832168340683\n",
      "Training Loss : 0.17871661484241486\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 127001\n",
      "mean reward (100 episodes) -98.713346\n",
      "best mean reward -98.713346\n",
      "running time 467.313754\n",
      "Train_EnvstepsSoFar : 127001\n",
      "Train_AverageReturn : -98.71334566968599\n",
      "Train_BestReturn : -98.71334566968599\n",
      "TimeSinceStart : 467.3137540817261\n",
      "Training Loss : 0.28056320548057556\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 128001\n",
      "mean reward (100 episodes) -94.247374\n",
      "best mean reward -94.247374\n",
      "running time 470.709390\n",
      "Train_EnvstepsSoFar : 128001\n",
      "Train_AverageReturn : -94.24737449780383\n",
      "Train_BestReturn : -94.24737449780383\n",
      "TimeSinceStart : 470.70939016342163\n",
      "Training Loss : 0.1454295665025711\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 129001\n",
      "mean reward (100 episodes) -93.573179\n",
      "best mean reward -93.573179\n",
      "running time 474.254670\n",
      "Train_EnvstepsSoFar : 129001\n",
      "Train_AverageReturn : -93.57317944431374\n",
      "Train_BestReturn : -93.57317944431374\n",
      "TimeSinceStart : 474.25467014312744\n",
      "Training Loss : 0.17031742632389069\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 130001\n",
      "mean reward (100 episodes) -93.299143\n",
      "best mean reward -93.299143\n",
      "running time 478.430979\n",
      "Train_EnvstepsSoFar : 130001\n",
      "Train_AverageReturn : -93.29914297823149\n",
      "Train_BestReturn : -93.29914297823149\n",
      "TimeSinceStart : 478.4309792518616\n",
      "Training Loss : 0.30731701850891113\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 131001\n",
      "mean reward (100 episodes) -86.373225\n",
      "best mean reward -86.373225\n",
      "running time 482.115604\n",
      "Train_EnvstepsSoFar : 131001\n",
      "Train_AverageReturn : -86.37322478228256\n",
      "Train_BestReturn : -86.37322478228256\n",
      "TimeSinceStart : 482.11560440063477\n",
      "Training Loss : 0.4933376908302307\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 132001\n",
      "mean reward (100 episodes) -82.639108\n",
      "best mean reward -82.639108\n",
      "running time 485.404619\n",
      "Train_EnvstepsSoFar : 132001\n",
      "Train_AverageReturn : -82.63910821872214\n",
      "Train_BestReturn : -82.63910821872214\n",
      "TimeSinceStart : 485.40461897850037\n",
      "Training Loss : 0.09830322116613388\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 133001\n",
      "mean reward (100 episodes) -79.530704\n",
      "best mean reward -79.530704\n",
      "running time 488.505682\n",
      "Train_EnvstepsSoFar : 133001\n",
      "Train_AverageReturn : -79.53070359123109\n",
      "Train_BestReturn : -79.53070359123109\n",
      "TimeSinceStart : 488.5056822299957\n",
      "Training Loss : 0.1743827611207962\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 134001\n",
      "mean reward (100 episodes) -76.459995\n",
      "best mean reward -76.459995\n",
      "running time 492.383509\n",
      "Train_EnvstepsSoFar : 134001\n",
      "Train_AverageReturn : -76.45999456291992\n",
      "Train_BestReturn : -76.45999456291992\n",
      "TimeSinceStart : 492.38350915908813\n",
      "Training Loss : 0.16397377848625183\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 135001\n",
      "mean reward (100 episodes) -74.276780\n",
      "best mean reward -74.276780\n",
      "running time 495.958022\n",
      "Train_EnvstepsSoFar : 135001\n",
      "Train_AverageReturn : -74.27678007750713\n",
      "Train_BestReturn : -74.27678007750713\n",
      "TimeSinceStart : 495.95802211761475\n",
      "Training Loss : 0.2104899138212204\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 136001\n",
      "mean reward (100 episodes) -72.446338\n",
      "best mean reward -72.446338\n",
      "running time 499.399360\n",
      "Train_EnvstepsSoFar : 136001\n",
      "Train_AverageReturn : -72.44633814505224\n",
      "Train_BestReturn : -72.44633814505224\n",
      "TimeSinceStart : 499.3993601799011\n",
      "Training Loss : 0.2605225443840027\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 137001\n",
      "mean reward (100 episodes) -64.901463\n",
      "best mean reward -64.901463\n",
      "running time 502.550292\n",
      "Train_EnvstepsSoFar : 137001\n",
      "Train_AverageReturn : -64.90146277542301\n",
      "Train_BestReturn : -64.90146277542301\n",
      "TimeSinceStart : 502.55029225349426\n",
      "Training Loss : 0.15072020888328552\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 138001\n",
      "mean reward (100 episodes) -61.954972\n",
      "best mean reward -61.954972\n",
      "running time 506.314382\n",
      "Train_EnvstepsSoFar : 138001\n",
      "Train_AverageReturn : -61.95497215218621\n",
      "Train_BestReturn : -61.95497215218621\n",
      "TimeSinceStart : 506.3143820762634\n",
      "Training Loss : 0.18319284915924072\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 139001\n",
      "mean reward (100 episodes) -61.563813\n",
      "best mean reward -61.563813\n",
      "running time 510.855679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 139001\n",
      "Train_AverageReturn : -61.5638133684805\n",
      "Train_BestReturn : -61.5638133684805\n",
      "TimeSinceStart : 510.85567927360535\n",
      "Training Loss : 0.2649880349636078\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 140001\n",
      "mean reward (100 episodes) -57.241312\n",
      "best mean reward -57.241312\n",
      "running time 514.991815\n",
      "Train_EnvstepsSoFar : 140001\n",
      "Train_AverageReturn : -57.24131242237132\n",
      "Train_BestReturn : -57.24131242237132\n",
      "TimeSinceStart : 514.991815328598\n",
      "Training Loss : 0.438372403383255\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 141001\n",
      "mean reward (100 episodes) -55.856632\n",
      "best mean reward -55.856632\n",
      "running time 518.517978\n",
      "Train_EnvstepsSoFar : 141001\n",
      "Train_AverageReturn : -55.85663242933067\n",
      "Train_BestReturn : -55.85663242933067\n",
      "TimeSinceStart : 518.5179781913757\n",
      "Training Loss : 0.4632313549518585\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 142001\n",
      "mean reward (100 episodes) -46.336828\n",
      "best mean reward -46.336828\n",
      "running time 521.616215\n",
      "Train_EnvstepsSoFar : 142001\n",
      "Train_AverageReturn : -46.336827598607044\n",
      "Train_BestReturn : -46.336827598607044\n",
      "TimeSinceStart : 521.6162152290344\n",
      "Training Loss : 0.18618933856487274\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 143001\n",
      "mean reward (100 episodes) -40.392196\n",
      "best mean reward -40.392196\n",
      "running time 524.767183\n",
      "Train_EnvstepsSoFar : 143001\n",
      "Train_AverageReturn : -40.39219551891149\n",
      "Train_BestReturn : -40.39219551891149\n",
      "TimeSinceStart : 524.7671830654144\n",
      "Training Loss : 1.026123046875\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 144001\n",
      "mean reward (100 episodes) -37.388879\n",
      "best mean reward -37.388879\n",
      "running time 528.051172\n",
      "Train_EnvstepsSoFar : 144001\n",
      "Train_AverageReturn : -37.3888791130268\n",
      "Train_BestReturn : -37.3888791130268\n",
      "TimeSinceStart : 528.0511722564697\n",
      "Training Loss : 0.28799134492874146\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 145001\n",
      "mean reward (100 episodes) -35.501668\n",
      "best mean reward -35.501668\n",
      "running time 532.021360\n",
      "Train_EnvstepsSoFar : 145001\n",
      "Train_AverageReturn : -35.50166793630069\n",
      "Train_BestReturn : -35.50166793630069\n",
      "TimeSinceStart : 532.0213601589203\n",
      "Training Loss : 0.43566375970840454\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 146001\n",
      "mean reward (100 episodes) -33.429936\n",
      "best mean reward -33.429936\n",
      "running time 535.990549\n",
      "Train_EnvstepsSoFar : 146001\n",
      "Train_AverageReturn : -33.42993626448664\n",
      "Train_BestReturn : -33.42993626448664\n",
      "TimeSinceStart : 535.9905490875244\n",
      "Training Loss : 0.8578381538391113\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 147001\n",
      "mean reward (100 episodes) -32.782732\n",
      "best mean reward -32.782732\n",
      "running time 539.650506\n",
      "Train_EnvstepsSoFar : 147001\n",
      "Train_AverageReturn : -32.78273192382014\n",
      "Train_BestReturn : -32.78273192382014\n",
      "TimeSinceStart : 539.6505062580109\n",
      "Training Loss : 0.47472959756851196\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 148001\n",
      "mean reward (100 episodes) -29.493536\n",
      "best mean reward -29.493536\n",
      "running time 542.998787\n",
      "Train_EnvstepsSoFar : 148001\n",
      "Train_AverageReturn : -29.49353613471446\n",
      "Train_BestReturn : -29.49353613471446\n",
      "TimeSinceStart : 542.9987871646881\n",
      "Training Loss : 0.09088097512722015\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 149001\n",
      "mean reward (100 episodes) -26.146784\n",
      "best mean reward -26.146784\n",
      "running time 546.081253\n",
      "Train_EnvstepsSoFar : 149001\n",
      "Train_AverageReturn : -26.146783516299315\n",
      "Train_BestReturn : -26.146783516299315\n",
      "TimeSinceStart : 546.0812532901764\n",
      "Training Loss : 0.11908379942178726\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 150001\n",
      "mean reward (100 episodes) -23.590849\n",
      "best mean reward -23.590849\n",
      "running time 550.062193\n",
      "Train_EnvstepsSoFar : 150001\n",
      "Train_AverageReturn : -23.590849366201795\n",
      "Train_BestReturn : -23.590849366201795\n",
      "TimeSinceStart : 550.0621931552887\n",
      "Training Loss : 2.6501433849334717\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 151001\n",
      "mean reward (100 episodes) -21.079174\n",
      "best mean reward -21.079174\n",
      "running time 553.801563\n",
      "Train_EnvstepsSoFar : 151001\n",
      "Train_AverageReturn : -21.079174160735562\n",
      "Train_BestReturn : -21.079174160735562\n",
      "TimeSinceStart : 553.8015632629395\n",
      "Training Loss : 0.16218505799770355\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 152001\n",
      "mean reward (100 episodes) -18.958878\n",
      "best mean reward -18.958878\n",
      "running time 557.235738\n",
      "Train_EnvstepsSoFar : 152001\n",
      "Train_AverageReturn : -18.95887785251325\n",
      "Train_BestReturn : -18.95887785251325\n",
      "TimeSinceStart : 557.2357382774353\n",
      "Training Loss : 0.12437770515680313\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 153001\n",
      "mean reward (100 episodes) -17.747775\n",
      "best mean reward -17.747775\n",
      "running time 561.748684\n",
      "Train_EnvstepsSoFar : 153001\n",
      "Train_AverageReturn : -17.747774644559538\n",
      "Train_BestReturn : -17.747774644559538\n",
      "TimeSinceStart : 561.7486841678619\n",
      "Training Loss : 0.09477223455905914\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 154001\n",
      "mean reward (100 episodes) -17.987701\n",
      "best mean reward -17.747775\n",
      "running time 566.071600\n",
      "Train_EnvstepsSoFar : 154001\n",
      "Train_AverageReturn : -17.987701004661645\n",
      "Train_BestReturn : -17.747774644559538\n",
      "TimeSinceStart : 566.0716001987457\n",
      "Training Loss : 0.9275364875793457\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 155001\n",
      "mean reward (100 episodes) -16.726643\n",
      "best mean reward -16.726643\n",
      "running time 570.433446\n",
      "Train_EnvstepsSoFar : 155001\n",
      "Train_AverageReturn : -16.72664269225165\n",
      "Train_BestReturn : -16.72664269225165\n",
      "TimeSinceStart : 570.4334461688995\n",
      "Training Loss : 0.20239928364753723\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 156001\n",
      "mean reward (100 episodes) -15.377936\n",
      "best mean reward -15.377936\n",
      "running time 574.313801\n",
      "Train_EnvstepsSoFar : 156001\n",
      "Train_AverageReturn : -15.377936354119727\n",
      "Train_BestReturn : -15.377936354119727\n",
      "TimeSinceStart : 574.3138012886047\n",
      "Training Loss : 0.15640656650066376\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 157001\n",
      "mean reward (100 episodes) -12.576653\n",
      "best mean reward -12.576653\n",
      "running time 577.768437\n",
      "Train_EnvstepsSoFar : 157001\n",
      "Train_AverageReturn : -12.57665262749254\n",
      "Train_BestReturn : -12.57665262749254\n",
      "TimeSinceStart : 577.7684371471405\n",
      "Training Loss : 0.5762704014778137\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 158001\n",
      "mean reward (100 episodes) -10.248502\n",
      "best mean reward -10.248502\n",
      "running time 581.968969\n",
      "Train_EnvstepsSoFar : 158001\n",
      "Train_AverageReturn : -10.2485017669645\n",
      "Train_BestReturn : -10.2485017669645\n",
      "TimeSinceStart : 581.9689693450928\n",
      "Training Loss : 0.05964888632297516\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 159001\n",
      "mean reward (100 episodes) -3.514683\n",
      "best mean reward -3.514683\n",
      "running time 585.174495\n",
      "Train_EnvstepsSoFar : 159001\n",
      "Train_AverageReturn : -3.514682613259568\n",
      "Train_BestReturn : -3.514682613259568\n",
      "TimeSinceStart : 585.1744952201843\n",
      "Training Loss : 0.28420060873031616\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 160001\n",
      "mean reward (100 episodes) -3.389754\n",
      "best mean reward -3.389754\n",
      "running time 589.040878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 160001\n",
      "Train_AverageReturn : -3.389753725682612\n",
      "Train_BestReturn : -3.389753725682612\n",
      "TimeSinceStart : 589.0408780574799\n",
      "Training Loss : 0.3007430136203766\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 161001\n",
      "mean reward (100 episodes) 0.316443\n",
      "best mean reward 0.316443\n",
      "running time 592.841819\n",
      "Train_EnvstepsSoFar : 161001\n",
      "Train_AverageReturn : 0.31644287118442976\n",
      "Train_BestReturn : 0.31644287118442976\n",
      "TimeSinceStart : 592.8418192863464\n",
      "Training Loss : 5.273673057556152\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 162001\n",
      "mean reward (100 episodes) 0.610461\n",
      "best mean reward 0.610461\n",
      "running time 597.705613\n",
      "Train_EnvstepsSoFar : 162001\n",
      "Train_AverageReturn : 0.6104609177799613\n",
      "Train_BestReturn : 0.6104609177799613\n",
      "TimeSinceStart : 597.7056131362915\n",
      "Training Loss : 0.1107105165719986\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 163001\n",
      "mean reward (100 episodes) 1.238842\n",
      "best mean reward 1.238842\n",
      "running time 602.364217\n",
      "Train_EnvstepsSoFar : 163001\n",
      "Train_AverageReturn : 1.238841872769572\n",
      "Train_BestReturn : 1.238841872769572\n",
      "TimeSinceStart : 602.364217042923\n",
      "Training Loss : 2.1406476497650146\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 164001\n",
      "mean reward (100 episodes) 4.198734\n",
      "best mean reward 4.198734\n",
      "running time 606.895644\n",
      "Train_EnvstepsSoFar : 164001\n",
      "Train_AverageReturn : 4.198733806384669\n",
      "Train_BestReturn : 4.198733806384669\n",
      "TimeSinceStart : 606.8956441879272\n",
      "Training Loss : 0.179340660572052\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 165001\n",
      "mean reward (100 episodes) 5.582291\n",
      "best mean reward 5.582291\n",
      "running time 610.448910\n",
      "Train_EnvstepsSoFar : 165001\n",
      "Train_AverageReturn : 5.582291310525951\n",
      "Train_BestReturn : 5.582291310525951\n",
      "TimeSinceStart : 610.4489102363586\n",
      "Training Loss : 0.06879808753728867\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 166001\n",
      "mean reward (100 episodes) 6.636417\n",
      "best mean reward 6.636417\n",
      "running time 613.816124\n",
      "Train_EnvstepsSoFar : 166001\n",
      "Train_AverageReturn : 6.636416560976418\n",
      "Train_BestReturn : 6.636416560976418\n",
      "TimeSinceStart : 613.8161242008209\n",
      "Training Loss : 2.0349693298339844\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 167001\n",
      "mean reward (100 episodes) 8.842024\n",
      "best mean reward 8.842024\n",
      "running time 617.515656\n",
      "Train_EnvstepsSoFar : 167001\n",
      "Train_AverageReturn : 8.842024067262358\n",
      "Train_BestReturn : 8.842024067262358\n",
      "TimeSinceStart : 617.5156562328339\n",
      "Training Loss : 0.13176022469997406\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 168001\n",
      "mean reward (100 episodes) 16.072321\n",
      "best mean reward 16.072321\n",
      "running time 620.905038\n",
      "Train_EnvstepsSoFar : 168001\n",
      "Train_AverageReturn : 16.072321333657243\n",
      "Train_BestReturn : 16.072321333657243\n",
      "TimeSinceStart : 620.9050381183624\n",
      "Training Loss : 0.07512126117944717\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 169001\n",
      "mean reward (100 episodes) 17.919151\n",
      "best mean reward 17.919151\n",
      "running time 624.622368\n",
      "Train_EnvstepsSoFar : 169001\n",
      "Train_AverageReturn : 17.919151444314622\n",
      "Train_BestReturn : 17.919151444314622\n",
      "TimeSinceStart : 624.6223680973053\n",
      "Training Loss : 0.16849581897258759\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 170001\n",
      "mean reward (100 episodes) 15.589909\n",
      "best mean reward 17.919151\n",
      "running time 629.463646\n",
      "Train_EnvstepsSoFar : 170001\n",
      "Train_AverageReturn : 15.58990915906118\n",
      "Train_BestReturn : 17.919151444314622\n",
      "TimeSinceStart : 629.4636461734772\n",
      "Training Loss : 0.3148762583732605\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 171001\n",
      "mean reward (100 episodes) 15.675911\n",
      "best mean reward 17.919151\n",
      "running time 633.871595\n",
      "Train_EnvstepsSoFar : 171001\n",
      "Train_AverageReturn : 15.675911465140775\n",
      "Train_BestReturn : 17.919151444314622\n",
      "TimeSinceStart : 633.8715951442719\n",
      "Training Loss : 0.1250586360692978\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 172001\n",
      "mean reward (100 episodes) 17.126890\n",
      "best mean reward 17.919151\n",
      "running time 638.002658\n",
      "Train_EnvstepsSoFar : 172001\n",
      "Train_AverageReturn : 17.126890489766378\n",
      "Train_BestReturn : 17.919151444314622\n",
      "TimeSinceStart : 638.0026581287384\n",
      "Training Loss : 0.10363427549600601\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 173001\n",
      "mean reward (100 episodes) 18.411688\n",
      "best mean reward 18.411688\n",
      "running time 642.026425\n",
      "Train_EnvstepsSoFar : 173001\n",
      "Train_AverageReturn : 18.411687590820982\n",
      "Train_BestReturn : 18.411687590820982\n",
      "TimeSinceStart : 642.0264251232147\n",
      "Training Loss : 0.250689297914505\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 174001\n",
      "mean reward (100 episodes) 20.638229\n",
      "best mean reward 20.638229\n",
      "running time 646.178154\n",
      "Train_EnvstepsSoFar : 174001\n",
      "Train_AverageReturn : 20.63822895308838\n",
      "Train_BestReturn : 20.63822895308838\n",
      "TimeSinceStart : 646.1781539916992\n",
      "Training Loss : 2.0123462677001953\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 175001\n",
      "mean reward (100 episodes) 23.310308\n",
      "best mean reward 23.310308\n",
      "running time 649.702887\n",
      "Train_EnvstepsSoFar : 175001\n",
      "Train_AverageReturn : 23.310307957027632\n",
      "Train_BestReturn : 23.310307957027632\n",
      "TimeSinceStart : 649.7028872966766\n",
      "Training Loss : 0.15032292902469635\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 176001\n",
      "mean reward (100 episodes) 22.999001\n",
      "best mean reward 23.310308\n",
      "running time 653.564092\n",
      "Train_EnvstepsSoFar : 176001\n",
      "Train_AverageReturn : 22.99900082386404\n",
      "Train_BestReturn : 23.310307957027632\n",
      "TimeSinceStart : 653.5640921592712\n",
      "Training Loss : 0.14437441527843475\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 177001\n",
      "mean reward (100 episodes) 27.059136\n",
      "best mean reward 27.059136\n",
      "running time 657.104306\n",
      "Train_EnvstepsSoFar : 177001\n",
      "Train_AverageReturn : 27.05913613695366\n",
      "Train_BestReturn : 27.05913613695366\n",
      "TimeSinceStart : 657.1043062210083\n",
      "Training Loss : 0.062154073268175125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 178001\n",
      "mean reward (100 episodes) 27.583313\n",
      "best mean reward 27.583313\n",
      "running time 661.437151\n",
      "Train_EnvstepsSoFar : 178001\n",
      "Train_AverageReturn : 27.58331297334493\n",
      "Train_BestReturn : 27.58331297334493\n",
      "TimeSinceStart : 661.4371511936188\n",
      "Training Loss : 0.08196743577718735\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 179001\n",
      "mean reward (100 episodes) 28.005010\n",
      "best mean reward 28.005010\n",
      "running time 664.907651\n",
      "Train_EnvstepsSoFar : 179001\n",
      "Train_AverageReturn : 28.005010261818228\n",
      "Train_BestReturn : 28.005010261818228\n",
      "TimeSinceStart : 664.9076511859894\n",
      "Training Loss : 5.1945881843566895\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 180001\n",
      "mean reward (100 episodes) 27.059661\n",
      "best mean reward 28.005010\n",
      "running time 668.913843\n",
      "Train_EnvstepsSoFar : 180001\n",
      "Train_AverageReturn : 27.059661350060882\n",
      "Train_BestReturn : 28.005010261818228\n",
      "TimeSinceStart : 668.9138431549072\n",
      "Training Loss : 0.09188786149024963\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 181001\n",
      "mean reward (100 episodes) 32.073610\n",
      "best mean reward 32.073610\n",
      "running time 672.834341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 181001\n",
      "Train_AverageReturn : 32.073610087613915\n",
      "Train_BestReturn : 32.073610087613915\n",
      "TimeSinceStart : 672.8343412876129\n",
      "Training Loss : 0.07645134627819061\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 182001\n",
      "mean reward (100 episodes) 32.806741\n",
      "best mean reward 32.806741\n",
      "running time 677.663931\n",
      "Train_EnvstepsSoFar : 182001\n",
      "Train_AverageReturn : 32.806741498180514\n",
      "Train_BestReturn : 32.806741498180514\n",
      "TimeSinceStart : 677.6639311313629\n",
      "Training Loss : 0.07221828401088715\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 183001\n",
      "mean reward (100 episodes) 34.789140\n",
      "best mean reward 34.789140\n",
      "running time 681.898948\n",
      "Train_EnvstepsSoFar : 183001\n",
      "Train_AverageReturn : 34.78914042476384\n",
      "Train_BestReturn : 34.78914042476384\n",
      "TimeSinceStart : 681.8989481925964\n",
      "Training Loss : 0.032665424048900604\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 184001\n",
      "mean reward (100 episodes) 34.723712\n",
      "best mean reward 34.789140\n",
      "running time 686.732464\n",
      "Train_EnvstepsSoFar : 184001\n",
      "Train_AverageReturn : 34.72371232942683\n",
      "Train_BestReturn : 34.78914042476384\n",
      "TimeSinceStart : 686.7324643135071\n",
      "Training Loss : 0.07170780003070831\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 185001\n",
      "mean reward (100 episodes) 36.695453\n",
      "best mean reward 36.695453\n",
      "running time 690.127559\n",
      "Train_EnvstepsSoFar : 185001\n",
      "Train_AverageReturn : 36.69545283274656\n",
      "Train_BestReturn : 36.69545283274656\n",
      "TimeSinceStart : 690.1275591850281\n",
      "Training Loss : 0.09448568522930145\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 186001\n",
      "mean reward (100 episodes) 38.144567\n",
      "best mean reward 38.144567\n",
      "running time 694.453754\n",
      "Train_EnvstepsSoFar : 186001\n",
      "Train_AverageReturn : 38.144567064334936\n",
      "Train_BestReturn : 38.144567064334936\n",
      "TimeSinceStart : 694.4537541866302\n",
      "Training Loss : 0.07930143922567368\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 187001\n",
      "mean reward (100 episodes) 39.515793\n",
      "best mean reward 39.515793\n",
      "running time 698.952325\n",
      "Train_EnvstepsSoFar : 187001\n",
      "Train_AverageReturn : 39.51579292661777\n",
      "Train_BestReturn : 39.51579292661777\n",
      "TimeSinceStart : 698.9523251056671\n",
      "Training Loss : 0.07743405550718307\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 188001\n",
      "mean reward (100 episodes) 38.577010\n",
      "best mean reward 39.515793\n",
      "running time 704.264987\n",
      "Train_EnvstepsSoFar : 188001\n",
      "Train_AverageReturn : 38.577009835285416\n",
      "Train_BestReturn : 39.51579292661777\n",
      "TimeSinceStart : 704.2649872303009\n",
      "Training Loss : 0.11507206410169601\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 189001\n",
      "mean reward (100 episodes) 38.712757\n",
      "best mean reward 39.515793\n",
      "running time 708.290589\n",
      "Train_EnvstepsSoFar : 189001\n",
      "Train_AverageReturn : 38.7127569397827\n",
      "Train_BestReturn : 39.51579292661777\n",
      "TimeSinceStart : 708.2905893325806\n",
      "Training Loss : 0.07049348205327988\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 190001\n",
      "mean reward (100 episodes) 37.835534\n",
      "best mean reward 39.515793\n",
      "running time 711.792807\n",
      "Train_EnvstepsSoFar : 190001\n",
      "Train_AverageReturn : 37.835534225528\n",
      "Train_BestReturn : 39.51579292661777\n",
      "TimeSinceStart : 711.792807340622\n",
      "Training Loss : 0.13329049944877625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 191001\n",
      "mean reward (100 episodes) 37.661159\n",
      "best mean reward 39.515793\n",
      "running time 716.259257\n",
      "Train_EnvstepsSoFar : 191001\n",
      "Train_AverageReturn : 37.66115949496054\n",
      "Train_BestReturn : 39.51579292661777\n",
      "TimeSinceStart : 716.2592570781708\n",
      "Training Loss : 0.07511397451162338\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 192001\n",
      "mean reward (100 episodes) 37.549345\n",
      "best mean reward 39.515793\n",
      "running time 720.532621\n",
      "Train_EnvstepsSoFar : 192001\n",
      "Train_AverageReturn : 37.54934514386882\n",
      "Train_BestReturn : 39.51579292661777\n",
      "TimeSinceStart : 720.5326211452484\n",
      "Training Loss : 0.07630947232246399\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 193001\n",
      "mean reward (100 episodes) 37.878650\n",
      "best mean reward 39.515793\n",
      "running time 724.083093\n",
      "Train_EnvstepsSoFar : 193001\n",
      "Train_AverageReturn : 37.878650366108\n",
      "Train_BestReturn : 39.51579292661777\n",
      "TimeSinceStart : 724.0830931663513\n",
      "Training Loss : 0.10523582994937897\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 194001\n",
      "mean reward (100 episodes) 41.732260\n",
      "best mean reward 41.732260\n",
      "running time 727.131258\n",
      "Train_EnvstepsSoFar : 194001\n",
      "Train_AverageReturn : 41.73225999511467\n",
      "Train_BestReturn : 41.73225999511467\n",
      "TimeSinceStart : 727.1312582492828\n",
      "Training Loss : 2.1336257457733154\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 195001\n",
      "mean reward (100 episodes) 44.558642\n",
      "best mean reward 44.558642\n",
      "running time 731.564937\n",
      "Train_EnvstepsSoFar : 195001\n",
      "Train_AverageReturn : 44.558642157328165\n",
      "Train_BestReturn : 44.558642157328165\n",
      "TimeSinceStart : 731.5649373531342\n",
      "Training Loss : 0.16233466565608978\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 196001\n",
      "mean reward (100 episodes) 44.614112\n",
      "best mean reward 44.614112\n",
      "running time 735.142363\n",
      "Train_EnvstepsSoFar : 196001\n",
      "Train_AverageReturn : 44.614112275433165\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 735.1423633098602\n",
      "Training Loss : 5.632750511169434\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 197001\n",
      "mean reward (100 episodes) 43.429201\n",
      "best mean reward 44.614112\n",
      "running time 738.822460\n",
      "Train_EnvstepsSoFar : 197001\n",
      "Train_AverageReturn : 43.429200583611156\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 738.8224601745605\n",
      "Training Loss : 0.2499394565820694\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 198001\n",
      "mean reward (100 episodes) 43.001946\n",
      "best mean reward 44.614112\n",
      "running time 742.496398\n",
      "Train_EnvstepsSoFar : 198001\n",
      "Train_AverageReturn : 43.00194552527409\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 742.4963982105255\n",
      "Training Loss : 0.05904129147529602\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 199001\n",
      "mean reward (100 episodes) 42.381079\n",
      "best mean reward 44.614112\n",
      "running time 746.631081\n",
      "Train_EnvstepsSoFar : 199001\n",
      "Train_AverageReturn : 42.3810792878234\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 746.6310813426971\n",
      "Training Loss : 0.12915785610675812\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 200001\n",
      "mean reward (100 episodes) 41.242399\n",
      "best mean reward 44.614112\n",
      "running time 750.873361\n",
      "Train_EnvstepsSoFar : 200001\n",
      "Train_AverageReturn : 41.242399277728765\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 750.8733611106873\n",
      "Training Loss : 0.10780802369117737\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 201001\n",
      "mean reward (100 episodes) 39.491677\n",
      "best mean reward 44.614112\n",
      "running time 755.695888\n",
      "Train_EnvstepsSoFar : 201001\n",
      "Train_AverageReturn : 39.491676569067856\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 755.6958882808685\n",
      "Training Loss : 0.09458275884389877\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 202001\n",
      "mean reward (100 episodes) 39.402910\n",
      "best mean reward 44.614112\n",
      "running time 759.349751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 202001\n",
      "Train_AverageReturn : 39.402909802494555\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 759.349750995636\n",
      "Training Loss : 0.0875818133354187\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 203001\n",
      "mean reward (100 episodes) 39.515292\n",
      "best mean reward 44.614112\n",
      "running time 763.199542\n",
      "Train_EnvstepsSoFar : 203001\n",
      "Train_AverageReturn : 39.51529163510105\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 763.1995420455933\n",
      "Training Loss : 0.11163073033094406\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 204001\n",
      "mean reward (100 episodes) 39.273146\n",
      "best mean reward 44.614112\n",
      "running time 766.729535\n",
      "Train_EnvstepsSoFar : 204001\n",
      "Train_AverageReturn : 39.27314551093652\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 766.7295353412628\n",
      "Training Loss : 0.06513969600200653\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 205001\n",
      "mean reward (100 episodes) 38.116542\n",
      "best mean reward 44.614112\n",
      "running time 770.117459\n",
      "Train_EnvstepsSoFar : 205001\n",
      "Train_AverageReturn : 38.11654159814889\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 770.1174592971802\n",
      "Training Loss : 0.04947670176625252\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 206001\n",
      "mean reward (100 episodes) 38.212212\n",
      "best mean reward 44.614112\n",
      "running time 774.068664\n",
      "Train_EnvstepsSoFar : 206001\n",
      "Train_AverageReturn : 38.21221212872532\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 774.0686640739441\n",
      "Training Loss : 0.08619870990514755\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 207001\n",
      "mean reward (100 episodes) 37.554355\n",
      "best mean reward 44.614112\n",
      "running time 778.351438\n",
      "Train_EnvstepsSoFar : 207001\n",
      "Train_AverageReturn : 37.55435479908056\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 778.3514380455017\n",
      "Training Loss : 0.15638293325901031\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 208001\n",
      "mean reward (100 episodes) 35.419470\n",
      "best mean reward 44.614112\n",
      "running time 782.473221\n",
      "Train_EnvstepsSoFar : 208001\n",
      "Train_AverageReturn : 35.419470243141646\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 782.4732213020325\n",
      "Training Loss : 0.10188499093055725\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 209001\n",
      "mean reward (100 episodes) 35.164900\n",
      "best mean reward 44.614112\n",
      "running time 786.526765\n",
      "Train_EnvstepsSoFar : 209001\n",
      "Train_AverageReturn : 35.164900247055535\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 786.5267651081085\n",
      "Training Loss : 0.1404060572385788\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 210001\n",
      "mean reward (100 episodes) 35.389521\n",
      "best mean reward 44.614112\n",
      "running time 790.845053\n",
      "Train_EnvstepsSoFar : 210001\n",
      "Train_AverageReturn : 35.389520930501014\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 790.8450531959534\n",
      "Training Loss : 0.05717364326119423\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 211001\n",
      "mean reward (100 episodes) 33.407748\n",
      "best mean reward 44.614112\n",
      "running time 794.741598\n",
      "Train_EnvstepsSoFar : 211001\n",
      "Train_AverageReturn : 33.407748443197626\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 794.741598367691\n",
      "Training Loss : 0.025499803945422173\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 212001\n",
      "mean reward (100 episodes) 33.105825\n",
      "best mean reward 44.614112\n",
      "running time 798.752175\n",
      "Train_EnvstepsSoFar : 212001\n",
      "Train_AverageReturn : 33.10582494526381\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 798.7521750926971\n",
      "Training Loss : 0.07281986624002457\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 213001\n",
      "mean reward (100 episodes) 31.175069\n",
      "best mean reward 44.614112\n",
      "running time 802.794368\n",
      "Train_EnvstepsSoFar : 213001\n",
      "Train_AverageReturn : 31.17506876179857\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 802.7943680286407\n",
      "Training Loss : 0.0453585721552372\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 214001\n",
      "mean reward (100 episodes) 29.492101\n",
      "best mean reward 44.614112\n",
      "running time 806.619480\n",
      "Train_EnvstepsSoFar : 214001\n",
      "Train_AverageReturn : 29.492100942611977\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 806.6194801330566\n",
      "Training Loss : 0.06028527021408081\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 215001\n",
      "mean reward (100 episodes) 31.035284\n",
      "best mean reward 44.614112\n",
      "running time 810.015821\n",
      "Train_EnvstepsSoFar : 215001\n",
      "Train_AverageReturn : 31.03528400553858\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 810.0158212184906\n",
      "Training Loss : 0.08072671294212341\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 216001\n",
      "mean reward (100 episodes) 32.285367\n",
      "best mean reward 44.614112\n",
      "running time 813.666335\n",
      "Train_EnvstepsSoFar : 216001\n",
      "Train_AverageReturn : 32.28536698218858\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 813.6663353443146\n",
      "Training Loss : 0.04940793290734291\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 217001\n",
      "mean reward (100 episodes) 31.761645\n",
      "best mean reward 44.614112\n",
      "running time 817.894241\n",
      "Train_EnvstepsSoFar : 217001\n",
      "Train_AverageReturn : 31.761644590503664\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 817.8942410945892\n",
      "Training Loss : 0.06151294708251953\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 218001\n",
      "mean reward (100 episodes) 31.568824\n",
      "best mean reward 44.614112\n",
      "running time 822.561741\n",
      "Train_EnvstepsSoFar : 218001\n",
      "Train_AverageReturn : 31.568824186392863\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 822.5617411136627\n",
      "Training Loss : 0.1156567633152008\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 219001\n",
      "mean reward (100 episodes) 28.309564\n",
      "best mean reward 44.614112\n",
      "running time 826.546716\n",
      "Train_EnvstepsSoFar : 219001\n",
      "Train_AverageReturn : 28.309564111616705\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 826.5467162132263\n",
      "Training Loss : 0.10920798033475876\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 220001\n",
      "mean reward (100 episodes) 25.545984\n",
      "best mean reward 44.614112\n",
      "running time 831.090975\n",
      "Train_EnvstepsSoFar : 220001\n",
      "Train_AverageReturn : 25.54598404236658\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 831.0909752845764\n",
      "Training Loss : 0.058198198676109314\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 221001\n",
      "mean reward (100 episodes) 26.096709\n",
      "best mean reward 44.614112\n",
      "running time 834.432680\n",
      "Train_EnvstepsSoFar : 221001\n",
      "Train_AverageReturn : 26.09670913756681\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 834.4326803684235\n",
      "Training Loss : 0.19710740447044373\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 222001\n",
      "mean reward (100 episodes) 26.547780\n",
      "best mean reward 44.614112\n",
      "running time 837.902668\n",
      "Train_EnvstepsSoFar : 222001\n",
      "Train_AverageReturn : 26.54778001301504\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 837.9026679992676\n",
      "Training Loss : 0.04162667691707611\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 223001\n",
      "mean reward (100 episodes) 26.777831\n",
      "best mean reward 44.614112\n",
      "running time 841.596451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 223001\n",
      "Train_AverageReturn : 26.777831342488106\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 841.5964510440826\n",
      "Training Loss : 0.05151914432644844\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 224001\n",
      "mean reward (100 episodes) 26.420706\n",
      "best mean reward 44.614112\n",
      "running time 845.005002\n",
      "Train_EnvstepsSoFar : 224001\n",
      "Train_AverageReturn : 26.42070645276496\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 845.0050022602081\n",
      "Training Loss : 0.3533135950565338\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 225001\n",
      "mean reward (100 episodes) 28.485175\n",
      "best mean reward 44.614112\n",
      "running time 849.867886\n",
      "Train_EnvstepsSoFar : 225001\n",
      "Train_AverageReturn : 28.48517452068364\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 849.8678860664368\n",
      "Training Loss : 0.11745458841323853\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 226001\n",
      "mean reward (100 episodes) 26.775003\n",
      "best mean reward 44.614112\n",
      "running time 853.698331\n",
      "Train_EnvstepsSoFar : 226001\n",
      "Train_AverageReturn : 26.775002842969137\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 853.69833111763\n",
      "Training Loss : 0.10111016035079956\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 227001\n",
      "mean reward (100 episodes) 27.017755\n",
      "best mean reward 44.614112\n",
      "running time 856.620832\n",
      "Train_EnvstepsSoFar : 227001\n",
      "Train_AverageReturn : 27.017754652541655\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 856.6208319664001\n",
      "Training Loss : 0.1325494796037674\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 228001\n",
      "mean reward (100 episodes) 29.379050\n",
      "best mean reward 44.614112\n",
      "running time 860.615542\n",
      "Train_EnvstepsSoFar : 228001\n",
      "Train_AverageReturn : 29.379049824013464\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 860.6155421733856\n",
      "Training Loss : 0.07704169303178787\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 229001\n",
      "mean reward (100 episodes) 28.089731\n",
      "best mean reward 44.614112\n",
      "running time 864.250953\n",
      "Train_EnvstepsSoFar : 229001\n",
      "Train_AverageReturn : 28.089731111308488\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 864.2509531974792\n",
      "Training Loss : 0.08203082531690598\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 230001\n",
      "mean reward (100 episodes) 26.904321\n",
      "best mean reward 44.614112\n",
      "running time 867.876731\n",
      "Train_EnvstepsSoFar : 230001\n",
      "Train_AverageReturn : 26.90432058227511\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 867.8767311573029\n",
      "Training Loss : 0.07792037725448608\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 231001\n",
      "mean reward (100 episodes) 28.281648\n",
      "best mean reward 44.614112\n",
      "running time 871.457116\n",
      "Train_EnvstepsSoFar : 231001\n",
      "Train_AverageReturn : 28.281647920858134\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 871.4571161270142\n",
      "Training Loss : 0.05220538750290871\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 232001\n",
      "mean reward (100 episodes) 28.461801\n",
      "best mean reward 44.614112\n",
      "running time 875.349383\n",
      "Train_EnvstepsSoFar : 232001\n",
      "Train_AverageReturn : 28.461800530898387\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 875.349383354187\n",
      "Training Loss : 0.4978712499141693\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 233001\n",
      "mean reward (100 episodes) 32.645254\n",
      "best mean reward 44.614112\n",
      "running time 878.415963\n",
      "Train_EnvstepsSoFar : 233001\n",
      "Train_AverageReturn : 32.64525380216696\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 878.4159631729126\n",
      "Training Loss : 0.062175095081329346\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 234001\n",
      "mean reward (100 episodes) 32.426952\n",
      "best mean reward 44.614112\n",
      "running time 882.152179\n",
      "Train_EnvstepsSoFar : 234001\n",
      "Train_AverageReturn : 32.42695151196169\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 882.1521792411804\n",
      "Training Loss : 0.06287882477045059\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 235001\n",
      "mean reward (100 episodes) 31.861484\n",
      "best mean reward 44.614112\n",
      "running time 885.662337\n",
      "Train_EnvstepsSoFar : 235001\n",
      "Train_AverageReturn : 31.8614844323019\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 885.662337064743\n",
      "Training Loss : 0.1051357313990593\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 236001\n",
      "mean reward (100 episodes) 33.329384\n",
      "best mean reward 44.614112\n",
      "running time 889.852413\n",
      "Train_EnvstepsSoFar : 236001\n",
      "Train_AverageReturn : 33.329384249697526\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 889.8524131774902\n",
      "Training Loss : 0.1557103544473648\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 237001\n",
      "mean reward (100 episodes) 35.009034\n",
      "best mean reward 44.614112\n",
      "running time 893.103277\n",
      "Train_EnvstepsSoFar : 237001\n",
      "Train_AverageReturn : 35.009033732887566\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 893.1032772064209\n",
      "Training Loss : 0.11248894780874252\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 238001\n",
      "mean reward (100 episodes) 34.955694\n",
      "best mean reward 44.614112\n",
      "running time 897.042032\n",
      "Train_EnvstepsSoFar : 238001\n",
      "Train_AverageReturn : 34.95569421835222\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 897.0420322418213\n",
      "Training Loss : 0.1050722524523735\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 239001\n",
      "mean reward (100 episodes) 41.025340\n",
      "best mean reward 44.614112\n",
      "running time 900.176747\n",
      "Train_EnvstepsSoFar : 239001\n",
      "Train_AverageReturn : 41.025339985150765\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 900.1767470836639\n",
      "Training Loss : 0.1600068211555481\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 240001\n",
      "mean reward (100 episodes) 39.176854\n",
      "best mean reward 44.614112\n",
      "running time 904.298671\n",
      "Train_EnvstepsSoFar : 240001\n",
      "Train_AverageReturn : 39.17685373489911\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 904.298671245575\n",
      "Training Loss : 0.034287966787815094\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 241001\n",
      "mean reward (100 episodes) 38.887343\n",
      "best mean reward 44.614112\n",
      "running time 908.638544\n",
      "Train_EnvstepsSoFar : 241001\n",
      "Train_AverageReturn : 38.88734327602043\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 908.6385440826416\n",
      "Training Loss : 0.203013613820076\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 242001\n",
      "mean reward (100 episodes) 39.717341\n",
      "best mean reward 44.614112\n",
      "running time 912.028406\n",
      "Train_EnvstepsSoFar : 242001\n",
      "Train_AverageReturn : 39.71734138907265\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 912.0284061431885\n",
      "Training Loss : 0.07047947496175766\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 243001\n",
      "mean reward (100 episodes) 43.433816\n",
      "best mean reward 44.614112\n",
      "running time 915.138806\n",
      "Train_EnvstepsSoFar : 243001\n",
      "Train_AverageReturn : 43.43381626520939\n",
      "Train_BestReturn : 44.614112275433165\n",
      "TimeSinceStart : 915.13880610466\n",
      "Training Loss : 0.038964807987213135\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 244001\n",
      "mean reward (100 episodes) 49.703821\n",
      "best mean reward 49.703821\n",
      "running time 918.319899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 244001\n",
      "Train_AverageReturn : 49.70382080194691\n",
      "Train_BestReturn : 49.70382080194691\n",
      "TimeSinceStart : 918.3198993206024\n",
      "Training Loss : 0.08140988647937775\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 245001\n",
      "mean reward (100 episodes) 54.870328\n",
      "best mean reward 54.870328\n",
      "running time 921.412668\n",
      "Train_EnvstepsSoFar : 245001\n",
      "Train_AverageReturn : 54.870327695040054\n",
      "Train_BestReturn : 54.870327695040054\n",
      "TimeSinceStart : 921.4126682281494\n",
      "Training Loss : 0.16218678653240204\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 246001\n",
      "mean reward (100 episodes) 60.176163\n",
      "best mean reward 60.176163\n",
      "running time 924.549884\n",
      "Train_EnvstepsSoFar : 246001\n",
      "Train_AverageReturn : 60.17616267389957\n",
      "Train_BestReturn : 60.17616267389957\n",
      "TimeSinceStart : 924.5498843193054\n",
      "Training Loss : 0.06997520476579666\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 247001\n",
      "mean reward (100 episodes) 61.487181\n",
      "best mean reward 61.487181\n",
      "running time 928.000018\n",
      "Train_EnvstepsSoFar : 247001\n",
      "Train_AverageReturn : 61.48718131721099\n",
      "Train_BestReturn : 61.48718131721099\n",
      "TimeSinceStart : 928.0000183582306\n",
      "Training Loss : 0.08453916013240814\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 248001\n",
      "mean reward (100 episodes) 63.676488\n",
      "best mean reward 63.676488\n",
      "running time 931.220739\n",
      "Train_EnvstepsSoFar : 248001\n",
      "Train_AverageReturn : 63.67648837091842\n",
      "Train_BestReturn : 63.67648837091842\n",
      "TimeSinceStart : 931.2207391262054\n",
      "Training Loss : 0.05876891687512398\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 249001\n",
      "mean reward (100 episodes) 65.741886\n",
      "best mean reward 65.741886\n",
      "running time 935.221730\n",
      "Train_EnvstepsSoFar : 249001\n",
      "Train_AverageReturn : 65.74188625742292\n",
      "Train_BestReturn : 65.74188625742292\n",
      "TimeSinceStart : 935.2217302322388\n",
      "Training Loss : 0.042438749223947525\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 250001\n",
      "mean reward (100 episodes) 67.775780\n",
      "best mean reward 67.775780\n",
      "running time 938.593996\n",
      "Train_EnvstepsSoFar : 250001\n",
      "Train_AverageReturn : 67.77578039126263\n",
      "Train_BestReturn : 67.77578039126263\n",
      "TimeSinceStart : 938.5939960479736\n",
      "Training Loss : 0.07680036872625351\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 251001\n",
      "mean reward (100 episodes) 68.926783\n",
      "best mean reward 68.926783\n",
      "running time 941.462644\n",
      "Train_EnvstepsSoFar : 251001\n",
      "Train_AverageReturn : 68.92678274031361\n",
      "Train_BestReturn : 68.92678274031361\n",
      "TimeSinceStart : 941.4626443386078\n",
      "Training Loss : 2.5905470848083496\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 252001\n",
      "mean reward (100 episodes) 72.908932\n",
      "best mean reward 72.908932\n",
      "running time 945.372483\n",
      "Train_EnvstepsSoFar : 252001\n",
      "Train_AverageReturn : 72.9089324777116\n",
      "Train_BestReturn : 72.9089324777116\n",
      "TimeSinceStart : 945.372483253479\n",
      "Training Loss : 0.10382053256034851\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 253001\n",
      "mean reward (100 episodes) 77.393954\n",
      "best mean reward 77.393954\n",
      "running time 948.648715\n",
      "Train_EnvstepsSoFar : 253001\n",
      "Train_AverageReturn : 77.39395370012662\n",
      "Train_BestReturn : 77.39395370012662\n",
      "TimeSinceStart : 948.6487150192261\n",
      "Training Loss : 0.04904260113835335\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 254001\n",
      "mean reward (100 episodes) 80.575676\n",
      "best mean reward 80.575676\n",
      "running time 951.730585\n",
      "Train_EnvstepsSoFar : 254001\n",
      "Train_AverageReturn : 80.57567575047378\n",
      "Train_BestReturn : 80.57567575047378\n",
      "TimeSinceStart : 951.7305850982666\n",
      "Training Loss : 0.0870799645781517\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 255001\n",
      "mean reward (100 episodes) 79.318485\n",
      "best mean reward 80.575676\n",
      "running time 955.134279\n",
      "Train_EnvstepsSoFar : 255001\n",
      "Train_AverageReturn : 79.31848494604183\n",
      "Train_BestReturn : 80.57567575047378\n",
      "TimeSinceStart : 955.1342792510986\n",
      "Training Loss : 0.13006700575351715\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 256001\n",
      "mean reward (100 episodes) 78.891389\n",
      "best mean reward 80.575676\n",
      "running time 958.401015\n",
      "Train_EnvstepsSoFar : 256001\n",
      "Train_AverageReturn : 78.89138873603788\n",
      "Train_BestReturn : 80.57567575047378\n",
      "TimeSinceStart : 958.4010150432587\n",
      "Training Loss : 0.07901795953512192\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 257001\n",
      "mean reward (100 episodes) 79.702636\n",
      "best mean reward 80.575676\n",
      "running time 961.820169\n",
      "Train_EnvstepsSoFar : 257001\n",
      "Train_AverageReturn : 79.70263643804057\n",
      "Train_BestReturn : 80.57567575047378\n",
      "TimeSinceStart : 961.820169210434\n",
      "Training Loss : 0.25959140062332153\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 258001\n",
      "mean reward (100 episodes) 83.797262\n",
      "best mean reward 83.797262\n",
      "running time 965.128917\n",
      "Train_EnvstepsSoFar : 258001\n",
      "Train_AverageReturn : 83.79726224705593\n",
      "Train_BestReturn : 83.79726224705593\n",
      "TimeSinceStart : 965.1289169788361\n",
      "Training Loss : 0.5544326901435852\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 259001\n",
      "mean reward (100 episodes) 89.582175\n",
      "best mean reward 89.582175\n",
      "running time 968.081500\n",
      "Train_EnvstepsSoFar : 259001\n",
      "Train_AverageReturn : 89.58217534437873\n",
      "Train_BestReturn : 89.58217534437873\n",
      "TimeSinceStart : 968.0815002918243\n",
      "Training Loss : 0.1016726940870285\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 260001\n",
      "mean reward (100 episodes) 89.929727\n",
      "best mean reward 89.929727\n",
      "running time 971.171367\n",
      "Train_EnvstepsSoFar : 260001\n",
      "Train_AverageReturn : 89.92972742048683\n",
      "Train_BestReturn : 89.92972742048683\n",
      "TimeSinceStart : 971.1713671684265\n",
      "Training Loss : 0.06404811888933182\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 261001\n",
      "mean reward (100 episodes) 94.527885\n",
      "best mean reward 94.527885\n",
      "running time 974.135288\n",
      "Train_EnvstepsSoFar : 261001\n",
      "Train_AverageReturn : 94.52788542699534\n",
      "Train_BestReturn : 94.52788542699534\n",
      "TimeSinceStart : 974.1352882385254\n",
      "Training Loss : 3.1529150009155273\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 262001\n",
      "mean reward (100 episodes) 96.320584\n",
      "best mean reward 96.320584\n",
      "running time 977.825433\n",
      "Train_EnvstepsSoFar : 262001\n",
      "Train_AverageReturn : 96.32058405779026\n",
      "Train_BestReturn : 96.32058405779026\n",
      "TimeSinceStart : 977.825433254242\n",
      "Training Loss : 0.10874916613101959\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 263001\n",
      "mean reward (100 episodes) 102.847222\n",
      "best mean reward 102.847222\n",
      "running time 980.978243\n",
      "Train_EnvstepsSoFar : 263001\n",
      "Train_AverageReturn : 102.84722221570611\n",
      "Train_BestReturn : 102.84722221570611\n",
      "TimeSinceStart : 980.9782431125641\n",
      "Training Loss : 0.06597872078418732\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 264001\n",
      "mean reward (100 episodes) 104.453179\n",
      "best mean reward 104.453179\n",
      "running time 984.211335\n",
      "Train_EnvstepsSoFar : 264001\n",
      "Train_AverageReturn : 104.45317871213457\n",
      "Train_BestReturn : 104.45317871213457\n",
      "TimeSinceStart : 984.2113351821899\n",
      "Training Loss : 0.053384821861982346\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 265001\n",
      "mean reward (100 episodes) 105.352656\n",
      "best mean reward 105.352656\n",
      "running time 987.048351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 265001\n",
      "Train_AverageReturn : 105.35265567400191\n",
      "Train_BestReturn : 105.35265567400191\n",
      "TimeSinceStart : 987.0483512878418\n",
      "Training Loss : 2.775606155395508\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 266001\n",
      "mean reward (100 episodes) 106.621388\n",
      "best mean reward 106.621388\n",
      "running time 990.399630\n",
      "Train_EnvstepsSoFar : 266001\n",
      "Train_AverageReturn : 106.62138814043428\n",
      "Train_BestReturn : 106.62138814043428\n",
      "TimeSinceStart : 990.3996300697327\n",
      "Training Loss : 0.08931712061166763\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 267001\n",
      "mean reward (100 episodes) 108.007266\n",
      "best mean reward 108.007266\n",
      "running time 994.153111\n",
      "Train_EnvstepsSoFar : 267001\n",
      "Train_AverageReturn : 108.00726625841506\n",
      "Train_BestReturn : 108.00726625841506\n",
      "TimeSinceStart : 994.1531112194061\n",
      "Training Loss : 1.050549030303955\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 268001\n",
      "mean reward (100 episodes) 112.009569\n",
      "best mean reward 112.009569\n",
      "running time 996.992990\n",
      "Train_EnvstepsSoFar : 268001\n",
      "Train_AverageReturn : 112.00956856799752\n",
      "Train_BestReturn : 112.00956856799752\n",
      "TimeSinceStart : 996.9929902553558\n",
      "Training Loss : 0.09729114174842834\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 269001\n",
      "mean reward (100 episodes) 117.121100\n",
      "best mean reward 117.121100\n",
      "running time 999.848171\n",
      "Train_EnvstepsSoFar : 269001\n",
      "Train_AverageReturn : 117.12110008612531\n",
      "Train_BestReturn : 117.12110008612531\n",
      "TimeSinceStart : 999.8481709957123\n",
      "Training Loss : 1.3673211336135864\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 270001\n",
      "mean reward (100 episodes) 121.862728\n",
      "best mean reward 121.862728\n",
      "running time 1002.642842\n",
      "Train_EnvstepsSoFar : 270001\n",
      "Train_AverageReturn : 121.86272790410781\n",
      "Train_BestReturn : 121.86272790410781\n",
      "TimeSinceStart : 1002.6428422927856\n",
      "Training Loss : 0.05396565422415733\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 271001\n",
      "mean reward (100 episodes) 126.257196\n",
      "best mean reward 126.257196\n",
      "running time 1005.575816\n",
      "Train_EnvstepsSoFar : 271001\n",
      "Train_AverageReturn : 126.25719559197276\n",
      "Train_BestReturn : 126.25719559197276\n",
      "TimeSinceStart : 1005.57581615448\n",
      "Training Loss : 2.7744767665863037\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 272001\n",
      "mean reward (100 episodes) 125.345861\n",
      "best mean reward 126.257196\n",
      "running time 1008.921560\n",
      "Train_EnvstepsSoFar : 272001\n",
      "Train_AverageReturn : 125.34586077508906\n",
      "Train_BestReturn : 126.25719559197276\n",
      "TimeSinceStart : 1008.921560049057\n",
      "Training Loss : 0.1559077352285385\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 273001\n",
      "mean reward (100 episodes) 127.734460\n",
      "best mean reward 127.734460\n",
      "running time 1012.202556\n",
      "Train_EnvstepsSoFar : 273001\n",
      "Train_AverageReturn : 127.7344603070207\n",
      "Train_BestReturn : 127.7344603070207\n",
      "TimeSinceStart : 1012.2025561332703\n",
      "Training Loss : 2.8032686710357666\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 274001\n",
      "mean reward (100 episodes) 128.290783\n",
      "best mean reward 128.290783\n",
      "running time 1015.035151\n",
      "Train_EnvstepsSoFar : 274001\n",
      "Train_AverageReturn : 128.29078283437718\n",
      "Train_BestReturn : 128.29078283437718\n",
      "TimeSinceStart : 1015.0351512432098\n",
      "Training Loss : 0.9440613389015198\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 275001\n",
      "mean reward (100 episodes) 128.677556\n",
      "best mean reward 128.677556\n",
      "running time 1017.887904\n",
      "Train_EnvstepsSoFar : 275001\n",
      "Train_AverageReturn : 128.67755561043796\n",
      "Train_BestReturn : 128.67755561043796\n",
      "TimeSinceStart : 1017.8879041671753\n",
      "Training Loss : 2.4942145347595215\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 276001\n",
      "mean reward (100 episodes) 132.645824\n",
      "best mean reward 132.645824\n",
      "running time 1020.814081\n",
      "Train_EnvstepsSoFar : 276001\n",
      "Train_AverageReturn : 132.64582434479016\n",
      "Train_BestReturn : 132.64582434479016\n",
      "TimeSinceStart : 1020.8140811920166\n",
      "Training Loss : 0.11223479360342026\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 277001\n",
      "mean reward (100 episodes) 130.514023\n",
      "best mean reward 132.645824\n",
      "running time 1024.421696\n",
      "Train_EnvstepsSoFar : 277001\n",
      "Train_AverageReturn : 130.51402316599317\n",
      "Train_BestReturn : 132.64582434479016\n",
      "TimeSinceStart : 1024.4216961860657\n",
      "Training Loss : 0.047146979719400406\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 278001\n",
      "mean reward (100 episodes) 131.928372\n",
      "best mean reward 132.645824\n",
      "running time 1027.201213\n",
      "Train_EnvstepsSoFar : 278001\n",
      "Train_AverageReturn : 131.92837230666103\n",
      "Train_BestReturn : 132.64582434479016\n",
      "TimeSinceStart : 1027.2012133598328\n",
      "Training Loss : 0.043107494711875916\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 279001\n",
      "mean reward (100 episodes) 130.382550\n",
      "best mean reward 132.645824\n",
      "running time 1030.169564\n",
      "Train_EnvstepsSoFar : 279001\n",
      "Train_AverageReturn : 130.38255047100583\n",
      "Train_BestReturn : 132.64582434479016\n",
      "TimeSinceStart : 1030.1695642471313\n",
      "Training Loss : 0.07961548119783401\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 280001\n",
      "mean reward (100 episodes) 130.848335\n",
      "best mean reward 132.645824\n",
      "running time 1033.196665\n",
      "Train_EnvstepsSoFar : 280001\n",
      "Train_AverageReturn : 130.84833490800145\n",
      "Train_BestReturn : 132.64582434479016\n",
      "TimeSinceStart : 1033.1966650485992\n",
      "Training Loss : 0.7289515733718872\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 281001\n",
      "mean reward (100 episodes) 133.293710\n",
      "best mean reward 133.293710\n",
      "running time 1035.933483\n",
      "Train_EnvstepsSoFar : 281001\n",
      "Train_AverageReturn : 133.29371039292883\n",
      "Train_BestReturn : 133.29371039292883\n",
      "TimeSinceStart : 1035.9334831237793\n",
      "Training Loss : 0.7780290842056274\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 282001\n",
      "mean reward (100 episodes) 137.212653\n",
      "best mean reward 137.212653\n",
      "running time 1038.696934\n",
      "Train_EnvstepsSoFar : 282001\n",
      "Train_AverageReturn : 137.2126534221349\n",
      "Train_BestReturn : 137.2126534221349\n",
      "TimeSinceStart : 1038.696934223175\n",
      "Training Loss : 0.9480693936347961\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 283001\n",
      "mean reward (100 episodes) 137.491155\n",
      "best mean reward 137.491155\n",
      "running time 1042.136483\n",
      "Train_EnvstepsSoFar : 283001\n",
      "Train_AverageReturn : 137.4911552694433\n",
      "Train_BestReturn : 137.4911552694433\n",
      "TimeSinceStart : 1042.1364831924438\n",
      "Training Loss : 0.1639576405286789\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 284001\n",
      "mean reward (100 episodes) 138.997581\n",
      "best mean reward 138.997581\n",
      "running time 1045.907363\n",
      "Train_EnvstepsSoFar : 284001\n",
      "Train_AverageReturn : 138.99758083029354\n",
      "Train_BestReturn : 138.99758083029354\n",
      "TimeSinceStart : 1045.9073631763458\n",
      "Training Loss : 0.16846197843551636\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 285001\n",
      "mean reward (100 episodes) 139.530413\n",
      "best mean reward 139.530413\n",
      "running time 1049.130536\n",
      "Train_EnvstepsSoFar : 285001\n",
      "Train_AverageReturn : 139.5304127016018\n",
      "Train_BestReturn : 139.5304127016018\n",
      "TimeSinceStart : 1049.1305363178253\n",
      "Training Loss : 2.530052423477173\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 286001\n",
      "mean reward (100 episodes) 141.039156\n",
      "best mean reward 141.039156\n",
      "running time 1052.371953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 286001\n",
      "Train_AverageReturn : 141.03915597003052\n",
      "Train_BestReturn : 141.03915597003052\n",
      "TimeSinceStart : 1052.3719532489777\n",
      "Training Loss : 1.0230305194854736\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 287001\n",
      "mean reward (100 episodes) 143.281603\n",
      "best mean reward 143.281603\n",
      "running time 1055.346989\n",
      "Train_EnvstepsSoFar : 287001\n",
      "Train_AverageReturn : 143.28160273306779\n",
      "Train_BestReturn : 143.28160273306779\n",
      "TimeSinceStart : 1055.3469891548157\n",
      "Training Loss : 0.19561129808425903\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 288001\n",
      "mean reward (100 episodes) 145.279422\n",
      "best mean reward 145.279422\n",
      "running time 1058.609653\n",
      "Train_EnvstepsSoFar : 288001\n",
      "Train_AverageReturn : 145.27942162841657\n",
      "Train_BestReturn : 145.27942162841657\n",
      "TimeSinceStart : 1058.6096529960632\n",
      "Training Loss : 0.06238973140716553\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 289001\n",
      "mean reward (100 episodes) 145.982272\n",
      "best mean reward 145.982272\n",
      "running time 1061.470522\n",
      "Train_EnvstepsSoFar : 289001\n",
      "Train_AverageReturn : 145.98227215657593\n",
      "Train_BestReturn : 145.98227215657593\n",
      "TimeSinceStart : 1061.4705221652985\n",
      "Training Loss : 0.6993870735168457\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 290001\n",
      "mean reward (100 episodes) 146.168426\n",
      "best mean reward 146.168426\n",
      "running time 1064.224910\n",
      "Train_EnvstepsSoFar : 290001\n",
      "Train_AverageReturn : 146.16842616946943\n",
      "Train_BestReturn : 146.16842616946943\n",
      "TimeSinceStart : 1064.2249102592468\n",
      "Training Loss : 0.44595637917518616\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 291001\n",
      "mean reward (100 episodes) 145.640777\n",
      "best mean reward 146.168426\n",
      "running time 1067.518309\n",
      "Train_EnvstepsSoFar : 291001\n",
      "Train_AverageReturn : 145.64077689874844\n",
      "Train_BestReturn : 146.16842616946943\n",
      "TimeSinceStart : 1067.5183091163635\n",
      "Training Loss : 3.0303895473480225\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 292001\n",
      "mean reward (100 episodes) 146.464536\n",
      "best mean reward 146.464536\n",
      "running time 1070.434355\n",
      "Train_EnvstepsSoFar : 292001\n",
      "Train_AverageReturn : 146.46453621249415\n",
      "Train_BestReturn : 146.46453621249415\n",
      "TimeSinceStart : 1070.4343552589417\n",
      "Training Loss : 0.07227860391139984\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 293001\n",
      "mean reward (100 episodes) 148.464973\n",
      "best mean reward 148.464973\n",
      "running time 1073.603906\n",
      "Train_EnvstepsSoFar : 293001\n",
      "Train_AverageReturn : 148.46497326390954\n",
      "Train_BestReturn : 148.46497326390954\n",
      "TimeSinceStart : 1073.6039061546326\n",
      "Training Loss : 2.847580909729004\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 294001\n",
      "mean reward (100 episodes) 149.951448\n",
      "best mean reward 149.951448\n",
      "running time 1076.491231\n",
      "Train_EnvstepsSoFar : 294001\n",
      "Train_AverageReturn : 149.95144847813927\n",
      "Train_BestReturn : 149.95144847813927\n",
      "TimeSinceStart : 1076.4912312030792\n",
      "Training Loss : 2.264782428741455\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 295001\n",
      "mean reward (100 episodes) 150.853363\n",
      "best mean reward 150.853363\n",
      "running time 1079.418917\n",
      "Train_EnvstepsSoFar : 295001\n",
      "Train_AverageReturn : 150.85336271984056\n",
      "Train_BestReturn : 150.85336271984056\n",
      "TimeSinceStart : 1079.4189171791077\n",
      "Training Loss : 0.15187719464302063\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 296001\n",
      "mean reward (100 episodes) 147.948311\n",
      "best mean reward 150.853363\n",
      "running time 1082.245542\n",
      "Train_EnvstepsSoFar : 296001\n",
      "Train_AverageReturn : 147.94831069126832\n",
      "Train_BestReturn : 150.85336271984056\n",
      "TimeSinceStart : 1082.2455422878265\n",
      "Training Loss : 3.313371181488037\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 297001\n",
      "mean reward (100 episodes) 146.331291\n",
      "best mean reward 150.853363\n",
      "running time 1085.502236\n",
      "Train_EnvstepsSoFar : 297001\n",
      "Train_AverageReturn : 146.33129138414822\n",
      "Train_BestReturn : 150.85336271984056\n",
      "TimeSinceStart : 1085.5022361278534\n",
      "Training Loss : 0.36876195669174194\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 298001\n",
      "mean reward (100 episodes) 149.781093\n",
      "best mean reward 150.853363\n",
      "running time 1088.323136\n",
      "Train_EnvstepsSoFar : 298001\n",
      "Train_AverageReturn : 149.78109275855144\n",
      "Train_BestReturn : 150.85336271984056\n",
      "TimeSinceStart : 1088.3231360912323\n",
      "Training Loss : 0.1359443962574005\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 299001\n",
      "mean reward (100 episodes) 148.375967\n",
      "best mean reward 150.853363\n",
      "running time 1092.184707\n",
      "Train_EnvstepsSoFar : 299001\n",
      "Train_AverageReturn : 148.375967161851\n",
      "Train_BestReturn : 150.85336271984056\n",
      "TimeSinceStart : 1092.1847071647644\n",
      "Training Loss : 0.19211672246456146\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 300001\n",
      "mean reward (100 episodes) 150.350372\n",
      "best mean reward 150.853363\n",
      "running time 1094.962758\n",
      "Train_EnvstepsSoFar : 300001\n",
      "Train_AverageReturn : 150.35037198380869\n",
      "Train_BestReturn : 150.85336271984056\n",
      "TimeSinceStart : 1094.9627583026886\n",
      "Training Loss : 0.07878611981868744\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 301001\n",
      "mean reward (100 episodes) 151.833167\n",
      "best mean reward 151.833167\n",
      "running time 1097.925050\n",
      "Train_EnvstepsSoFar : 301001\n",
      "Train_AverageReturn : 151.83316675451607\n",
      "Train_BestReturn : 151.83316675451607\n",
      "TimeSinceStart : 1097.9250502586365\n",
      "Training Loss : 4.599884986877441\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 302001\n",
      "mean reward (100 episodes) 152.286242\n",
      "best mean reward 152.286242\n",
      "running time 1100.867298\n",
      "Train_EnvstepsSoFar : 302001\n",
      "Train_AverageReturn : 152.2862416558111\n",
      "Train_BestReturn : 152.2862416558111\n",
      "TimeSinceStart : 1100.8672981262207\n",
      "Training Loss : 0.13401874899864197\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 303001\n",
      "mean reward (100 episodes) 154.280998\n",
      "best mean reward 154.280998\n",
      "running time 1103.733258\n",
      "Train_EnvstepsSoFar : 303001\n",
      "Train_AverageReturn : 154.28099773601443\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1103.7332582473755\n",
      "Training Loss : 4.597270965576172\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 304001\n",
      "mean reward (100 episodes) 150.434849\n",
      "best mean reward 154.280998\n",
      "running time 1106.529012\n",
      "Train_EnvstepsSoFar : 304001\n",
      "Train_AverageReturn : 150.43484942966887\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1106.5290122032166\n",
      "Training Loss : 0.06179202347993851\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 305001\n",
      "mean reward (100 episodes) 151.761568\n",
      "best mean reward 154.280998\n",
      "running time 1109.381574\n",
      "Train_EnvstepsSoFar : 305001\n",
      "Train_AverageReturn : 151.76156812466942\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1109.3815741539001\n",
      "Training Loss : 0.06970034539699554\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 306001\n",
      "mean reward (100 episodes) 148.968691\n",
      "best mean reward 154.280998\n",
      "running time 1112.186256\n",
      "Train_EnvstepsSoFar : 306001\n",
      "Train_AverageReturn : 148.9686912867473\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1112.1862561702728\n",
      "Training Loss : 0.29957595467567444\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 307001\n",
      "mean reward (100 episodes) 145.272343\n",
      "best mean reward 154.280998\n",
      "running time 1114.961461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 307001\n",
      "Train_AverageReturn : 145.27234260710148\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1114.9614610671997\n",
      "Training Loss : 0.9248181581497192\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 308001\n",
      "mean reward (100 episodes) 145.270862\n",
      "best mean reward 154.280998\n",
      "running time 1117.816237\n",
      "Train_EnvstepsSoFar : 308001\n",
      "Train_AverageReturn : 145.27086176904487\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1117.8162372112274\n",
      "Training Loss : 0.073231540620327\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 309001\n",
      "mean reward (100 episodes) 141.675181\n",
      "best mean reward 154.280998\n",
      "running time 1120.603170\n",
      "Train_EnvstepsSoFar : 309001\n",
      "Train_AverageReturn : 141.6751809860166\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1120.6031701564789\n",
      "Training Loss : 0.10196651518344879\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 310001\n",
      "mean reward (100 episodes) 131.776794\n",
      "best mean reward 154.280998\n",
      "running time 1123.416983\n",
      "Train_EnvstepsSoFar : 310001\n",
      "Train_AverageReturn : 131.77679366588077\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1123.416983127594\n",
      "Training Loss : 2.1634228229522705\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 311001\n",
      "mean reward (100 episodes) 130.317963\n",
      "best mean reward 154.280998\n",
      "running time 1126.281435\n",
      "Train_EnvstepsSoFar : 311001\n",
      "Train_AverageReturn : 130.31796300835708\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1126.281435251236\n",
      "Training Loss : 0.12684720754623413\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 312001\n",
      "mean reward (100 episodes) 132.516909\n",
      "best mean reward 154.280998\n",
      "running time 1129.175470\n",
      "Train_EnvstepsSoFar : 312001\n",
      "Train_AverageReturn : 132.5169093251633\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1129.1754701137543\n",
      "Training Loss : 1.5339702367782593\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 313001\n",
      "mean reward (100 episodes) 130.296564\n",
      "best mean reward 154.280998\n",
      "running time 1132.004047\n",
      "Train_EnvstepsSoFar : 313001\n",
      "Train_AverageReturn : 130.2965640090878\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1132.0040471553802\n",
      "Training Loss : 0.143645778298378\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 314001\n",
      "mean reward (100 episodes) 131.130985\n",
      "best mean reward 154.280998\n",
      "running time 1134.802188\n",
      "Train_EnvstepsSoFar : 314001\n",
      "Train_AverageReturn : 131.1309851167391\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1134.8021881580353\n",
      "Training Loss : 0.2105272263288498\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 315001\n",
      "mean reward (100 episodes) 121.619792\n",
      "best mean reward 154.280998\n",
      "running time 1137.803571\n",
      "Train_EnvstepsSoFar : 315001\n",
      "Train_AverageReturn : 121.61979158140606\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1137.8035712242126\n",
      "Training Loss : 2.3816263675689697\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 316001\n",
      "mean reward (100 episodes) 121.488100\n",
      "best mean reward 154.280998\n",
      "running time 1140.510125\n",
      "Train_EnvstepsSoFar : 316001\n",
      "Train_AverageReturn : 121.4881002476362\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1140.5101251602173\n",
      "Training Loss : 0.06818763166666031\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 317001\n",
      "mean reward (100 episodes) 120.920740\n",
      "best mean reward 154.280998\n",
      "running time 1143.583318\n",
      "Train_EnvstepsSoFar : 317001\n",
      "Train_AverageReturn : 120.92074000799909\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1143.5833179950714\n",
      "Training Loss : 0.14146125316619873\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 318001\n",
      "mean reward (100 episodes) 122.566225\n",
      "best mean reward 154.280998\n",
      "running time 1146.485704\n",
      "Train_EnvstepsSoFar : 318001\n",
      "Train_AverageReturn : 122.56622468993983\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1146.4857041835785\n",
      "Training Loss : 2.342111349105835\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 319001\n",
      "mean reward (100 episodes) 124.003848\n",
      "best mean reward 154.280998\n",
      "running time 1149.337152\n",
      "Train_EnvstepsSoFar : 319001\n",
      "Train_AverageReturn : 124.00384824307976\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1149.337152004242\n",
      "Training Loss : 3.7508201599121094\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 320001\n",
      "mean reward (100 episodes) 127.890363\n",
      "best mean reward 154.280998\n",
      "running time 1152.167416\n",
      "Train_EnvstepsSoFar : 320001\n",
      "Train_AverageReturn : 127.89036322987887\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1152.1674160957336\n",
      "Training Loss : 2.8433163166046143\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 321001\n",
      "mean reward (100 episodes) 118.876841\n",
      "best mean reward 154.280998\n",
      "running time 1154.952368\n",
      "Train_EnvstepsSoFar : 321001\n",
      "Train_AverageReturn : 118.87684103866951\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1154.95236825943\n",
      "Training Loss : 1.6364012956619263\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 322001\n",
      "mean reward (100 episodes) 118.597018\n",
      "best mean reward 154.280998\n",
      "running time 1157.771609\n",
      "Train_EnvstepsSoFar : 322001\n",
      "Train_AverageReturn : 118.59701790219515\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1157.7716090679169\n",
      "Training Loss : 0.2809820771217346\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 323001\n",
      "mean reward (100 episodes) 118.094555\n",
      "best mean reward 154.280998\n",
      "running time 1160.662158\n",
      "Train_EnvstepsSoFar : 323001\n",
      "Train_AverageReturn : 118.0945549299157\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1160.6621582508087\n",
      "Training Loss : 5.615053176879883\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 324001\n",
      "mean reward (100 episodes) 116.864672\n",
      "best mean reward 154.280998\n",
      "running time 1163.371914\n",
      "Train_EnvstepsSoFar : 324001\n",
      "Train_AverageReturn : 116.86467238116533\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1163.3719143867493\n",
      "Training Loss : 5.017994403839111\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 325001\n",
      "mean reward (100 episodes) 119.127653\n",
      "best mean reward 154.280998\n",
      "running time 1166.251147\n",
      "Train_EnvstepsSoFar : 325001\n",
      "Train_AverageReturn : 119.12765329244353\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1166.251147031784\n",
      "Training Loss : 0.1336081176996231\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 326001\n",
      "mean reward (100 episodes) 117.310565\n",
      "best mean reward 154.280998\n",
      "running time 1169.043368\n",
      "Train_EnvstepsSoFar : 326001\n",
      "Train_AverageReturn : 117.31056469571986\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1169.0433683395386\n",
      "Training Loss : 2.9856948852539062\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 327001\n",
      "mean reward (100 episodes) 119.260637\n",
      "best mean reward 154.280998\n",
      "running time 1172.590512\n",
      "Train_EnvstepsSoFar : 327001\n",
      "Train_AverageReturn : 119.260637467505\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1172.5905122756958\n",
      "Training Loss : 2.2254745960235596\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 328001\n",
      "mean reward (100 episodes) 117.480481\n",
      "best mean reward 154.280998\n",
      "running time 1175.905245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 328001\n",
      "Train_AverageReturn : 117.48048108313962\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1175.9052453041077\n",
      "Training Loss : 0.07791432738304138\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 329001\n",
      "mean reward (100 episodes) 112.745800\n",
      "best mean reward 154.280998\n",
      "running time 1178.629293\n",
      "Train_EnvstepsSoFar : 329001\n",
      "Train_AverageReturn : 112.74579967472475\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1178.6292932033539\n",
      "Training Loss : 0.10193532705307007\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 330001\n",
      "mean reward (100 episodes) 116.893294\n",
      "best mean reward 154.280998\n",
      "running time 1181.314324\n",
      "Train_EnvstepsSoFar : 330001\n",
      "Train_AverageReturn : 116.89329382201629\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1181.3143243789673\n",
      "Training Loss : 2.7822487354278564\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 331001\n",
      "mean reward (100 episodes) 116.431643\n",
      "best mean reward 154.280998\n",
      "running time 1184.895033\n",
      "Train_EnvstepsSoFar : 331001\n",
      "Train_AverageReturn : 116.43164330380631\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1184.895033121109\n",
      "Training Loss : 13.43104362487793\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 332001\n",
      "mean reward (100 episodes) 113.870235\n",
      "best mean reward 154.280998\n",
      "running time 1187.962782\n",
      "Train_EnvstepsSoFar : 332001\n",
      "Train_AverageReturn : 113.87023532158166\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1187.962782382965\n",
      "Training Loss : 3.520481824874878\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 333001\n",
      "mean reward (100 episodes) 115.595650\n",
      "best mean reward 154.280998\n",
      "running time 1190.964394\n",
      "Train_EnvstepsSoFar : 333001\n",
      "Train_AverageReturn : 115.59565046971252\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1190.9643940925598\n",
      "Training Loss : 0.11145920306444168\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 334001\n",
      "mean reward (100 episodes) 121.306713\n",
      "best mean reward 154.280998\n",
      "running time 1194.113799\n",
      "Train_EnvstepsSoFar : 334001\n",
      "Train_AverageReturn : 121.30671300596768\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1194.1137993335724\n",
      "Training Loss : 0.12144728749990463\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 335001\n",
      "mean reward (100 episodes) 121.619726\n",
      "best mean reward 154.280998\n",
      "running time 1196.828967\n",
      "Train_EnvstepsSoFar : 335001\n",
      "Train_AverageReturn : 121.61972606877791\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1196.82896733284\n",
      "Training Loss : 2.6989200115203857\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 336001\n",
      "mean reward (100 episodes) 120.582602\n",
      "best mean reward 154.280998\n",
      "running time 1199.983207\n",
      "Train_EnvstepsSoFar : 336001\n",
      "Train_AverageReturn : 120.58260168016494\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1199.9832072257996\n",
      "Training Loss : 0.8858622908592224\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 337001\n",
      "mean reward (100 episodes) 119.852821\n",
      "best mean reward 154.280998\n",
      "running time 1203.551462\n",
      "Train_EnvstepsSoFar : 337001\n",
      "Train_AverageReturn : 119.85282081194256\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1203.551462173462\n",
      "Training Loss : 0.12667903304100037\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 338001\n",
      "mean reward (100 episodes) 119.783001\n",
      "best mean reward 154.280998\n",
      "running time 1206.164201\n",
      "Train_EnvstepsSoFar : 338001\n",
      "Train_AverageReturn : 119.78300108417386\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1206.164201259613\n",
      "Training Loss : 0.08997231721878052\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 339001\n",
      "mean reward (100 episodes) 128.141448\n",
      "best mean reward 154.280998\n",
      "running time 1209.163045\n",
      "Train_EnvstepsSoFar : 339001\n",
      "Train_AverageReturn : 128.14144818013702\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1209.163045167923\n",
      "Training Loss : 1.9897314310073853\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 340001\n",
      "mean reward (100 episodes) 131.440630\n",
      "best mean reward 154.280998\n",
      "running time 1211.817996\n",
      "Train_EnvstepsSoFar : 340001\n",
      "Train_AverageReturn : 131.44063038517103\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1211.8179960250854\n",
      "Training Loss : 2.771306037902832\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 341001\n",
      "mean reward (100 episodes) 132.548504\n",
      "best mean reward 154.280998\n",
      "running time 1214.837082\n",
      "Train_EnvstepsSoFar : 341001\n",
      "Train_AverageReturn : 132.54850448820332\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1214.8370821475983\n",
      "Training Loss : 5.26970100402832\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 342001\n",
      "mean reward (100 episodes) 129.622100\n",
      "best mean reward 154.280998\n",
      "running time 1218.178501\n",
      "Train_EnvstepsSoFar : 342001\n",
      "Train_AverageReturn : 129.6221000331934\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1218.1785011291504\n",
      "Training Loss : 0.05299503356218338\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 343001\n",
      "mean reward (100 episodes) 130.868362\n",
      "best mean reward 154.280998\n",
      "running time 1221.050584\n",
      "Train_EnvstepsSoFar : 343001\n",
      "Train_AverageReturn : 130.86836230084032\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1221.050584077835\n",
      "Training Loss : 0.5493685007095337\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 344001\n",
      "mean reward (100 episodes) 130.951161\n",
      "best mean reward 154.280998\n",
      "running time 1223.685005\n",
      "Train_EnvstepsSoFar : 344001\n",
      "Train_AverageReturn : 130.95116113465227\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1223.6850051879883\n",
      "Training Loss : 0.11824939399957657\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 345001\n",
      "mean reward (100 episodes) 136.697845\n",
      "best mean reward 154.280998\n",
      "running time 1226.482149\n",
      "Train_EnvstepsSoFar : 345001\n",
      "Train_AverageReturn : 136.6978450105311\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1226.4821491241455\n",
      "Training Loss : 0.2084919512271881\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 346001\n",
      "mean reward (100 episodes) 139.825149\n",
      "best mean reward 154.280998\n",
      "running time 1229.167961\n",
      "Train_EnvstepsSoFar : 346001\n",
      "Train_AverageReturn : 139.8251493901375\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1229.1679611206055\n",
      "Training Loss : 0.07406827062368393\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 347001\n",
      "mean reward (100 episodes) 134.504098\n",
      "best mean reward 154.280998\n",
      "running time 1232.298513\n",
      "Train_EnvstepsSoFar : 347001\n",
      "Train_AverageReturn : 134.5040979113545\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1232.298513174057\n",
      "Training Loss : 0.060584861785173416\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 348001\n",
      "mean reward (100 episodes) 124.711490\n",
      "best mean reward 154.280998\n",
      "running time 1235.258695\n",
      "Train_EnvstepsSoFar : 348001\n",
      "Train_AverageReturn : 124.71148979814116\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1235.2586951255798\n",
      "Training Loss : 0.1151353269815445\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 349001\n",
      "mean reward (100 episodes) 123.654982\n",
      "best mean reward 154.280998\n",
      "running time 1237.997854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 349001\n",
      "Train_AverageReturn : 123.65498163827974\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1237.997854232788\n",
      "Training Loss : 0.4157940745353699\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 350001\n",
      "mean reward (100 episodes) 121.685108\n",
      "best mean reward 154.280998\n",
      "running time 1240.955635\n",
      "Train_EnvstepsSoFar : 350001\n",
      "Train_AverageReturn : 121.6851080594648\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1240.9556353092194\n",
      "Training Loss : 0.20579469203948975\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 351001\n",
      "mean reward (100 episodes) 121.258097\n",
      "best mean reward 154.280998\n",
      "running time 1243.709301\n",
      "Train_EnvstepsSoFar : 351001\n",
      "Train_AverageReturn : 121.25809738313195\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1243.709300994873\n",
      "Training Loss : 5.090541839599609\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 352001\n",
      "mean reward (100 episodes) 125.500074\n",
      "best mean reward 154.280998\n",
      "running time 1246.401364\n",
      "Train_EnvstepsSoFar : 352001\n",
      "Train_AverageReturn : 125.50007370555585\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1246.401364326477\n",
      "Training Loss : 9.917229652404785\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 353001\n",
      "mean reward (100 episodes) 125.544914\n",
      "best mean reward 154.280998\n",
      "running time 1249.108226\n",
      "Train_EnvstepsSoFar : 353001\n",
      "Train_AverageReturn : 125.54491355787083\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1249.1082260608673\n",
      "Training Loss : 0.08705244213342667\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 354001\n",
      "mean reward (100 episodes) 129.523859\n",
      "best mean reward 154.280998\n",
      "running time 1251.885074\n",
      "Train_EnvstepsSoFar : 354001\n",
      "Train_AverageReturn : 129.523858861183\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1251.8850741386414\n",
      "Training Loss : 4.161108016967773\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 355001\n",
      "mean reward (100 episodes) 129.043802\n",
      "best mean reward 154.280998\n",
      "running time 1254.597150\n",
      "Train_EnvstepsSoFar : 355001\n",
      "Train_AverageReturn : 129.0438024632154\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1254.5971503257751\n",
      "Training Loss : 0.2299409806728363\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 356001\n",
      "mean reward (100 episodes) 125.128231\n",
      "best mean reward 154.280998\n",
      "running time 1257.356348\n",
      "Train_EnvstepsSoFar : 356001\n",
      "Train_AverageReturn : 125.12823142472119\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1257.3563482761383\n",
      "Training Loss : 0.5506467223167419\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 357001\n",
      "mean reward (100 episodes) 127.681505\n",
      "best mean reward 154.280998\n",
      "running time 1259.994493\n",
      "Train_EnvstepsSoFar : 357001\n",
      "Train_AverageReturn : 127.68150515398273\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1259.9944932460785\n",
      "Training Loss : 0.12246408313512802\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 358001\n",
      "mean reward (100 episodes) 128.697461\n",
      "best mean reward 154.280998\n",
      "running time 1262.672949\n",
      "Train_EnvstepsSoFar : 358001\n",
      "Train_AverageReturn : 128.69746137190162\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1262.6729490756989\n",
      "Training Loss : 0.2108205407857895\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 359001\n",
      "mean reward (100 episodes) 126.863083\n",
      "best mean reward 154.280998\n",
      "running time 1265.568427\n",
      "Train_EnvstepsSoFar : 359001\n",
      "Train_AverageReturn : 126.86308268277352\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1265.5684270858765\n",
      "Training Loss : 3.0872485637664795\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 360001\n",
      "mean reward (100 episodes) 131.392837\n",
      "best mean reward 154.280998\n",
      "running time 1268.218251\n",
      "Train_EnvstepsSoFar : 360001\n",
      "Train_AverageReturn : 131.3928370331834\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1268.218250989914\n",
      "Training Loss : 0.12688544392585754\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 361001\n",
      "mean reward (100 episodes) 132.300162\n",
      "best mean reward 154.280998\n",
      "running time 1270.920557\n",
      "Train_EnvstepsSoFar : 361001\n",
      "Train_AverageReturn : 132.30016160931638\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1270.9205570220947\n",
      "Training Loss : 2.3033549785614014\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 362001\n",
      "mean reward (100 episodes) 125.888848\n",
      "best mean reward 154.280998\n",
      "running time 1273.537171\n",
      "Train_EnvstepsSoFar : 362001\n",
      "Train_AverageReturn : 125.88884755924728\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1273.5371713638306\n",
      "Training Loss : 0.06957308202981949\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 363001\n",
      "mean reward (100 episodes) 121.989082\n",
      "best mean reward 154.280998\n",
      "running time 1276.203960\n",
      "Train_EnvstepsSoFar : 363001\n",
      "Train_AverageReturn : 121.98908151282174\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1276.2039601802826\n",
      "Training Loss : 0.2029758095741272\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 364001\n",
      "mean reward (100 episodes) 120.641137\n",
      "best mean reward 154.280998\n",
      "running time 1278.906608\n",
      "Train_EnvstepsSoFar : 364001\n",
      "Train_AverageReturn : 120.64113735184281\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1278.9066081047058\n",
      "Training Loss : 0.09259717911481857\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 365001\n",
      "mean reward (100 episodes) 125.786669\n",
      "best mean reward 154.280998\n",
      "running time 1281.644832\n",
      "Train_EnvstepsSoFar : 365001\n",
      "Train_AverageReturn : 125.78666868931163\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1281.6448321342468\n",
      "Training Loss : 0.3761083781719208\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 366001\n",
      "mean reward (100 episodes) 128.478162\n",
      "best mean reward 154.280998\n",
      "running time 1284.420457\n",
      "Train_EnvstepsSoFar : 366001\n",
      "Train_AverageReturn : 128.4781618038355\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1284.4204573631287\n",
      "Training Loss : 7.446108341217041\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 367001\n",
      "mean reward (100 episodes) 135.344713\n",
      "best mean reward 154.280998\n",
      "running time 1287.125861\n",
      "Train_EnvstepsSoFar : 367001\n",
      "Train_AverageReturn : 135.3447130406791\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1287.1258611679077\n",
      "Training Loss : 1.9811557531356812\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 368001\n",
      "mean reward (100 episodes) 136.072139\n",
      "best mean reward 154.280998\n",
      "running time 1290.010248\n",
      "Train_EnvstepsSoFar : 368001\n",
      "Train_AverageReturn : 136.07213872650743\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1290.010248184204\n",
      "Training Loss : 1.9408061504364014\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 369001\n",
      "mean reward (100 episodes) 135.994633\n",
      "best mean reward 154.280998\n",
      "running time 1292.979968\n",
      "Train_EnvstepsSoFar : 369001\n",
      "Train_AverageReturn : 135.99463272927878\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1292.979968070984\n",
      "Training Loss : 0.27863189578056335\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 370001\n",
      "mean reward (100 episodes) 134.690937\n",
      "best mean reward 154.280998\n",
      "running time 1296.121641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 370001\n",
      "Train_AverageReturn : 134.69093720742424\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1296.1216413974762\n",
      "Training Loss : 0.07017703354358673\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 371001\n",
      "mean reward (100 episodes) 132.093181\n",
      "best mean reward 154.280998\n",
      "running time 1299.475226\n",
      "Train_EnvstepsSoFar : 371001\n",
      "Train_AverageReturn : 132.09318083531412\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1299.4752261638641\n",
      "Training Loss : 0.17159268260002136\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 372001\n",
      "mean reward (100 episodes) 128.968215\n",
      "best mean reward 154.280998\n",
      "running time 1302.196106\n",
      "Train_EnvstepsSoFar : 372001\n",
      "Train_AverageReturn : 128.9682152260247\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1302.1961061954498\n",
      "Training Loss : 0.5718422532081604\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 373001\n",
      "mean reward (100 episodes) 128.859024\n",
      "best mean reward 154.280998\n",
      "running time 1305.052113\n",
      "Train_EnvstepsSoFar : 373001\n",
      "Train_AverageReturn : 128.85902433511205\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1305.0521132946014\n",
      "Training Loss : 1.8422998189926147\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 374001\n",
      "mean reward (100 episodes) 132.744343\n",
      "best mean reward 154.280998\n",
      "running time 1307.721262\n",
      "Train_EnvstepsSoFar : 374001\n",
      "Train_AverageReturn : 132.7443426203773\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1307.721262216568\n",
      "Training Loss : 1.0950485467910767\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 375001\n",
      "mean reward (100 episodes) 137.890084\n",
      "best mean reward 154.280998\n",
      "running time 1310.585516\n",
      "Train_EnvstepsSoFar : 375001\n",
      "Train_AverageReturn : 137.89008433381704\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1310.5855162143707\n",
      "Training Loss : 0.1311454027891159\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 376001\n",
      "mean reward (100 episodes) 140.410649\n",
      "best mean reward 154.280998\n",
      "running time 1313.511647\n",
      "Train_EnvstepsSoFar : 376001\n",
      "Train_AverageReturn : 140.41064868365385\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1313.5116472244263\n",
      "Training Loss : 0.1683400720357895\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 377001\n",
      "mean reward (100 episodes) 135.879986\n",
      "best mean reward 154.280998\n",
      "running time 1316.328395\n",
      "Train_EnvstepsSoFar : 377001\n",
      "Train_AverageReturn : 135.87998575707627\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1316.3283951282501\n",
      "Training Loss : 0.19774368405342102\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 378001\n",
      "mean reward (100 episodes) 137.935619\n",
      "best mean reward 154.280998\n",
      "running time 1319.111202\n",
      "Train_EnvstepsSoFar : 378001\n",
      "Train_AverageReturn : 137.93561859228427\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1319.1112020015717\n",
      "Training Loss : 0.25011730194091797\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 379001\n",
      "mean reward (100 episodes) 137.130685\n",
      "best mean reward 154.280998\n",
      "running time 1322.043125\n",
      "Train_EnvstepsSoFar : 379001\n",
      "Train_AverageReturn : 137.13068526699968\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1322.043125152588\n",
      "Training Loss : 2.8532021045684814\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 380001\n",
      "mean reward (100 episodes) 140.766302\n",
      "best mean reward 154.280998\n",
      "running time 1324.794072\n",
      "Train_EnvstepsSoFar : 380001\n",
      "Train_AverageReturn : 140.76630219887448\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1324.794072151184\n",
      "Training Loss : 0.5131736397743225\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 381001\n",
      "mean reward (100 episodes) 136.339281\n",
      "best mean reward 154.280998\n",
      "running time 1327.840813\n",
      "Train_EnvstepsSoFar : 381001\n",
      "Train_AverageReturn : 136.33928143165184\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1327.8408131599426\n",
      "Training Loss : 0.2697322368621826\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 382001\n",
      "mean reward (100 episodes) 135.746008\n",
      "best mean reward 154.280998\n",
      "running time 1330.594892\n",
      "Train_EnvstepsSoFar : 382001\n",
      "Train_AverageReturn : 135.74600833572748\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1330.594892024994\n",
      "Training Loss : 0.251365602016449\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 383001\n",
      "mean reward (100 episodes) 125.817728\n",
      "best mean reward 154.280998\n",
      "running time 1333.298309\n",
      "Train_EnvstepsSoFar : 383001\n",
      "Train_AverageReturn : 125.81772814115244\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1333.2983090877533\n",
      "Training Loss : 0.776423990726471\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 384001\n",
      "mean reward (100 episodes) 132.681562\n",
      "best mean reward 154.280998\n",
      "running time 1336.182850\n",
      "Train_EnvstepsSoFar : 384001\n",
      "Train_AverageReturn : 132.68156226951976\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1336.1828501224518\n",
      "Training Loss : 0.8405900001525879\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 385001\n",
      "mean reward (100 episodes) 132.392652\n",
      "best mean reward 154.280998\n",
      "running time 1339.703153\n",
      "Train_EnvstepsSoFar : 385001\n",
      "Train_AverageReturn : 132.3926523478427\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1339.7031531333923\n",
      "Training Loss : 0.4375244379043579\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 386001\n",
      "mean reward (100 episodes) 131.061635\n",
      "best mean reward 154.280998\n",
      "running time 1342.482404\n",
      "Train_EnvstepsSoFar : 386001\n",
      "Train_AverageReturn : 131.06163469185407\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1342.4824042320251\n",
      "Training Loss : 0.2965069115161896\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 387001\n",
      "mean reward (100 episodes) 129.592731\n",
      "best mean reward 154.280998\n",
      "running time 1346.275415\n",
      "Train_EnvstepsSoFar : 387001\n",
      "Train_AverageReturn : 129.59273075177995\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1346.2754151821136\n",
      "Training Loss : 0.17167003452777863\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 388001\n",
      "mean reward (100 episodes) 129.961808\n",
      "best mean reward 154.280998\n",
      "running time 1348.992842\n",
      "Train_EnvstepsSoFar : 388001\n",
      "Train_AverageReturn : 129.96180770944184\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1348.9928421974182\n",
      "Training Loss : 0.06974335014820099\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 389001\n",
      "mean reward (100 episodes) 128.687555\n",
      "best mean reward 154.280998\n",
      "running time 1351.883498\n",
      "Train_EnvstepsSoFar : 389001\n",
      "Train_AverageReturn : 128.68755494347602\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1351.883497953415\n",
      "Training Loss : 0.1455107480287552\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 390001\n",
      "mean reward (100 episodes) 130.844886\n",
      "best mean reward 154.280998\n",
      "running time 1354.686622\n",
      "Train_EnvstepsSoFar : 390001\n",
      "Train_AverageReturn : 130.84488593961945\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1354.6866223812103\n",
      "Training Loss : 1.69685697555542\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 391001\n",
      "mean reward (100 episodes) 131.839380\n",
      "best mean reward 154.280998\n",
      "running time 1357.469370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 391001\n",
      "Train_AverageReturn : 131.83938026148272\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1357.4693701267242\n",
      "Training Loss : 0.14985671639442444\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 392001\n",
      "mean reward (100 episodes) 131.281820\n",
      "best mean reward 154.280998\n",
      "running time 1360.150543\n",
      "Train_EnvstepsSoFar : 392001\n",
      "Train_AverageReturn : 131.28182001293806\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1360.1505432128906\n",
      "Training Loss : 0.21108773350715637\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 393001\n",
      "mean reward (100 episodes) 132.526317\n",
      "best mean reward 154.280998\n",
      "running time 1362.891650\n",
      "Train_EnvstepsSoFar : 393001\n",
      "Train_AverageReturn : 132.5263167659822\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1362.8916501998901\n",
      "Training Loss : 2.980391025543213\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 394001\n",
      "mean reward (100 episodes) 132.628398\n",
      "best mean reward 154.280998\n",
      "running time 1366.033125\n",
      "Train_EnvstepsSoFar : 394001\n",
      "Train_AverageReturn : 132.6283977589885\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1366.0331251621246\n",
      "Training Loss : 1.508829951286316\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 395001\n",
      "mean reward (100 episodes) 131.000358\n",
      "best mean reward 154.280998\n",
      "running time 1369.126496\n",
      "Train_EnvstepsSoFar : 395001\n",
      "Train_AverageReturn : 131.00035779517742\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1369.1264960765839\n",
      "Training Loss : 3.5109217166900635\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 396001\n",
      "mean reward (100 episodes) 130.608193\n",
      "best mean reward 154.280998\n",
      "running time 1371.928649\n",
      "Train_EnvstepsSoFar : 396001\n",
      "Train_AverageReturn : 130.6081927338098\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1371.928649187088\n",
      "Training Loss : 2.2178192138671875\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 397001\n",
      "mean reward (100 episodes) 125.016970\n",
      "best mean reward 154.280998\n",
      "running time 1375.702700\n",
      "Train_EnvstepsSoFar : 397001\n",
      "Train_AverageReturn : 125.01696952528236\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1375.702700138092\n",
      "Training Loss : 2.8151400089263916\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 398001\n",
      "mean reward (100 episodes) 126.932301\n",
      "best mean reward 154.280998\n",
      "running time 1378.542862\n",
      "Train_EnvstepsSoFar : 398001\n",
      "Train_AverageReturn : 126.93230135236969\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1378.5428621768951\n",
      "Training Loss : 1.8839975595474243\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 399001\n",
      "mean reward (100 episodes) 129.614278\n",
      "best mean reward 154.280998\n",
      "running time 1381.398662\n",
      "Train_EnvstepsSoFar : 399001\n",
      "Train_AverageReturn : 129.6142784685245\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1381.39866232872\n",
      "Training Loss : 0.21018502116203308\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 400001\n",
      "mean reward (100 episodes) 122.444014\n",
      "best mean reward 154.280998\n",
      "running time 1384.191763\n",
      "Train_EnvstepsSoFar : 400001\n",
      "Train_AverageReturn : 122.44401400113904\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1384.191763162613\n",
      "Training Loss : 1.2573158740997314\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 401001\n",
      "mean reward (100 episodes) 115.738680\n",
      "best mean reward 154.280998\n",
      "running time 1386.856042\n",
      "Train_EnvstepsSoFar : 401001\n",
      "Train_AverageReturn : 115.73868033642079\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1386.8560421466827\n",
      "Training Loss : 0.19476106762886047\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 402001\n",
      "mean reward (100 episodes) 118.973505\n",
      "best mean reward 154.280998\n",
      "running time 1389.695726\n",
      "Train_EnvstepsSoFar : 402001\n",
      "Train_AverageReturn : 118.97350496871495\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1389.6957261562347\n",
      "Training Loss : 0.6390755772590637\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 403001\n",
      "mean reward (100 episodes) 127.106110\n",
      "best mean reward 154.280998\n",
      "running time 1392.414040\n",
      "Train_EnvstepsSoFar : 403001\n",
      "Train_AverageReturn : 127.10610970974604\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1392.4140400886536\n",
      "Training Loss : 0.11699751019477844\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 404001\n",
      "mean reward (100 episodes) 139.124192\n",
      "best mean reward 154.280998\n",
      "running time 1395.175475\n",
      "Train_EnvstepsSoFar : 404001\n",
      "Train_AverageReturn : 139.12419236499673\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1395.175475358963\n",
      "Training Loss : 0.35973960161209106\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 405001\n",
      "mean reward (100 episodes) 135.201129\n",
      "best mean reward 154.280998\n",
      "running time 1397.871016\n",
      "Train_EnvstepsSoFar : 405001\n",
      "Train_AverageReturn : 135.20112925198208\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1397.8710162639618\n",
      "Training Loss : 0.14048299193382263\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 406001\n",
      "mean reward (100 episodes) 134.738402\n",
      "best mean reward 154.280998\n",
      "running time 1400.633335\n",
      "Train_EnvstepsSoFar : 406001\n",
      "Train_AverageReturn : 134.73840167548434\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1400.6333351135254\n",
      "Training Loss : 0.24776697158813477\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 407001\n",
      "mean reward (100 episodes) 135.257574\n",
      "best mean reward 154.280998\n",
      "running time 1403.555259\n",
      "Train_EnvstepsSoFar : 407001\n",
      "Train_AverageReturn : 135.2575737454613\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1403.5552592277527\n",
      "Training Loss : 0.45930805802345276\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 408001\n",
      "mean reward (100 episodes) 124.373289\n",
      "best mean reward 154.280998\n",
      "running time 1406.233444\n",
      "Train_EnvstepsSoFar : 408001\n",
      "Train_AverageReturn : 124.37328875259065\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1406.2334442138672\n",
      "Training Loss : 0.25394192337989807\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 409001\n",
      "mean reward (100 episodes) 125.628332\n",
      "best mean reward 154.280998\n",
      "running time 1408.888232\n",
      "Train_EnvstepsSoFar : 409001\n",
      "Train_AverageReturn : 125.62833209688466\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1408.8882322311401\n",
      "Training Loss : 1.7290271520614624\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 410001\n",
      "mean reward (100 episodes) 124.777870\n",
      "best mean reward 154.280998\n",
      "running time 1411.554638\n",
      "Train_EnvstepsSoFar : 410001\n",
      "Train_AverageReturn : 124.77786968399653\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1411.5546381473541\n",
      "Training Loss : 0.1400427669286728\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 411001\n",
      "mean reward (100 episodes) 123.464648\n",
      "best mean reward 154.280998\n",
      "running time 1414.260068\n",
      "Train_EnvstepsSoFar : 411001\n",
      "Train_AverageReturn : 123.46464775765642\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1414.2600681781769\n",
      "Training Loss : 2.2958128452301025\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 412001\n",
      "mean reward (100 episodes) 115.940110\n",
      "best mean reward 154.280998\n",
      "running time 1416.899337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 412001\n",
      "Train_AverageReturn : 115.94011008173494\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1416.8993372917175\n",
      "Training Loss : 0.3488525152206421\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 413001\n",
      "mean reward (100 episodes) 116.943736\n",
      "best mean reward 154.280998\n",
      "running time 1419.660407\n",
      "Train_EnvstepsSoFar : 413001\n",
      "Train_AverageReturn : 116.94373561465076\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1419.6604070663452\n",
      "Training Loss : 2.4622206687927246\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 414001\n",
      "mean reward (100 episodes) 122.909716\n",
      "best mean reward 154.280998\n",
      "running time 1422.390275\n",
      "Train_EnvstepsSoFar : 414001\n",
      "Train_AverageReturn : 122.9097156130925\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1422.3902752399445\n",
      "Training Loss : 1.606164574623108\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 415001\n",
      "mean reward (100 episodes) 114.964947\n",
      "best mean reward 154.280998\n",
      "running time 1425.048753\n",
      "Train_EnvstepsSoFar : 415001\n",
      "Train_AverageReturn : 114.96494661456914\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1425.0487532615662\n",
      "Training Loss : 0.6031155586242676\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 416001\n",
      "mean reward (100 episodes) 119.824336\n",
      "best mean reward 154.280998\n",
      "running time 1427.796214\n",
      "Train_EnvstepsSoFar : 416001\n",
      "Train_AverageReturn : 119.8243357617244\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1427.7962141036987\n",
      "Training Loss : 0.15785598754882812\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 417001\n",
      "mean reward (100 episodes) 119.516096\n",
      "best mean reward 154.280998\n",
      "running time 1430.479827\n",
      "Train_EnvstepsSoFar : 417001\n",
      "Train_AverageReturn : 119.5160956661988\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1430.4798271656036\n",
      "Training Loss : 1.0274937152862549\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 418001\n",
      "mean reward (100 episodes) 117.470091\n",
      "best mean reward 154.280998\n",
      "running time 1433.182876\n",
      "Train_EnvstepsSoFar : 418001\n",
      "Train_AverageReturn : 117.47009106842351\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1433.1828763484955\n",
      "Training Loss : 0.10079826414585114\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 419001\n",
      "mean reward (100 episodes) 117.503461\n",
      "best mean reward 154.280998\n",
      "running time 1436.130426\n",
      "Train_EnvstepsSoFar : 419001\n",
      "Train_AverageReturn : 117.50346104646171\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1436.1304261684418\n",
      "Training Loss : 1.227159023284912\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 420001\n",
      "mean reward (100 episodes) 119.327891\n",
      "best mean reward 154.280998\n",
      "running time 1438.828537\n",
      "Train_EnvstepsSoFar : 420001\n",
      "Train_AverageReturn : 119.32789088214189\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1438.8285372257233\n",
      "Training Loss : 0.26187795400619507\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 421001\n",
      "mean reward (100 episodes) 126.679841\n",
      "best mean reward 154.280998\n",
      "running time 1441.605452\n",
      "Train_EnvstepsSoFar : 421001\n",
      "Train_AverageReturn : 126.67984104348496\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1441.6054520606995\n",
      "Training Loss : 0.1263100504875183\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 422001\n",
      "mean reward (100 episodes) 133.834422\n",
      "best mean reward 154.280998\n",
      "running time 1444.362865\n",
      "Train_EnvstepsSoFar : 422001\n",
      "Train_AverageReturn : 133.83442166496266\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1444.3628652095795\n",
      "Training Loss : 0.1886105239391327\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 423001\n",
      "mean reward (100 episodes) 131.141679\n",
      "best mean reward 154.280998\n",
      "running time 1447.054270\n",
      "Train_EnvstepsSoFar : 423001\n",
      "Train_AverageReturn : 131.14167895976698\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1447.0542702674866\n",
      "Training Loss : 0.1593095064163208\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 424001\n",
      "mean reward (100 episodes) 134.821877\n",
      "best mean reward 154.280998\n",
      "running time 1449.734378\n",
      "Train_EnvstepsSoFar : 424001\n",
      "Train_AverageReturn : 134.82187701248304\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1449.7343780994415\n",
      "Training Loss : 0.10300512611865997\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 425001\n",
      "mean reward (100 episodes) 140.029370\n",
      "best mean reward 154.280998\n",
      "running time 1452.490813\n",
      "Train_EnvstepsSoFar : 425001\n",
      "Train_AverageReturn : 140.029369502496\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1452.49081325531\n",
      "Training Loss : 1.6761938333511353\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 426001\n",
      "mean reward (100 episodes) 140.390123\n",
      "best mean reward 154.280998\n",
      "running time 1455.181745\n",
      "Train_EnvstepsSoFar : 426001\n",
      "Train_AverageReturn : 140.39012345313927\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1455.1817452907562\n",
      "Training Loss : 2.0634536743164062\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 427001\n",
      "mean reward (100 episodes) 134.110367\n",
      "best mean reward 154.280998\n",
      "running time 1457.870043\n",
      "Train_EnvstepsSoFar : 427001\n",
      "Train_AverageReturn : 134.11036737746545\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1457.8700432777405\n",
      "Training Loss : 1.3485972881317139\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 428001\n",
      "mean reward (100 episodes) 142.440095\n",
      "best mean reward 154.280998\n",
      "running time 1460.753771\n",
      "Train_EnvstepsSoFar : 428001\n",
      "Train_AverageReturn : 142.44009548497326\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1460.7537710666656\n",
      "Training Loss : 0.08374112099409103\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 429001\n",
      "mean reward (100 episodes) 150.412051\n",
      "best mean reward 154.280998\n",
      "running time 1463.530262\n",
      "Train_EnvstepsSoFar : 429001\n",
      "Train_AverageReturn : 150.41205075207063\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1463.5302619934082\n",
      "Training Loss : 0.0860685482621193\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 430001\n",
      "mean reward (100 episodes) 150.991513\n",
      "best mean reward 154.280998\n",
      "running time 1466.272438\n",
      "Train_EnvstepsSoFar : 430001\n",
      "Train_AverageReturn : 150.99151293688743\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1466.2724380493164\n",
      "Training Loss : 1.4652483463287354\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 431001\n",
      "mean reward (100 episodes) 147.942834\n",
      "best mean reward 154.280998\n",
      "running time 1469.000373\n",
      "Train_EnvstepsSoFar : 431001\n",
      "Train_AverageReturn : 147.94283350568054\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1469.0003731250763\n",
      "Training Loss : 0.10081656277179718\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 432001\n",
      "mean reward (100 episodes) 153.163625\n",
      "best mean reward 154.280998\n",
      "running time 1471.678301\n",
      "Train_EnvstepsSoFar : 432001\n",
      "Train_AverageReturn : 153.16362524304455\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1471.678301334381\n",
      "Training Loss : 0.136570006608963\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 433001\n",
      "mean reward (100 episodes) 151.629823\n",
      "best mean reward 154.280998\n",
      "running time 1474.533934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 433001\n",
      "Train_AverageReturn : 151.6298229944743\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1474.5339341163635\n",
      "Training Loss : 0.09618035703897476\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 434001\n",
      "mean reward (100 episodes) 153.193028\n",
      "best mean reward 154.280998\n",
      "running time 1477.205372\n",
      "Train_EnvstepsSoFar : 434001\n",
      "Train_AverageReturn : 153.1930275809381\n",
      "Train_BestReturn : 154.28099773601443\n",
      "TimeSinceStart : 1477.2053723335266\n",
      "Training Loss : 2.216078042984009\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 435001\n",
      "mean reward (100 episodes) 159.145950\n",
      "best mean reward 159.145950\n",
      "running time 1479.874536\n",
      "Train_EnvstepsSoFar : 435001\n",
      "Train_AverageReturn : 159.14594958299395\n",
      "Train_BestReturn : 159.14594958299395\n",
      "TimeSinceStart : 1479.8745362758636\n",
      "Training Loss : 0.22744882106781006\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 436000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 436001\n",
      "mean reward (100 episodes) 161.486796\n",
      "best mean reward 161.486796\n",
      "running time 1482.564658\n",
      "Train_EnvstepsSoFar : 436001\n",
      "Train_AverageReturn : 161.48679586390293\n",
      "Train_BestReturn : 161.48679586390293\n",
      "TimeSinceStart : 1482.564658164978\n",
      "Training Loss : 12.701733589172363\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 437001\n",
      "mean reward (100 episodes) 161.944809\n",
      "best mean reward 161.944809\n",
      "running time 1485.218218\n",
      "Train_EnvstepsSoFar : 437001\n",
      "Train_AverageReturn : 161.94480942757139\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1485.21821808815\n",
      "Training Loss : 0.3219209313392639\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 438001\n",
      "mean reward (100 episodes) 151.231683\n",
      "best mean reward 161.944809\n",
      "running time 1487.859543\n",
      "Train_EnvstepsSoFar : 438001\n",
      "Train_AverageReturn : 151.2316826743073\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1487.8595430850983\n",
      "Training Loss : 1.327120065689087\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 439001\n",
      "mean reward (100 episodes) 145.596846\n",
      "best mean reward 161.944809\n",
      "running time 1490.621398\n",
      "Train_EnvstepsSoFar : 439001\n",
      "Train_AverageReturn : 145.596845560146\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1490.621397972107\n",
      "Training Loss : 0.16894108057022095\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 440001\n",
      "mean reward (100 episodes) 146.855868\n",
      "best mean reward 161.944809\n",
      "running time 1493.350213\n",
      "Train_EnvstepsSoFar : 440001\n",
      "Train_AverageReturn : 146.85586809022314\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1493.3502132892609\n",
      "Training Loss : 3.972484588623047\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 441001\n",
      "mean reward (100 episodes) 147.994284\n",
      "best mean reward 161.944809\n",
      "running time 1496.085954\n",
      "Train_EnvstepsSoFar : 441001\n",
      "Train_AverageReturn : 147.99428388639913\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1496.0859541893005\n",
      "Training Loss : 1.4704432487487793\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 442001\n",
      "mean reward (100 episodes) 155.015003\n",
      "best mean reward 161.944809\n",
      "running time 1498.777532\n",
      "Train_EnvstepsSoFar : 442001\n",
      "Train_AverageReturn : 155.01500295191576\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1498.777532339096\n",
      "Training Loss : 0.09069333970546722\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 443001\n",
      "mean reward (100 episodes) 161.025822\n",
      "best mean reward 161.944809\n",
      "running time 1501.444190\n",
      "Train_EnvstepsSoFar : 443001\n",
      "Train_AverageReturn : 161.0258222070454\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1501.4441902637482\n",
      "Training Loss : 0.227501779794693\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 444001\n",
      "mean reward (100 episodes) 157.958243\n",
      "best mean reward 161.944809\n",
      "running time 1504.196984\n",
      "Train_EnvstepsSoFar : 444001\n",
      "Train_AverageReturn : 157.95824291209703\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1504.196984052658\n",
      "Training Loss : 0.3363618552684784\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 445001\n",
      "mean reward (100 episodes) 150.781069\n",
      "best mean reward 161.944809\n",
      "running time 1506.803045\n",
      "Train_EnvstepsSoFar : 445001\n",
      "Train_AverageReturn : 150.78106877861447\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1506.8030452728271\n",
      "Training Loss : 1.4864392280578613\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 446001\n",
      "mean reward (100 episodes) 151.576517\n",
      "best mean reward 161.944809\n",
      "running time 1509.514837\n",
      "Train_EnvstepsSoFar : 446001\n",
      "Train_AverageReturn : 151.57651689263278\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1509.514837026596\n",
      "Training Loss : 1.1954206228256226\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 447001\n",
      "mean reward (100 episodes) 151.383731\n",
      "best mean reward 161.944809\n",
      "running time 1512.190000\n",
      "Train_EnvstepsSoFar : 447001\n",
      "Train_AverageReturn : 151.38373125703913\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1512.190000295639\n",
      "Training Loss : 0.11757199466228485\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 448001\n",
      "mean reward (100 episodes) 155.466236\n",
      "best mean reward 161.944809\n",
      "running time 1514.856946\n",
      "Train_EnvstepsSoFar : 448001\n",
      "Train_AverageReturn : 155.4662361262831\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1514.8569462299347\n",
      "Training Loss : 0.11626224219799042\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 449001\n",
      "mean reward (100 episodes) 153.274927\n",
      "best mean reward 161.944809\n",
      "running time 1517.470075\n",
      "Train_EnvstepsSoFar : 449001\n",
      "Train_AverageReturn : 153.27492655642212\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1517.4700751304626\n",
      "Training Loss : 2.331784725189209\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 450001\n",
      "mean reward (100 episodes) 152.221623\n",
      "best mean reward 161.944809\n",
      "running time 1520.117215\n",
      "Train_EnvstepsSoFar : 450001\n",
      "Train_AverageReturn : 152.221623013588\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1520.1172153949738\n",
      "Training Loss : 0.1246550977230072\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 451001\n",
      "mean reward (100 episodes) 147.285430\n",
      "best mean reward 161.944809\n",
      "running time 1522.888036\n",
      "Train_EnvstepsSoFar : 451001\n",
      "Train_AverageReturn : 147.28543033707166\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1522.8880362510681\n",
      "Training Loss : 0.263714998960495\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 452001\n",
      "mean reward (100 episodes) 148.176836\n",
      "best mean reward 161.944809\n",
      "running time 1525.585729\n",
      "Train_EnvstepsSoFar : 452001\n",
      "Train_AverageReturn : 148.17683577494356\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1525.5857291221619\n",
      "Training Loss : 0.22562137246131897\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 453001\n",
      "mean reward (100 episodes) 148.219261\n",
      "best mean reward 161.944809\n",
      "running time 1528.202165\n",
      "Train_EnvstepsSoFar : 453001\n",
      "Train_AverageReturn : 148.21926122174602\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1528.2021651268005\n",
      "Training Loss : 0.06371275335550308\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 454001\n",
      "mean reward (100 episodes) 147.721364\n",
      "best mean reward 161.944809\n",
      "running time 1530.831465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 454001\n",
      "Train_AverageReturn : 147.72136412578413\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1530.8314652442932\n",
      "Training Loss : 0.08983515948057175\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 455001\n",
      "mean reward (100 episodes) 147.857998\n",
      "best mean reward 161.944809\n",
      "running time 1533.480659\n",
      "Train_EnvstepsSoFar : 455001\n",
      "Train_AverageReturn : 147.8579983535985\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1533.4806592464447\n",
      "Training Loss : 3.9369094371795654\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 456001\n",
      "mean reward (100 episodes) 145.908502\n",
      "best mean reward 161.944809\n",
      "running time 1536.164843\n",
      "Train_EnvstepsSoFar : 456001\n",
      "Train_AverageReturn : 145.9085020744729\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1536.164843082428\n",
      "Training Loss : 0.20499911904335022\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 457001\n",
      "mean reward (100 episodes) 141.638125\n",
      "best mean reward 161.944809\n",
      "running time 1538.797059\n",
      "Train_EnvstepsSoFar : 457001\n",
      "Train_AverageReturn : 141.63812549663987\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1538.797059059143\n",
      "Training Loss : 0.9189102649688721\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 458001\n",
      "mean reward (100 episodes) 143.199085\n",
      "best mean reward 161.944809\n",
      "running time 1541.467748\n",
      "Train_EnvstepsSoFar : 458001\n",
      "Train_AverageReturn : 143.19908462753799\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1541.4677481651306\n",
      "Training Loss : 0.26537197828292847\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 459001\n",
      "mean reward (100 episodes) 144.011076\n",
      "best mean reward 161.944809\n",
      "running time 1544.172006\n",
      "Train_EnvstepsSoFar : 459001\n",
      "Train_AverageReturn : 144.0110755519462\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1544.1720061302185\n",
      "Training Loss : 0.1666446030139923\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 460001\n",
      "mean reward (100 episodes) 148.482782\n",
      "best mean reward 161.944809\n",
      "running time 1546.821080\n",
      "Train_EnvstepsSoFar : 460001\n",
      "Train_AverageReturn : 148.48278243609172\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1546.8210802078247\n",
      "Training Loss : 0.3520919680595398\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 461001\n",
      "mean reward (100 episodes) 142.729305\n",
      "best mean reward 161.944809\n",
      "running time 1549.496056\n",
      "Train_EnvstepsSoFar : 461001\n",
      "Train_AverageReturn : 142.72930497438716\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1549.4960560798645\n",
      "Training Loss : 0.31894785165786743\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 462001\n",
      "mean reward (100 episodes) 138.636001\n",
      "best mean reward 161.944809\n",
      "running time 1552.191054\n",
      "Train_EnvstepsSoFar : 462001\n",
      "Train_AverageReturn : 138.63600131587805\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1552.1910541057587\n",
      "Training Loss : 0.23700019717216492\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 463001\n",
      "mean reward (100 episodes) 130.520945\n",
      "best mean reward 161.944809\n",
      "running time 1554.990050\n",
      "Train_EnvstepsSoFar : 463001\n",
      "Train_AverageReturn : 130.52094460993536\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1554.9900500774384\n",
      "Training Loss : 2.168447971343994\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 464001\n",
      "mean reward (100 episodes) 119.191618\n",
      "best mean reward 161.944809\n",
      "running time 1557.815498\n",
      "Train_EnvstepsSoFar : 464001\n",
      "Train_AverageReturn : 119.19161787441674\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1557.8154983520508\n",
      "Training Loss : 0.20962688326835632\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 465001\n",
      "mean reward (100 episodes) 113.528380\n",
      "best mean reward 161.944809\n",
      "running time 1560.568569\n",
      "Train_EnvstepsSoFar : 465001\n",
      "Train_AverageReturn : 113.52837971193895\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1560.5685691833496\n",
      "Training Loss : 1.5005240440368652\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 466001\n",
      "mean reward (100 episodes) 112.920195\n",
      "best mean reward 161.944809\n",
      "running time 1563.261403\n",
      "Train_EnvstepsSoFar : 466001\n",
      "Train_AverageReturn : 112.92019484047928\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1563.2614033222198\n",
      "Training Loss : 0.1534038782119751\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 467001\n",
      "mean reward (100 episodes) 111.390623\n",
      "best mean reward 161.944809\n",
      "running time 1565.970964\n",
      "Train_EnvstepsSoFar : 467001\n",
      "Train_AverageReturn : 111.39062269358224\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1565.9709641933441\n",
      "Training Loss : 0.27134960889816284\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 468001\n",
      "mean reward (100 episodes) 114.651868\n",
      "best mean reward 161.944809\n",
      "running time 1568.688276\n",
      "Train_EnvstepsSoFar : 468001\n",
      "Train_AverageReturn : 114.65186786808289\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1568.6882762908936\n",
      "Training Loss : 0.11040893197059631\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 469001\n",
      "mean reward (100 episodes) 113.090325\n",
      "best mean reward 161.944809\n",
      "running time 1571.410852\n",
      "Train_EnvstepsSoFar : 469001\n",
      "Train_AverageReturn : 113.09032477672748\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1571.4108521938324\n",
      "Training Loss : 0.396413117647171\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 470001\n",
      "mean reward (100 episodes) 114.892418\n",
      "best mean reward 161.944809\n",
      "running time 1574.087972\n",
      "Train_EnvstepsSoFar : 470001\n",
      "Train_AverageReturn : 114.89241840410732\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1574.087972164154\n",
      "Training Loss : 0.7510015964508057\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 471001\n",
      "mean reward (100 episodes) 116.367054\n",
      "best mean reward 161.944809\n",
      "running time 1576.762197\n",
      "Train_EnvstepsSoFar : 471001\n",
      "Train_AverageReturn : 116.36705407133518\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1576.7621970176697\n",
      "Training Loss : 0.12147374451160431\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 472001\n",
      "mean reward (100 episodes) 115.669825\n",
      "best mean reward 161.944809\n",
      "running time 1579.624605\n",
      "Train_EnvstepsSoFar : 472001\n",
      "Train_AverageReturn : 115.66982488476442\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1579.624605178833\n",
      "Training Loss : 0.1904548853635788\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 473001\n",
      "mean reward (100 episodes) 116.816711\n",
      "best mean reward 161.944809\n",
      "running time 1582.262436\n",
      "Train_EnvstepsSoFar : 473001\n",
      "Train_AverageReturn : 116.8167114996224\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1582.2624361515045\n",
      "Training Loss : 0.4907536804676056\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 474001\n",
      "mean reward (100 episodes) 106.373228\n",
      "best mean reward 161.944809\n",
      "running time 1585.266761\n",
      "Train_EnvstepsSoFar : 474001\n",
      "Train_AverageReturn : 106.37322780992469\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1585.266761302948\n",
      "Training Loss : 1.7602499723434448\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 475001\n",
      "mean reward (100 episodes) 109.070737\n",
      "best mean reward 161.944809\n",
      "running time 1588.067921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 475001\n",
      "Train_AverageReturn : 109.07073747742672\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1588.0679211616516\n",
      "Training Loss : 1.6317424774169922\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 476001\n",
      "mean reward (100 episodes) 108.157606\n",
      "best mean reward 161.944809\n",
      "running time 1590.824520\n",
      "Train_EnvstepsSoFar : 476001\n",
      "Train_AverageReturn : 108.15760568752131\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1590.8245203495026\n",
      "Training Loss : 0.20396649837493896\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 477001\n",
      "mean reward (100 episodes) 100.193685\n",
      "best mean reward 161.944809\n",
      "running time 1593.555040\n",
      "Train_EnvstepsSoFar : 477001\n",
      "Train_AverageReturn : 100.1936852575259\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1593.5550401210785\n",
      "Training Loss : 0.18437299132347107\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 478001\n",
      "mean reward (100 episodes) 96.025644\n",
      "best mean reward 161.944809\n",
      "running time 1596.188425\n",
      "Train_EnvstepsSoFar : 478001\n",
      "Train_AverageReturn : 96.0256443236013\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1596.1884253025055\n",
      "Training Loss : 0.2733091115951538\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 479001\n",
      "mean reward (100 episodes) 97.523938\n",
      "best mean reward 161.944809\n",
      "running time 1598.863642\n",
      "Train_EnvstepsSoFar : 479001\n",
      "Train_AverageReturn : 97.52393802613591\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1598.8636422157288\n",
      "Training Loss : 2.5930750370025635\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 480001\n",
      "mean reward (100 episodes) 107.880882\n",
      "best mean reward 161.944809\n",
      "running time 1601.509183\n",
      "Train_EnvstepsSoFar : 480001\n",
      "Train_AverageReturn : 107.8808821528865\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1601.5091834068298\n",
      "Training Loss : 1.7313402891159058\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 481001\n",
      "mean reward (100 episodes) 111.479056\n",
      "best mean reward 161.944809\n",
      "running time 1604.220260\n",
      "Train_EnvstepsSoFar : 481001\n",
      "Train_AverageReturn : 111.47905569040678\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1604.22026014328\n",
      "Training Loss : 2.1394436359405518\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 482001\n",
      "mean reward (100 episodes) 109.651929\n",
      "best mean reward 161.944809\n",
      "running time 1607.193641\n",
      "Train_EnvstepsSoFar : 482001\n",
      "Train_AverageReturn : 109.65192913803659\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1607.1936411857605\n",
      "Training Loss : 2.212986707687378\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 483001\n",
      "mean reward (100 episodes) 109.245338\n",
      "best mean reward 161.944809\n",
      "running time 1610.084381\n",
      "Train_EnvstepsSoFar : 483001\n",
      "Train_AverageReturn : 109.2453384102045\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1610.0843813419342\n",
      "Training Loss : 13.155438423156738\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 484001\n",
      "mean reward (100 episodes) 109.895170\n",
      "best mean reward 161.944809\n",
      "running time 1613.087357\n",
      "Train_EnvstepsSoFar : 484001\n",
      "Train_AverageReturn : 109.89517033946763\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1613.0873572826385\n",
      "Training Loss : 0.2080238312482834\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 485001\n",
      "mean reward (100 episodes) 110.182145\n",
      "best mean reward 161.944809\n",
      "running time 1615.905910\n",
      "Train_EnvstepsSoFar : 485001\n",
      "Train_AverageReturn : 110.18214548910572\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1615.9059100151062\n",
      "Training Loss : 0.23997320234775543\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 486001\n",
      "mean reward (100 episodes) 101.873080\n",
      "best mean reward 161.944809\n",
      "running time 1618.677687\n",
      "Train_EnvstepsSoFar : 486001\n",
      "Train_AverageReturn : 101.87308023562771\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1618.6776871681213\n",
      "Training Loss : 0.9321064352989197\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 487001\n",
      "mean reward (100 episodes) 106.253979\n",
      "best mean reward 161.944809\n",
      "running time 1621.860969\n",
      "Train_EnvstepsSoFar : 487001\n",
      "Train_AverageReturn : 106.25397898558413\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1621.8609693050385\n",
      "Training Loss : 1.4647865295410156\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 488001\n",
      "mean reward (100 episodes) 107.155497\n",
      "best mean reward 161.944809\n",
      "running time 1624.663542\n",
      "Train_EnvstepsSoFar : 488001\n",
      "Train_AverageReturn : 107.15549667746374\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1624.6635420322418\n",
      "Training Loss : 0.4789448082447052\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 489001\n",
      "mean reward (100 episodes) 108.420708\n",
      "best mean reward 161.944809\n",
      "running time 1627.350913\n",
      "Train_EnvstepsSoFar : 489001\n",
      "Train_AverageReturn : 108.42070804385028\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1627.3509130477905\n",
      "Training Loss : 2.7858967781066895\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 490001\n",
      "mean reward (100 episodes) 94.407918\n",
      "best mean reward 161.944809\n",
      "running time 1630.063322\n",
      "Train_EnvstepsSoFar : 490001\n",
      "Train_AverageReturn : 94.40791804828379\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1630.0633220672607\n",
      "Training Loss : 5.124368667602539\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 491001\n",
      "mean reward (100 episodes) 98.742656\n",
      "best mean reward 161.944809\n",
      "running time 1632.733193\n",
      "Train_EnvstepsSoFar : 491001\n",
      "Train_AverageReturn : 98.74265642821334\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1632.733193397522\n",
      "Training Loss : 0.17977993190288544\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 492001\n",
      "mean reward (100 episodes) 87.226447\n",
      "best mean reward 161.944809\n",
      "running time 1635.404150\n",
      "Train_EnvstepsSoFar : 492001\n",
      "Train_AverageReturn : 87.22644651734139\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1635.4041500091553\n",
      "Training Loss : 3.226506471633911\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 493001\n",
      "mean reward (100 episodes) 88.699021\n",
      "best mean reward 161.944809\n",
      "running time 1638.128235\n",
      "Train_EnvstepsSoFar : 493001\n",
      "Train_AverageReturn : 88.69902109759005\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1638.1282351016998\n",
      "Training Loss : 0.12237252295017242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 494001\n",
      "mean reward (100 episodes) 89.079854\n",
      "best mean reward 161.944809\n",
      "running time 1640.789037\n",
      "Train_EnvstepsSoFar : 494001\n",
      "Train_AverageReturn : 89.07985448997638\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1640.7890372276306\n",
      "Training Loss : 0.927770733833313\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 495001\n",
      "mean reward (100 episodes) 76.459009\n",
      "best mean reward 161.944809\n",
      "running time 1643.470587\n",
      "Train_EnvstepsSoFar : 495001\n",
      "Train_AverageReturn : 76.45900933098218\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1643.4705872535706\n",
      "Training Loss : 0.3036058843135834\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 496001\n",
      "mean reward (100 episodes) 70.370337\n",
      "best mean reward 161.944809\n",
      "running time 1646.297246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 496001\n",
      "Train_AverageReturn : 70.37033736636005\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1646.2972462177277\n",
      "Training Loss : 7.371368408203125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 497001\n",
      "mean reward (100 episodes) 66.434760\n",
      "best mean reward 161.944809\n",
      "running time 1648.955200\n",
      "Train_EnvstepsSoFar : 497001\n",
      "Train_AverageReturn : 66.4347597544451\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1648.955199956894\n",
      "Training Loss : 1.1901475191116333\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 498001\n",
      "mean reward (100 episodes) 64.204122\n",
      "best mean reward 161.944809\n",
      "running time 1651.603387\n",
      "Train_EnvstepsSoFar : 498001\n",
      "Train_AverageReturn : 64.20412236253841\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1651.6033871173859\n",
      "Training Loss : 0.20250026881694794\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 499000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 499001\n",
      "mean reward (100 episodes) 66.503869\n",
      "best mean reward 161.944809\n",
      "running time 1654.313317\n",
      "Train_EnvstepsSoFar : 499001\n",
      "Train_AverageReturn : 66.50386918945716\n",
      "Train_BestReturn : 161.94480942757139\n",
      "TimeSinceStart : 1654.3133172988892\n",
      "Training Loss : 0.7788086533546448\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dqn_args = dict(dqn_base_args_dict)\n",
    "\n",
    "env_str = 'LunarLander'\n",
    "dqn_args['env_name'] = '{}-v3'.format(env_str)\n",
    "dqn_args['double_q'] = False\n",
    "\n",
    "# Delete all previous logs\n",
    "remove_folder('logs/dqn/{}/vanilla_dqn'.format(env_str))\n",
    "\n",
    "for seed in range(3):\n",
    "    print(\"Running DQN experiment with seed\", seed)\n",
    "    dqn_args['seed'] = seed\n",
    "    dqn_args['logdir'] = 'logs/dqn/{}/vanilla_dqn/seed{}'.format(env_str, seed)\n",
    "    dqntrainer = DQN_Trainer(dqn_args)\n",
    "    dqntrainer.run_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9b3b956e1a98bc53\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9b3b956e1a98bc53\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6011;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Visualize vanilla DQN results on Lunar Lander\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/dqn/LunarLander/vanilla_dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double DQN\n",
    "One potential issue with learning our Q functions with bootstrapping is _maximization bias_, where the learned Q-values tend to overestimate the actual expected future returns. The main idea is that when there is estimation error in the next state's Q-values, even if the values were correct on average, picking the action with the maximum Q-value would tend to select one where the value is overestimated. This overoptimistic value would then also get propagated via the Bellman backups to other states and actions, and can potentially slow down learning.\n",
    "\n",
    "Double DQN (https://arxiv.org/abs/1509.06461) proposes a simple solution to alleviate this _maximization bias_. Instead of taking the next action that maximizes the target network's Q-value, it selects the action to maximize the _current_ Q function at the next state, and then takes the target network's estimate of that action's value. \n",
    "\n",
    "Implement the double DQN target value in the update method in <code>critics/dqn_critic.py</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################\n",
      "logging outputs to  test\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "LunarLander-v3\n",
      "Weight before update (first row) [ 0.07646337 -0.07932475  0.09140956 -0.01702595  0.14239588  0.07935759\n",
      " -0.03831157 -0.2694876   0.07610479]\n",
      "Initial loss 0.93894196\n",
      "Initial Loss Error 2.3609519617425807e-09 should be on the order of 1e-6 or lower\n",
      "0.8988525\n",
      "0.8602768\n",
      "0.8228415\n",
      "0.7871182\n",
      "Loss Error 2.231287016432748e-09 should be on the order of 1e-6 or lower\n",
      "Weight after update (first row) (64, 9)\n",
      "[ 0.07154967 -0.08432532  0.08641818 -0.02193821  0.13749473  0.08425266\n",
      " -0.04115245 -0.27120697  0.08096215]\n",
      "[-0.0049137  -0.00500057 -0.00499138 -0.00491226 -0.00490116  0.00489506\n",
      " -0.00284088 -0.00171939  0.00485736]\n",
      "Weight Update Error 1.3418982564751781e-06 should be on the order of 1e-6 or lower\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sagarasanghavi/.pyenv/versions/3.7.7/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "#### Test DQN target value with double Q\n",
    "dqn_args = dict(dqn_base_args_dict)\n",
    "\n",
    "env_str = 'LunarLander'\n",
    "dqn_args['env_name'] = '{}-v3'.format(env_str)\n",
    "dqn_args['double_q'] = True\n",
    "dqntrainer = DQN_Trainer(dqn_args)\n",
    "dqnagent = dqntrainer.rl_trainer.agent\n",
    "critic = dqnagent.critic\n",
    "\n",
    "ob_dim = critic.ob_dim\n",
    "ac_dim = 6\n",
    "N = 5\n",
    "\n",
    "np.random.seed(0)\n",
    "obs = np.random.normal(size=(N, ob_dim))\n",
    "acts = np.random.choice(ac_dim, size=(N,))\n",
    "next_obs = np.random.normal(size=(N, ob_dim))\n",
    "rewards = np.random.normal(size=N)\n",
    "terminals = np.zeros(N)\n",
    "terminals[0] = 1\n",
    "\n",
    "first_weight_before = np.array(ptu.to_numpy(next(critic.q_net.parameters())))\n",
    "print(\"Weight before update (first row)\", first_weight_before[0])\n",
    "\n",
    "\n",
    "loss = critic.update(obs, acts, next_obs, rewards, terminals)['Training Loss']\n",
    "expected_loss = 0.93894196\n",
    "loss_error = rel_error(loss, expected_loss)\n",
    "print(\"Initial loss\", loss)\n",
    "print(\"Initial Loss Error\", loss_error, \"should be on the order of 1e-6 or lower\")\n",
    "\n",
    "for i in range(4):\n",
    "    loss = critic.update(obs, acts, next_obs, rewards, terminals)['Training Loss']\n",
    "    print(loss)\n",
    "\n",
    "expected_loss = 0.7871182\n",
    "loss_error = rel_error(loss, expected_loss)\n",
    "print(\"Loss Error\", loss_error, \"should be on the order of 1e-6 or lower\")\n",
    "\n",
    "\n",
    "first_weight_after = np.array(ptu.to_numpy(next(critic.q_net.parameters())))\n",
    "print(\"Weight after update (first row)\", first_weight_after.shape)\n",
    "# Test DQN gradient\n",
    "print(first_weight_after[0])\n",
    "weight_change_partial = first_weight_after[0] - first_weight_before[0]\n",
    "print(weight_change_partial)\n",
    "expected_weight_change = np.array([-0.0049137, -0.00500057, -0.00499138, -0.00491226, -0.00490116,  0.00489506,\n",
    " -0.00284088, -0.00171939,  0.00485736])\n",
    "\n",
    "\n",
    "updated_weight_error = rel_error(weight_change_partial, expected_weight_change)\n",
    "print(\"Weight Update Error\", updated_weight_error, \"should be on the order of 1e-6 or lower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now also run some experiments on LunarLander with Double DQN. You may be able to see that double DQN performs slightly better and more stably, but as there is very high variance, dont' worry if you do not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder logs/dqn/LunarLander/double_dqn does not exist yet. No old results to delete\n",
      "Running DQN experiment with seed 0\n",
      "########################\n",
      "logging outputs to  logs/dqn/LunarLander/double_dqn/seed0\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "LunarLander-v3\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1\n",
      "mean reward (100 episodes) nan\n",
      "best mean reward -inf\n",
      "running time 0.000730\n",
      "Train_EnvstepsSoFar : 1\n",
      "TimeSinceStart : 0.0007300376892089844\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sagarasanghavi/.pyenv/versions/3.7.7/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 1000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1001\n",
      "mean reward (100 episodes) -361.458927\n",
      "best mean reward -inf\n",
      "running time 0.657439\n",
      "Train_EnvstepsSoFar : 1001\n",
      "Train_AverageReturn : -361.4589273606985\n",
      "TimeSinceStart : 0.6574389934539795\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2001\n",
      "mean reward (100 episodes) -317.658504\n",
      "best mean reward -inf\n",
      "running time 4.847034\n",
      "Train_EnvstepsSoFar : 2001\n",
      "Train_AverageReturn : -317.6585039542492\n",
      "TimeSinceStart : 4.847033977508545\n",
      "Training Loss : 0.3789442181587219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3001\n",
      "mean reward (100 episodes) -298.225730\n",
      "best mean reward -inf\n",
      "running time 8.871307\n",
      "Train_EnvstepsSoFar : 3001\n",
      "Train_AverageReturn : -298.2257298425541\n",
      "TimeSinceStart : 8.871306896209717\n",
      "Training Loss : 0.24803029000759125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4001\n",
      "mean reward (100 episodes) -282.918092\n",
      "best mean reward -inf\n",
      "running time 14.842299\n",
      "Train_EnvstepsSoFar : 4001\n",
      "Train_AverageReturn : -282.91809152461036\n",
      "TimeSinceStart : 14.842298746109009\n",
      "Training Loss : 4.226571559906006\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5001\n",
      "mean reward (100 episodes) -277.909930\n",
      "best mean reward -inf\n",
      "running time 20.118753\n",
      "Train_EnvstepsSoFar : 5001\n",
      "Train_AverageReturn : -277.90993049511326\n",
      "TimeSinceStart : 20.11875295639038\n",
      "Training Loss : 3.447936773300171\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6001\n",
      "mean reward (100 episodes) -266.652893\n",
      "best mean reward -inf\n",
      "running time 25.057109\n",
      "Train_EnvstepsSoFar : 6001\n",
      "Train_AverageReturn : -266.65289332492273\n",
      "TimeSinceStart : 25.057108879089355\n",
      "Training Loss : 0.9429066777229309\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7001\n",
      "mean reward (100 episodes) -268.093860\n",
      "best mean reward -inf\n",
      "running time 29.492624\n",
      "Train_EnvstepsSoFar : 7001\n",
      "Train_AverageReturn : -268.09386009187807\n",
      "TimeSinceStart : 29.492623805999756\n",
      "Training Loss : 0.5014455318450928\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8001\n",
      "mean reward (100 episodes) -264.580020\n",
      "best mean reward -inf\n",
      "running time 36.459729\n",
      "Train_EnvstepsSoFar : 8001\n",
      "Train_AverageReturn : -264.58001990467824\n",
      "TimeSinceStart : 36.459728956222534\n",
      "Training Loss : 0.9670765399932861\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9001\n",
      "mean reward (100 episodes) -256.601722\n",
      "best mean reward -inf\n",
      "running time 40.527976\n",
      "Train_EnvstepsSoFar : 9001\n",
      "Train_AverageReturn : -256.6017219100866\n",
      "TimeSinceStart : 40.5279757976532\n",
      "Training Loss : 0.5097922086715698\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10001\n",
      "mean reward (100 episodes) -254.947207\n",
      "best mean reward -inf\n",
      "running time 45.416442\n",
      "Train_EnvstepsSoFar : 10001\n",
      "Train_AverageReturn : -254.94720730516397\n",
      "TimeSinceStart : 45.41644215583801\n",
      "Training Loss : 1.4701049327850342\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 11000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 11001\n",
      "mean reward (100 episodes) -250.187628\n",
      "best mean reward -inf\n",
      "running time 48.985585\n",
      "Train_EnvstepsSoFar : 11001\n",
      "Train_AverageReturn : -250.18762782998903\n",
      "TimeSinceStart : 48.98558497428894\n",
      "Training Loss : 0.3513171970844269\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 12000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 12001\n",
      "mean reward (100 episodes) -247.320640\n",
      "best mean reward -inf\n",
      "running time 54.484557\n",
      "Train_EnvstepsSoFar : 12001\n",
      "Train_AverageReturn : -247.32063984961894\n",
      "TimeSinceStart : 54.484556913375854\n",
      "Training Loss : 1.0230896472930908\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 13000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 13001\n",
      "mean reward (100 episodes) -240.783094\n",
      "best mean reward -inf\n",
      "running time 58.153734\n",
      "Train_EnvstepsSoFar : 13001\n",
      "Train_AverageReturn : -240.78309421384847\n",
      "TimeSinceStart : 58.15373396873474\n",
      "Training Loss : 1.2495532035827637\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 14000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 14001\n",
      "mean reward (100 episodes) -240.084690\n",
      "best mean reward -inf\n",
      "running time 61.958887\n",
      "Train_EnvstepsSoFar : 14001\n",
      "Train_AverageReturn : -240.08468955085323\n",
      "TimeSinceStart : 61.95888686180115\n",
      "Training Loss : 3.4116876125335693\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 15000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 15001\n",
      "mean reward (100 episodes) -239.093600\n",
      "best mean reward -inf\n",
      "running time 65.638218\n",
      "Train_EnvstepsSoFar : 15001\n",
      "Train_AverageReturn : -239.09359973774426\n",
      "TimeSinceStart : 65.63821792602539\n",
      "Training Loss : 0.6379950642585754\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 16000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 16001\n",
      "mean reward (100 episodes) -239.924190\n",
      "best mean reward -inf\n",
      "running time 69.639015\n",
      "Train_EnvstepsSoFar : 16001\n",
      "Train_AverageReturn : -239.92418995242573\n",
      "TimeSinceStart : 69.63901495933533\n",
      "Training Loss : 0.5668277740478516\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 17000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 17001\n",
      "mean reward (100 episodes) -238.224063\n",
      "best mean reward -inf\n",
      "running time 73.634729\n",
      "Train_EnvstepsSoFar : 17001\n",
      "Train_AverageReturn : -238.2240634774102\n",
      "TimeSinceStart : 73.63472890853882\n",
      "Training Loss : 3.98356556892395\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 18000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 18001\n",
      "mean reward (100 episodes) -233.722398\n",
      "best mean reward -233.722398\n",
      "running time 80.204322\n",
      "Train_EnvstepsSoFar : 18001\n",
      "Train_AverageReturn : -233.72239811440363\n",
      "Train_BestReturn : -233.72239811440363\n",
      "TimeSinceStart : 80.20432209968567\n",
      "Training Loss : 0.6405782699584961\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 19000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 19001\n",
      "mean reward (100 episodes) -224.992374\n",
      "best mean reward -224.992374\n",
      "running time 85.010987\n",
      "Train_EnvstepsSoFar : 19001\n",
      "Train_AverageReturn : -224.99237449945494\n",
      "Train_BestReturn : -224.99237449945494\n",
      "TimeSinceStart : 85.01098704338074\n",
      "Training Loss : 3.9320180416107178\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 20001\n",
      "mean reward (100 episodes) -222.914519\n",
      "best mean reward -222.914519\n",
      "running time 89.909453\n",
      "Train_EnvstepsSoFar : 20001\n",
      "Train_AverageReturn : -222.91451870482067\n",
      "Train_BestReturn : -222.91451870482067\n",
      "TimeSinceStart : 89.90945291519165\n",
      "Training Loss : 0.5125091075897217\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 21000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 21001\n",
      "mean reward (100 episodes) -219.447207\n",
      "best mean reward -219.447207\n",
      "running time 94.176890\n",
      "Train_EnvstepsSoFar : 21001\n",
      "Train_AverageReturn : -219.44720663773566\n",
      "Train_BestReturn : -219.44720663773566\n",
      "TimeSinceStart : 94.1768901348114\n",
      "Training Loss : 5.408292293548584\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 22000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 22001\n",
      "mean reward (100 episodes) -218.047836\n",
      "best mean reward -218.047836\n",
      "running time 99.122103\n",
      "Train_EnvstepsSoFar : 22001\n",
      "Train_AverageReturn : -218.04783556466123\n",
      "Train_BestReturn : -218.04783556466123\n",
      "TimeSinceStart : 99.12210297584534\n",
      "Training Loss : 0.6950637102127075\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 23000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 23001\n",
      "mean reward (100 episodes) -215.016228\n",
      "best mean reward -215.016228\n",
      "running time 104.643006\n",
      "Train_EnvstepsSoFar : 23001\n",
      "Train_AverageReturn : -215.01622795258237\n",
      "Train_BestReturn : -215.01622795258237\n",
      "TimeSinceStart : 104.64300584793091\n",
      "Training Loss : 1.6912081241607666\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 24000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 24001\n",
      "mean reward (100 episodes) -212.727138\n",
      "best mean reward -212.727138\n",
      "running time 109.342728\n",
      "Train_EnvstepsSoFar : 24001\n",
      "Train_AverageReturn : -212.7271384089546\n",
      "Train_BestReturn : -212.7271384089546\n",
      "TimeSinceStart : 109.34272789955139\n",
      "Training Loss : 0.836423397064209\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 25000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 25001\n",
      "mean reward (100 episodes) -210.460493\n",
      "best mean reward -210.460493\n",
      "running time 114.993388\n",
      "Train_EnvstepsSoFar : 25001\n",
      "Train_AverageReturn : -210.46049303442564\n",
      "Train_BestReturn : -210.46049303442564\n",
      "TimeSinceStart : 114.99338793754578\n",
      "Training Loss : 1.9687288999557495\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 26000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 26001\n",
      "mean reward (100 episodes) -210.516870\n",
      "best mean reward -210.460493\n",
      "running time 120.341567\n",
      "Train_EnvstepsSoFar : 26001\n",
      "Train_AverageReturn : -210.51686996109402\n",
      "Train_BestReturn : -210.46049303442564\n",
      "TimeSinceStart : 120.34156703948975\n",
      "Training Loss : 0.4330020546913147\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 27000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 27001\n",
      "mean reward (100 episodes) -210.383170\n",
      "best mean reward -210.383170\n",
      "running time 125.203849\n",
      "Train_EnvstepsSoFar : 27001\n",
      "Train_AverageReturn : -210.38316952210403\n",
      "Train_BestReturn : -210.38316952210403\n",
      "TimeSinceStart : 125.20384883880615\n",
      "Training Loss : 0.42548689246177673\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 28000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 28001\n",
      "mean reward (100 episodes) -207.652913\n",
      "best mean reward -207.652913\n",
      "running time 129.907932\n",
      "Train_EnvstepsSoFar : 28001\n",
      "Train_AverageReturn : -207.65291290919077\n",
      "Train_BestReturn : -207.65291290919077\n",
      "TimeSinceStart : 129.90793180465698\n",
      "Training Loss : 1.000068187713623\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 29000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 29001\n",
      "mean reward (100 episodes) -205.142517\n",
      "best mean reward -205.142517\n",
      "running time 135.010528\n",
      "Train_EnvstepsSoFar : 29001\n",
      "Train_AverageReturn : -205.1425174024294\n",
      "Train_BestReturn : -205.1425174024294\n",
      "TimeSinceStart : 135.0105278491974\n",
      "Training Loss : 0.3451642394065857\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 30001\n",
      "mean reward (100 episodes) -204.182015\n",
      "best mean reward -204.182015\n",
      "running time 140.124355\n",
      "Train_EnvstepsSoFar : 30001\n",
      "Train_AverageReturn : -204.18201525260181\n",
      "Train_BestReturn : -204.18201525260181\n",
      "TimeSinceStart : 140.12435507774353\n",
      "Training Loss : 0.41495630145072937\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 31000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 31001\n",
      "mean reward (100 episodes) -203.085759\n",
      "best mean reward -203.085759\n",
      "running time 144.933305\n",
      "Train_EnvstepsSoFar : 31001\n",
      "Train_AverageReturn : -203.08575949065457\n",
      "Train_BestReturn : -203.08575949065457\n",
      "TimeSinceStart : 144.9333050251007\n",
      "Training Loss : 1.2193031311035156\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 32000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 32001\n",
      "mean reward (100 episodes) -198.462898\n",
      "best mean reward -198.462898\n",
      "running time 149.422920\n",
      "Train_EnvstepsSoFar : 32001\n",
      "Train_AverageReturn : -198.46289793466727\n",
      "Train_BestReturn : -198.46289793466727\n",
      "TimeSinceStart : 149.4229199886322\n",
      "Training Loss : 1.1900917291641235\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 33000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 33001\n",
      "mean reward (100 episodes) -194.045011\n",
      "best mean reward -194.045011\n",
      "running time 153.579547\n",
      "Train_EnvstepsSoFar : 33001\n",
      "Train_AverageReturn : -194.04501106363983\n",
      "Train_BestReturn : -194.04501106363983\n",
      "TimeSinceStart : 153.57954692840576\n",
      "Training Loss : 1.30045485496521\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 34000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 34001\n",
      "mean reward (100 episodes) -193.465684\n",
      "best mean reward -193.465684\n",
      "running time 158.762433\n",
      "Train_EnvstepsSoFar : 34001\n",
      "Train_AverageReturn : -193.4656844331411\n",
      "Train_BestReturn : -193.4656844331411\n",
      "TimeSinceStart : 158.762433052063\n",
      "Training Loss : 1.6998538970947266\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 35000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 35001\n",
      "mean reward (100 episodes) -194.194998\n",
      "best mean reward -193.465684\n",
      "running time 164.151963\n",
      "Train_EnvstepsSoFar : 35001\n",
      "Train_AverageReturn : -194.19499759777213\n",
      "Train_BestReturn : -193.4656844331411\n",
      "TimeSinceStart : 164.1519627571106\n",
      "Training Loss : 0.4325931966304779\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 36000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 36001\n",
      "mean reward (100 episodes) -194.159908\n",
      "best mean reward -193.465684\n",
      "running time 169.133415\n",
      "Train_EnvstepsSoFar : 36001\n",
      "Train_AverageReturn : -194.15990782670087\n",
      "Train_BestReturn : -193.4656844331411\n",
      "TimeSinceStart : 169.1334149837494\n",
      "Training Loss : 1.0132067203521729\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 37000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 37001\n",
      "mean reward (100 episodes) -193.525744\n",
      "best mean reward -193.465684\n",
      "running time 173.842696\n",
      "Train_EnvstepsSoFar : 37001\n",
      "Train_AverageReturn : -193.5257443091231\n",
      "Train_BestReturn : -193.4656844331411\n",
      "TimeSinceStart : 173.8426959514618\n",
      "Training Loss : 0.5728771090507507\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 38000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 38001\n",
      "mean reward (100 episodes) -194.010309\n",
      "best mean reward -193.465684\n",
      "running time 178.950007\n",
      "Train_EnvstepsSoFar : 38001\n",
      "Train_AverageReturn : -194.01030850874724\n",
      "Train_BestReturn : -193.4656844331411\n",
      "TimeSinceStart : 178.9500069618225\n",
      "Training Loss : 3.7469165325164795\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 39000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 39001\n",
      "mean reward (100 episodes) -195.008247\n",
      "best mean reward -193.465684\n",
      "running time 183.854392\n",
      "Train_EnvstepsSoFar : 39001\n",
      "Train_AverageReturn : -195.0082467059253\n",
      "Train_BestReturn : -193.4656844331411\n",
      "TimeSinceStart : 183.85439205169678\n",
      "Training Loss : 0.5315822958946228\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 40001\n",
      "mean reward (100 episodes) -193.847827\n",
      "best mean reward -193.465684\n",
      "running time 188.703408\n",
      "Train_EnvstepsSoFar : 40001\n",
      "Train_AverageReturn : -193.84782684739093\n",
      "Train_BestReturn : -193.4656844331411\n",
      "TimeSinceStart : 188.70340776443481\n",
      "Training Loss : 0.44548916816711426\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 41000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 41001\n",
      "mean reward (100 episodes) -184.893168\n",
      "best mean reward -184.893168\n",
      "running time 193.340150\n",
      "Train_EnvstepsSoFar : 41001\n",
      "Train_AverageReturn : -184.89316753941898\n",
      "Train_BestReturn : -184.89316753941898\n",
      "TimeSinceStart : 193.34014987945557\n",
      "Training Loss : 0.951633870601654\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 42000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 42001\n",
      "mean reward (100 episodes) -184.429775\n",
      "best mean reward -184.429775\n",
      "running time 197.816173\n",
      "Train_EnvstepsSoFar : 42001\n",
      "Train_AverageReturn : -184.42977524381328\n",
      "Train_BestReturn : -184.42977524381328\n",
      "TimeSinceStart : 197.81617307662964\n",
      "Training Loss : 0.7010585069656372\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 43000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 43001\n",
      "mean reward (100 episodes) -181.750515\n",
      "best mean reward -181.750515\n",
      "running time 203.272739\n",
      "Train_EnvstepsSoFar : 43001\n",
      "Train_AverageReturn : -181.75051510895813\n",
      "Train_BestReturn : -181.75051510895813\n",
      "TimeSinceStart : 203.27273893356323\n",
      "Training Loss : 0.295045405626297\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 44000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 44001\n",
      "mean reward (100 episodes) -178.452483\n",
      "best mean reward -178.452483\n",
      "running time 208.343402\n",
      "Train_EnvstepsSoFar : 44001\n",
      "Train_AverageReturn : -178.45248330012586\n",
      "Train_BestReturn : -178.45248330012586\n",
      "TimeSinceStart : 208.3434019088745\n",
      "Training Loss : 0.513253390789032\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 45000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 45001\n",
      "mean reward (100 episodes) -178.152212\n",
      "best mean reward -178.152212\n",
      "running time 214.358650\n",
      "Train_EnvstepsSoFar : 45001\n",
      "Train_AverageReturn : -178.1522118803098\n",
      "Train_BestReturn : -178.1522118803098\n",
      "TimeSinceStart : 214.35864996910095\n",
      "Training Loss : 0.4385693073272705\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 46000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 46001\n",
      "mean reward (100 episodes) -171.024686\n",
      "best mean reward -171.024686\n",
      "running time 219.908688\n",
      "Train_EnvstepsSoFar : 46001\n",
      "Train_AverageReturn : -171.0246862056383\n",
      "Train_BestReturn : -171.0246862056383\n",
      "TimeSinceStart : 219.9086878299713\n",
      "Training Loss : 3.0320754051208496\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 47000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 47001\n",
      "mean reward (100 episodes) -169.350078\n",
      "best mean reward -169.350078\n",
      "running time 224.242533\n",
      "Train_EnvstepsSoFar : 47001\n",
      "Train_AverageReturn : -169.35007810553793\n",
      "Train_BestReturn : -169.35007810553793\n",
      "TimeSinceStart : 224.24253296852112\n",
      "Training Loss : 0.3704071044921875\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 48000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 48001\n",
      "mean reward (100 episodes) -165.843853\n",
      "best mean reward -165.843853\n",
      "running time 229.305529\n",
      "Train_EnvstepsSoFar : 48001\n",
      "Train_AverageReturn : -165.8438529371687\n",
      "Train_BestReturn : -165.8438529371687\n",
      "TimeSinceStart : 229.30552911758423\n",
      "Training Loss : 0.9207974076271057\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 49000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 49001\n",
      "mean reward (100 episodes) -165.894237\n",
      "best mean reward -165.843853\n",
      "running time 236.209051\n",
      "Train_EnvstepsSoFar : 49001\n",
      "Train_AverageReturn : -165.8942370187637\n",
      "Train_BestReturn : -165.8438529371687\n",
      "TimeSinceStart : 236.20905089378357\n",
      "Training Loss : 3.0575313568115234\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 50001\n",
      "mean reward (100 episodes) -166.111594\n",
      "best mean reward -165.843853\n",
      "running time 242.947960\n",
      "Train_EnvstepsSoFar : 50001\n",
      "Train_AverageReturn : -166.11159384521525\n",
      "Train_BestReturn : -165.8438529371687\n",
      "TimeSinceStart : 242.94796013832092\n",
      "Training Loss : 0.4355700612068176\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 51000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 51001\n",
      "mean reward (100 episodes) -162.763783\n",
      "best mean reward -162.763783\n",
      "running time 248.486651\n",
      "Train_EnvstepsSoFar : 51001\n",
      "Train_AverageReturn : -162.76378295503304\n",
      "Train_BestReturn : -162.76378295503304\n",
      "TimeSinceStart : 248.4866509437561\n",
      "Training Loss : 0.4897962510585785\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 52000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 52001\n",
      "mean reward (100 episodes) -163.291310\n",
      "best mean reward -162.763783\n",
      "running time 253.686782\n",
      "Train_EnvstepsSoFar : 52001\n",
      "Train_AverageReturn : -163.29131008747726\n",
      "Train_BestReturn : -162.76378295503304\n",
      "TimeSinceStart : 253.68678188323975\n",
      "Training Loss : 0.46097850799560547\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 53000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 53001\n",
      "mean reward (100 episodes) -159.016278\n",
      "best mean reward -159.016278\n",
      "running time 258.131171\n",
      "Train_EnvstepsSoFar : 53001\n",
      "Train_AverageReturn : -159.01627751336184\n",
      "Train_BestReturn : -159.01627751336184\n",
      "TimeSinceStart : 258.1311709880829\n",
      "Training Loss : 2.2130959033966064\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 54000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 54001\n",
      "mean reward (100 episodes) -158.461821\n",
      "best mean reward -158.461821\n",
      "running time 263.021059\n",
      "Train_EnvstepsSoFar : 54001\n",
      "Train_AverageReturn : -158.4618208652916\n",
      "Train_BestReturn : -158.4618208652916\n",
      "TimeSinceStart : 263.0210590362549\n",
      "Training Loss : 1.2023415565490723\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 55000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 55001\n",
      "mean reward (100 episodes) -156.162049\n",
      "best mean reward -156.162049\n",
      "running time 267.901522\n",
      "Train_EnvstepsSoFar : 55001\n",
      "Train_AverageReturn : -156.16204922711074\n",
      "Train_BestReturn : -156.16204922711074\n",
      "TimeSinceStart : 267.90152192115784\n",
      "Training Loss : 0.3328304886817932\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 56000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 56001\n",
      "mean reward (100 episodes) -156.239659\n",
      "best mean reward -156.162049\n",
      "running time 274.332889\n",
      "Train_EnvstepsSoFar : 56001\n",
      "Train_AverageReturn : -156.23965948311832\n",
      "Train_BestReturn : -156.16204922711074\n",
      "TimeSinceStart : 274.3328890800476\n",
      "Training Loss : 1.4393783807754517\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 57000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 57001\n",
      "mean reward (100 episodes) -152.107192\n",
      "best mean reward -152.107192\n",
      "running time 285.480686\n",
      "Train_EnvstepsSoFar : 57001\n",
      "Train_AverageReturn : -152.10719187379635\n",
      "Train_BestReturn : -152.10719187379635\n",
      "TimeSinceStart : 285.48068594932556\n",
      "Training Loss : 0.28759875893592834\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 58000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 58001\n",
      "mean reward (100 episodes) -154.901714\n",
      "best mean reward -152.107192\n",
      "running time 292.017483\n",
      "Train_EnvstepsSoFar : 58001\n",
      "Train_AverageReturn : -154.9017143605787\n",
      "Train_BestReturn : -152.10719187379635\n",
      "TimeSinceStart : 292.01748299598694\n",
      "Training Loss : 1.0822362899780273\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 59000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 59001\n",
      "mean reward (100 episodes) -148.163704\n",
      "best mean reward -148.163704\n",
      "running time 297.451658\n",
      "Train_EnvstepsSoFar : 59001\n",
      "Train_AverageReturn : -148.1637042129103\n",
      "Train_BestReturn : -148.1637042129103\n",
      "TimeSinceStart : 297.4516580104828\n",
      "Training Loss : 0.339290589094162\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 60001\n",
      "mean reward (100 episodes) -144.958344\n",
      "best mean reward -144.958344\n",
      "running time 306.032157\n",
      "Train_EnvstepsSoFar : 60001\n",
      "Train_AverageReturn : -144.9583438010291\n",
      "Train_BestReturn : -144.9583438010291\n",
      "TimeSinceStart : 306.0321569442749\n",
      "Training Loss : 0.2764451503753662\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 61000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 61001\n",
      "mean reward (100 episodes) -142.944531\n",
      "best mean reward -142.944531\n",
      "running time 315.395435\n",
      "Train_EnvstepsSoFar : 61001\n",
      "Train_AverageReturn : -142.94453105482734\n",
      "Train_BestReturn : -142.94453105482734\n",
      "TimeSinceStart : 315.3954348564148\n",
      "Training Loss : 0.42528554797172546\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 62000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 62001\n",
      "mean reward (100 episodes) -138.636425\n",
      "best mean reward -138.636425\n",
      "running time 321.171431\n",
      "Train_EnvstepsSoFar : 62001\n",
      "Train_AverageReturn : -138.63642478307082\n",
      "Train_BestReturn : -138.63642478307082\n",
      "TimeSinceStart : 321.17143082618713\n",
      "Training Loss : 2.499194860458374\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 63000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 63001\n",
      "mean reward (100 episodes) -138.655618\n",
      "best mean reward -138.636425\n",
      "running time 328.271658\n",
      "Train_EnvstepsSoFar : 63001\n",
      "Train_AverageReturn : -138.65561846599635\n",
      "Train_BestReturn : -138.63642478307082\n",
      "TimeSinceStart : 328.2716579437256\n",
      "Training Loss : 0.5466467142105103\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 64000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 64001\n",
      "mean reward (100 episodes) -137.961136\n",
      "best mean reward -137.961136\n",
      "running time 334.242108\n",
      "Train_EnvstepsSoFar : 64001\n",
      "Train_AverageReturn : -137.96113608534304\n",
      "Train_BestReturn : -137.96113608534304\n",
      "TimeSinceStart : 334.2421078681946\n",
      "Training Loss : 0.37457510828971863\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 65000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 65001\n",
      "mean reward (100 episodes) -133.404094\n",
      "best mean reward -133.404094\n",
      "running time 339.826466\n",
      "Train_EnvstepsSoFar : 65001\n",
      "Train_AverageReturn : -133.4040937374207\n",
      "Train_BestReturn : -133.4040937374207\n",
      "TimeSinceStart : 339.8264660835266\n",
      "Training Loss : 0.7806757688522339\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 66000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 66001\n",
      "mean reward (100 episodes) -130.446549\n",
      "best mean reward -130.446549\n",
      "running time 346.445589\n",
      "Train_EnvstepsSoFar : 66001\n",
      "Train_AverageReturn : -130.4465490782374\n",
      "Train_BestReturn : -130.4465490782374\n",
      "TimeSinceStart : 346.44558906555176\n",
      "Training Loss : 0.29139989614486694\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 67000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 67001\n",
      "mean reward (100 episodes) -129.396502\n",
      "best mean reward -129.396502\n",
      "running time 353.440817\n",
      "Train_EnvstepsSoFar : 67001\n",
      "Train_AverageReturn : -129.3965021382186\n",
      "Train_BestReturn : -129.3965021382186\n",
      "TimeSinceStart : 353.44081687927246\n",
      "Training Loss : 0.9827867746353149\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 68000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 68001\n",
      "mean reward (100 episodes) -128.893449\n",
      "best mean reward -128.893449\n",
      "running time 360.792584\n",
      "Train_EnvstepsSoFar : 68001\n",
      "Train_AverageReturn : -128.8934492559585\n",
      "Train_BestReturn : -128.8934492559585\n",
      "TimeSinceStart : 360.79258394241333\n",
      "Training Loss : 0.4330836236476898\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 69000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 69001\n",
      "mean reward (100 episodes) -128.267895\n",
      "best mean reward -128.267895\n",
      "running time 369.328057\n",
      "Train_EnvstepsSoFar : 69001\n",
      "Train_AverageReturn : -128.2678952306535\n",
      "Train_BestReturn : -128.2678952306535\n",
      "TimeSinceStart : 369.32805705070496\n",
      "Training Loss : 1.3513175249099731\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 70001\n",
      "mean reward (100 episodes) -129.298234\n",
      "best mean reward -128.267895\n",
      "running time 376.759669\n",
      "Train_EnvstepsSoFar : 70001\n",
      "Train_AverageReturn : -129.2982344777865\n",
      "Train_BestReturn : -128.2678952306535\n",
      "TimeSinceStart : 376.75966906547546\n",
      "Training Loss : 0.3563244938850403\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 71000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 71001\n",
      "mean reward (100 episodes) -123.334095\n",
      "best mean reward -123.334095\n",
      "running time 381.119341\n",
      "Train_EnvstepsSoFar : 71001\n",
      "Train_AverageReturn : -123.33409526016884\n",
      "Train_BestReturn : -123.33409526016884\n",
      "TimeSinceStart : 381.119341135025\n",
      "Training Loss : 0.3172609210014343\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 72000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 72001\n",
      "mean reward (100 episodes) -122.463125\n",
      "best mean reward -122.463125\n",
      "running time 385.958543\n",
      "Train_EnvstepsSoFar : 72001\n",
      "Train_AverageReturn : -122.4631246247395\n",
      "Train_BestReturn : -122.4631246247395\n",
      "TimeSinceStart : 385.9585430622101\n",
      "Training Loss : 0.21457025408744812\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 73000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 73001\n",
      "mean reward (100 episodes) -124.839617\n",
      "best mean reward -122.463125\n",
      "running time 391.068582\n",
      "Train_EnvstepsSoFar : 73001\n",
      "Train_AverageReturn : -124.83961740757765\n",
      "Train_BestReturn : -122.4631246247395\n",
      "TimeSinceStart : 391.0685818195343\n",
      "Training Loss : 0.15268951654434204\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 74000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 74001\n",
      "mean reward (100 episodes) -122.023757\n",
      "best mean reward -122.023757\n",
      "running time 396.377748\n",
      "Train_EnvstepsSoFar : 74001\n",
      "Train_AverageReturn : -122.02375695555565\n",
      "Train_BestReturn : -122.02375695555565\n",
      "TimeSinceStart : 396.3777480125427\n",
      "Training Loss : 0.32231977581977844\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 75000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 75001\n",
      "mean reward (100 episodes) -120.559218\n",
      "best mean reward -120.559218\n",
      "running time 403.105054\n",
      "Train_EnvstepsSoFar : 75001\n",
      "Train_AverageReturn : -120.55921801596263\n",
      "Train_BestReturn : -120.55921801596263\n",
      "TimeSinceStart : 403.10505390167236\n",
      "Training Loss : 0.3805074691772461\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 76000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 76001\n",
      "mean reward (100 episodes) -116.392830\n",
      "best mean reward -116.392830\n",
      "running time 408.498968\n",
      "Train_EnvstepsSoFar : 76001\n",
      "Train_AverageReturn : -116.39282979703584\n",
      "Train_BestReturn : -116.39282979703584\n",
      "TimeSinceStart : 408.49896812438965\n",
      "Training Loss : 0.34361082315444946\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 77000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 77001\n",
      "mean reward (100 episodes) -114.665180\n",
      "best mean reward -114.665180\n",
      "running time 413.158519\n",
      "Train_EnvstepsSoFar : 77001\n",
      "Train_AverageReturn : -114.66518029757064\n",
      "Train_BestReturn : -114.66518029757064\n",
      "TimeSinceStart : 413.1585190296173\n",
      "Training Loss : 1.1336708068847656\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 78000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 78001\n",
      "mean reward (100 episodes) -114.114122\n",
      "best mean reward -114.114122\n",
      "running time 418.076262\n",
      "Train_EnvstepsSoFar : 78001\n",
      "Train_AverageReturn : -114.11412165082085\n",
      "Train_BestReturn : -114.11412165082085\n",
      "TimeSinceStart : 418.0762617588043\n",
      "Training Loss : 0.27916255593299866\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 79000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 79001\n",
      "mean reward (100 episodes) -109.678662\n",
      "best mean reward -109.678662\n",
      "running time 422.254420\n",
      "Train_EnvstepsSoFar : 79001\n",
      "Train_AverageReturn : -109.6786622328359\n",
      "Train_BestReturn : -109.6786622328359\n",
      "TimeSinceStart : 422.25442004203796\n",
      "Training Loss : 0.21908144652843475\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 80001\n",
      "mean reward (100 episodes) -104.341205\n",
      "best mean reward -104.341205\n",
      "running time 426.428678\n",
      "Train_EnvstepsSoFar : 80001\n",
      "Train_AverageReturn : -104.34120455301021\n",
      "Train_BestReturn : -104.34120455301021\n",
      "TimeSinceStart : 426.4286777973175\n",
      "Training Loss : 0.32520201802253723\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 81000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 81001\n",
      "mean reward (100 episodes) -100.839783\n",
      "best mean reward -100.839783\n",
      "running time 431.363592\n",
      "Train_EnvstepsSoFar : 81001\n",
      "Train_AverageReturn : -100.83978251494435\n",
      "Train_BestReturn : -100.83978251494435\n",
      "TimeSinceStart : 431.36359190940857\n",
      "Training Loss : 0.3980766236782074\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 82000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 82001\n",
      "mean reward (100 episodes) -94.222755\n",
      "best mean reward -94.222755\n",
      "running time 435.586519\n",
      "Train_EnvstepsSoFar : 82001\n",
      "Train_AverageReturn : -94.22275457112404\n",
      "Train_BestReturn : -94.22275457112404\n",
      "TimeSinceStart : 435.58651900291443\n",
      "Training Loss : 0.26245665550231934\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 83000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 83001\n",
      "mean reward (100 episodes) -95.054838\n",
      "best mean reward -94.222755\n",
      "running time 440.399807\n",
      "Train_EnvstepsSoFar : 83001\n",
      "Train_AverageReturn : -95.05483796371142\n",
      "Train_BestReturn : -94.22275457112404\n",
      "TimeSinceStart : 440.39980697631836\n",
      "Training Loss : 0.3220731317996979\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 84000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 84001\n",
      "mean reward (100 episodes) -95.951078\n",
      "best mean reward -94.222755\n",
      "running time 444.898312\n",
      "Train_EnvstepsSoFar : 84001\n",
      "Train_AverageReturn : -95.95107775876626\n",
      "Train_BestReturn : -94.22275457112404\n",
      "TimeSinceStart : 444.8983118534088\n",
      "Training Loss : 0.17531649768352509\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 85000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 85001\n",
      "mean reward (100 episodes) -89.139250\n",
      "best mean reward -89.139250\n",
      "running time 449.468559\n",
      "Train_EnvstepsSoFar : 85001\n",
      "Train_AverageReturn : -89.13924972866096\n",
      "Train_BestReturn : -89.13924972866096\n",
      "TimeSinceStart : 449.46855902671814\n",
      "Training Loss : 0.19883449375629425\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 86000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 86001\n",
      "mean reward (100 episodes) -85.570402\n",
      "best mean reward -85.570402\n",
      "running time 453.577600\n",
      "Train_EnvstepsSoFar : 86001\n",
      "Train_AverageReturn : -85.57040173237489\n",
      "Train_BestReturn : -85.57040173237489\n",
      "TimeSinceStart : 453.5776000022888\n",
      "Training Loss : 0.18861745297908783\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 87000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 87001\n",
      "mean reward (100 episodes) -86.418797\n",
      "best mean reward -85.570402\n",
      "running time 457.873509\n",
      "Train_EnvstepsSoFar : 87001\n",
      "Train_AverageReturn : -86.41879664527397\n",
      "Train_BestReturn : -85.57040173237489\n",
      "TimeSinceStart : 457.8735091686249\n",
      "Training Loss : 0.29239621758461\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 88000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 88001\n",
      "mean reward (100 episodes) -83.353286\n",
      "best mean reward -83.353286\n",
      "running time 462.310347\n",
      "Train_EnvstepsSoFar : 88001\n",
      "Train_AverageReturn : -83.35328572454827\n",
      "Train_BestReturn : -83.35328572454827\n",
      "TimeSinceStart : 462.3103470802307\n",
      "Training Loss : 0.40402737259864807\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 89000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 89001\n",
      "mean reward (100 episodes) -80.977610\n",
      "best mean reward -80.977610\n",
      "running time 466.754385\n",
      "Train_EnvstepsSoFar : 89001\n",
      "Train_AverageReturn : -80.97760962888147\n",
      "Train_BestReturn : -80.97760962888147\n",
      "TimeSinceStart : 466.75438475608826\n",
      "Training Loss : 0.2749120891094208\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 90001\n",
      "mean reward (100 episodes) -81.806956\n",
      "best mean reward -80.977610\n",
      "running time 472.236373\n",
      "Train_EnvstepsSoFar : 90001\n",
      "Train_AverageReturn : -81.80695610522996\n",
      "Train_BestReturn : -80.97760962888147\n",
      "TimeSinceStart : 472.23637294769287\n",
      "Training Loss : 0.4564530849456787\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 91000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 91001\n",
      "mean reward (100 episodes) -81.530409\n",
      "best mean reward -80.977610\n",
      "running time 477.192786\n",
      "Train_EnvstepsSoFar : 91001\n",
      "Train_AverageReturn : -81.53040938950265\n",
      "Train_BestReturn : -80.97760962888147\n",
      "TimeSinceStart : 477.19278597831726\n",
      "Training Loss : 0.1528700590133667\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 92000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 92001\n",
      "mean reward (100 episodes) -79.449821\n",
      "best mean reward -79.449821\n",
      "running time 481.726795\n",
      "Train_EnvstepsSoFar : 92001\n",
      "Train_AverageReturn : -79.44982128281075\n",
      "Train_BestReturn : -79.44982128281075\n",
      "TimeSinceStart : 481.7267949581146\n",
      "Training Loss : 0.21660611033439636\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 93000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 93001\n",
      "mean reward (100 episodes) -75.965008\n",
      "best mean reward -75.965008\n",
      "running time 486.413548\n",
      "Train_EnvstepsSoFar : 93001\n",
      "Train_AverageReturn : -75.96500762587192\n",
      "Train_BestReturn : -75.96500762587192\n",
      "TimeSinceStart : 486.4135479927063\n",
      "Training Loss : 0.8068925738334656\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 94000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 94001\n",
      "mean reward (100 episodes) -74.248249\n",
      "best mean reward -74.248249\n",
      "running time 490.973839\n",
      "Train_EnvstepsSoFar : 94001\n",
      "Train_AverageReturn : -74.24824922229938\n",
      "Train_BestReturn : -74.24824922229938\n",
      "TimeSinceStart : 490.9738390445709\n",
      "Training Loss : 0.30900654196739197\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 95000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 95001\n",
      "mean reward (100 episodes) -73.432764\n",
      "best mean reward -73.432764\n",
      "running time 495.686236\n",
      "Train_EnvstepsSoFar : 95001\n",
      "Train_AverageReturn : -73.43276394449431\n",
      "Train_BestReturn : -73.43276394449431\n",
      "TimeSinceStart : 495.6862359046936\n",
      "Training Loss : 3.098863124847412\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 96000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 96001\n",
      "mean reward (100 episodes) -72.067463\n",
      "best mean reward -72.067463\n",
      "running time 500.052391\n",
      "Train_EnvstepsSoFar : 96001\n",
      "Train_AverageReturn : -72.06746341744687\n",
      "Train_BestReturn : -72.06746341744687\n",
      "TimeSinceStart : 500.0523908138275\n",
      "Training Loss : 0.21722599864006042\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 97000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 97001\n",
      "mean reward (100 episodes) -71.742623\n",
      "best mean reward -71.742623\n",
      "running time 504.293067\n",
      "Train_EnvstepsSoFar : 97001\n",
      "Train_AverageReturn : -71.74262300409819\n",
      "Train_BestReturn : -71.74262300409819\n",
      "TimeSinceStart : 504.2930669784546\n",
      "Training Loss : 0.15133684873580933\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 98000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 98001\n",
      "mean reward (100 episodes) -68.296001\n",
      "best mean reward -68.296001\n",
      "running time 508.453483\n",
      "Train_EnvstepsSoFar : 98001\n",
      "Train_AverageReturn : -68.29600120820844\n",
      "Train_BestReturn : -68.29600120820844\n",
      "TimeSinceStart : 508.45348286628723\n",
      "Training Loss : 0.7942405343055725\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 99000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 99001\n",
      "mean reward (100 episodes) -67.502412\n",
      "best mean reward -67.502412\n",
      "running time 512.940071\n",
      "Train_EnvstepsSoFar : 99001\n",
      "Train_AverageReturn : -67.50241199957817\n",
      "Train_BestReturn : -67.50241199957817\n",
      "TimeSinceStart : 512.9400708675385\n",
      "Training Loss : 0.19102634489536285\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 100001\n",
      "mean reward (100 episodes) -62.783904\n",
      "best mean reward -62.783904\n",
      "running time 517.075284\n",
      "Train_EnvstepsSoFar : 100001\n",
      "Train_AverageReturn : -62.78390433809617\n",
      "Train_BestReturn : -62.78390433809617\n",
      "TimeSinceStart : 517.0752840042114\n",
      "Training Loss : 0.17876671254634857\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 101001\n",
      "mean reward (100 episodes) -61.621914\n",
      "best mean reward -61.621914\n",
      "running time 521.312409\n",
      "Train_EnvstepsSoFar : 101001\n",
      "Train_AverageReturn : -61.621914105930855\n",
      "Train_BestReturn : -61.621914105930855\n",
      "TimeSinceStart : 521.3124091625214\n",
      "Training Loss : 3.431318998336792\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 102001\n",
      "mean reward (100 episodes) -57.786703\n",
      "best mean reward -57.786703\n",
      "running time 526.033768\n",
      "Train_EnvstepsSoFar : 102001\n",
      "Train_AverageReturn : -57.78670252286871\n",
      "Train_BestReturn : -57.78670252286871\n",
      "TimeSinceStart : 526.0337679386139\n",
      "Training Loss : 0.15783929824829102\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 103001\n",
      "mean reward (100 episodes) -58.006098\n",
      "best mean reward -57.786703\n",
      "running time 530.304137\n",
      "Train_EnvstepsSoFar : 103001\n",
      "Train_AverageReturn : -58.00609847988462\n",
      "Train_BestReturn : -57.78670252286871\n",
      "TimeSinceStart : 530.3041369915009\n",
      "Training Loss : 0.13299252092838287\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 104001\n",
      "mean reward (100 episodes) -54.840051\n",
      "best mean reward -54.840051\n",
      "running time 534.243999\n",
      "Train_EnvstepsSoFar : 104001\n",
      "Train_AverageReturn : -54.84005065419609\n",
      "Train_BestReturn : -54.84005065419609\n",
      "TimeSinceStart : 534.243999004364\n",
      "Training Loss : 0.11238609999418259\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 105001\n",
      "mean reward (100 episodes) -51.724974\n",
      "best mean reward -51.724974\n",
      "running time 539.023844\n",
      "Train_EnvstepsSoFar : 105001\n",
      "Train_AverageReturn : -51.7249741506043\n",
      "Train_BestReturn : -51.7249741506043\n",
      "TimeSinceStart : 539.0238440036774\n",
      "Training Loss : 0.2492205798625946\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 106001\n",
      "mean reward (100 episodes) -51.446098\n",
      "best mean reward -51.446098\n",
      "running time 544.417237\n",
      "Train_EnvstepsSoFar : 106001\n",
      "Train_AverageReturn : -51.44609844724514\n",
      "Train_BestReturn : -51.44609844724514\n",
      "TimeSinceStart : 544.4172370433807\n",
      "Training Loss : 0.11301222443580627\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 107001\n",
      "mean reward (100 episodes) -43.786751\n",
      "best mean reward -43.786751\n",
      "running time 548.441081\n",
      "Train_EnvstepsSoFar : 107001\n",
      "Train_AverageReturn : -43.78675074820081\n",
      "Train_BestReturn : -43.78675074820081\n",
      "TimeSinceStart : 548.4410810470581\n",
      "Training Loss : 0.18552266061306\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 108001\n",
      "mean reward (100 episodes) -44.270229\n",
      "best mean reward -43.786751\n",
      "running time 558.282304\n",
      "Train_EnvstepsSoFar : 108001\n",
      "Train_AverageReturn : -44.27022919220976\n",
      "Train_BestReturn : -43.78675074820081\n",
      "TimeSinceStart : 558.2823040485382\n",
      "Training Loss : 0.21593137085437775\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 109001\n",
      "mean reward (100 episodes) -44.664730\n",
      "best mean reward -43.786751\n",
      "running time 567.411525\n",
      "Train_EnvstepsSoFar : 109001\n",
      "Train_AverageReturn : -44.66473042235596\n",
      "Train_BestReturn : -43.78675074820081\n",
      "TimeSinceStart : 567.411524772644\n",
      "Training Loss : 0.20420609414577484\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 110001\n",
      "mean reward (100 episodes) -43.211142\n",
      "best mean reward -43.211142\n",
      "running time 574.705217\n",
      "Train_EnvstepsSoFar : 110001\n",
      "Train_AverageReturn : -43.211141636158345\n",
      "Train_BestReturn : -43.211141636158345\n",
      "TimeSinceStart : 574.705216884613\n",
      "Training Loss : 0.1892005205154419\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 111001\n",
      "mean reward (100 episodes) -41.229180\n",
      "best mean reward -41.229180\n",
      "running time 585.228445\n",
      "Train_EnvstepsSoFar : 111001\n",
      "Train_AverageReturn : -41.22918048006955\n",
      "Train_BestReturn : -41.22918048006955\n",
      "TimeSinceStart : 585.228444814682\n",
      "Training Loss : 0.09409227222204208\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 112001\n",
      "mean reward (100 episodes) -39.623195\n",
      "best mean reward -39.623195\n",
      "running time 593.111243\n",
      "Train_EnvstepsSoFar : 112001\n",
      "Train_AverageReturn : -39.62319479332592\n",
      "Train_BestReturn : -39.62319479332592\n",
      "TimeSinceStart : 593.1112430095673\n",
      "Training Loss : 0.6446453332901001\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 113001\n",
      "mean reward (100 episodes) -37.310919\n",
      "best mean reward -37.310919\n",
      "running time 601.413263\n",
      "Train_EnvstepsSoFar : 113001\n",
      "Train_AverageReturn : -37.31091886092061\n",
      "Train_BestReturn : -37.31091886092061\n",
      "TimeSinceStart : 601.4132630825043\n",
      "Training Loss : 0.1795843243598938\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 114001\n",
      "mean reward (100 episodes) -37.246648\n",
      "best mean reward -37.246648\n",
      "running time 606.145754\n",
      "Train_EnvstepsSoFar : 114001\n",
      "Train_AverageReturn : -37.24664776572256\n",
      "Train_BestReturn : -37.24664776572256\n",
      "TimeSinceStart : 606.1457540988922\n",
      "Training Loss : 0.16070938110351562\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 115001\n",
      "mean reward (100 episodes) -33.762929\n",
      "best mean reward -33.762929\n",
      "running time 611.004090\n",
      "Train_EnvstepsSoFar : 115001\n",
      "Train_AverageReturn : -33.76292948739051\n",
      "Train_BestReturn : -33.76292948739051\n",
      "TimeSinceStart : 611.0040900707245\n",
      "Training Loss : 1.2065467834472656\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 116001\n",
      "mean reward (100 episodes) -28.094270\n",
      "best mean reward -28.094270\n",
      "running time 617.304827\n",
      "Train_EnvstepsSoFar : 116001\n",
      "Train_AverageReturn : -28.09426964474275\n",
      "Train_BestReturn : -28.09426964474275\n",
      "TimeSinceStart : 617.3048267364502\n",
      "Training Loss : 0.3518664836883545\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 117001\n",
      "mean reward (100 episodes) -25.603293\n",
      "best mean reward -25.603293\n",
      "running time 623.215246\n",
      "Train_EnvstepsSoFar : 117001\n",
      "Train_AverageReturn : -25.60329315404577\n",
      "Train_BestReturn : -25.60329315404577\n",
      "TimeSinceStart : 623.215245962143\n",
      "Training Loss : 0.1361652910709381\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 118001\n",
      "mean reward (100 episodes) -25.802724\n",
      "best mean reward -25.603293\n",
      "running time 627.927263\n",
      "Train_EnvstepsSoFar : 118001\n",
      "Train_AverageReturn : -25.802723711640514\n",
      "Train_BestReturn : -25.60329315404577\n",
      "TimeSinceStart : 627.9272630214691\n",
      "Training Loss : 0.12832263112068176\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 119001\n",
      "mean reward (100 episodes) -23.266405\n",
      "best mean reward -23.266405\n",
      "running time 632.493187\n",
      "Train_EnvstepsSoFar : 119001\n",
      "Train_AverageReturn : -23.26640505200635\n",
      "Train_BestReturn : -23.26640505200635\n",
      "TimeSinceStart : 632.4931869506836\n",
      "Training Loss : 0.6504656076431274\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 120001\n",
      "mean reward (100 episodes) -18.707848\n",
      "best mean reward -18.707848\n",
      "running time 638.225648\n",
      "Train_EnvstepsSoFar : 120001\n",
      "Train_AverageReturn : -18.707847626295937\n",
      "Train_BestReturn : -18.707847626295937\n",
      "TimeSinceStart : 638.2256479263306\n",
      "Training Loss : 2.982664108276367\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 121001\n",
      "mean reward (100 episodes) -11.552655\n",
      "best mean reward -11.552655\n",
      "running time 642.383898\n",
      "Train_EnvstepsSoFar : 121001\n",
      "Train_AverageReturn : -11.552655492827707\n",
      "Train_BestReturn : -11.552655492827707\n",
      "TimeSinceStart : 642.3838980197906\n",
      "Training Loss : 0.0838872641324997\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 122001\n",
      "mean reward (100 episodes) -11.562427\n",
      "best mean reward -11.552655\n",
      "running time 646.822560\n",
      "Train_EnvstepsSoFar : 122001\n",
      "Train_AverageReturn : -11.562426894237914\n",
      "Train_BestReturn : -11.552655492827707\n",
      "TimeSinceStart : 646.8225600719452\n",
      "Training Loss : 0.1387927085161209\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 123001\n",
      "mean reward (100 episodes) -10.037478\n",
      "best mean reward -10.037478\n",
      "running time 652.708761\n",
      "Train_EnvstepsSoFar : 123001\n",
      "Train_AverageReturn : -10.037477944280875\n",
      "Train_BestReturn : -10.037477944280875\n",
      "TimeSinceStart : 652.7087609767914\n",
      "Training Loss : 0.44222044944763184\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 124001\n",
      "mean reward (100 episodes) -9.297227\n",
      "best mean reward -9.297227\n",
      "running time 661.017105\n",
      "Train_EnvstepsSoFar : 124001\n",
      "Train_AverageReturn : -9.297227040237287\n",
      "Train_BestReturn : -9.297227040237287\n",
      "TimeSinceStart : 661.0171048641205\n",
      "Training Loss : 0.1010492742061615\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 125001\n",
      "mean reward (100 episodes) -8.453795\n",
      "best mean reward -8.453795\n",
      "running time 668.943679\n",
      "Train_EnvstepsSoFar : 125001\n",
      "Train_AverageReturn : -8.45379531427044\n",
      "Train_BestReturn : -8.45379531427044\n",
      "TimeSinceStart : 668.943678855896\n",
      "Training Loss : 0.1775946319103241\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 126001\n",
      "mean reward (100 episodes) -7.641064\n",
      "best mean reward -7.641064\n",
      "running time 675.674479\n",
      "Train_EnvstepsSoFar : 126001\n",
      "Train_AverageReturn : -7.641064443342923\n",
      "Train_BestReturn : -7.641064443342923\n",
      "TimeSinceStart : 675.674479007721\n",
      "Training Loss : 0.07836157828569412\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 127001\n",
      "mean reward (100 episodes) -2.531139\n",
      "best mean reward -2.531139\n",
      "running time 680.330649\n",
      "Train_EnvstepsSoFar : 127001\n",
      "Train_AverageReturn : -2.5311386665267297\n",
      "Train_BestReturn : -2.5311386665267297\n",
      "TimeSinceStart : 680.330649137497\n",
      "Training Loss : 0.5010187029838562\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 128001\n",
      "mean reward (100 episodes) 1.968910\n",
      "best mean reward 1.968910\n",
      "running time 684.558067\n",
      "Train_EnvstepsSoFar : 128001\n",
      "Train_AverageReturn : 1.9689099582116676\n",
      "Train_BestReturn : 1.9689099582116676\n",
      "TimeSinceStart : 684.5580668449402\n",
      "Training Loss : 0.1467663198709488\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 129001\n",
      "mean reward (100 episodes) 3.618463\n",
      "best mean reward 3.618463\n",
      "running time 689.343130\n",
      "Train_EnvstepsSoFar : 129001\n",
      "Train_AverageReturn : 3.6184631740100484\n",
      "Train_BestReturn : 3.6184631740100484\n",
      "TimeSinceStart : 689.3431301116943\n",
      "Training Loss : 0.177327498793602\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 130001\n",
      "mean reward (100 episodes) 8.750686\n",
      "best mean reward 8.750686\n",
      "running time 693.615366\n",
      "Train_EnvstepsSoFar : 130001\n",
      "Train_AverageReturn : 8.750686000995888\n",
      "Train_BestReturn : 8.750686000995888\n",
      "TimeSinceStart : 693.6153657436371\n",
      "Training Loss : 0.14056003093719482\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 131001\n",
      "mean reward (100 episodes) 10.993360\n",
      "best mean reward 10.993360\n",
      "running time 698.961270\n",
      "Train_EnvstepsSoFar : 131001\n",
      "Train_AverageReturn : 10.993360275558743\n",
      "Train_BestReturn : 10.993360275558743\n",
      "TimeSinceStart : 698.9612700939178\n",
      "Training Loss : 0.12979981303215027\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 132001\n",
      "mean reward (100 episodes) 13.736625\n",
      "best mean reward 13.736625\n",
      "running time 704.099602\n",
      "Train_EnvstepsSoFar : 132001\n",
      "Train_AverageReturn : 13.73662481414486\n",
      "Train_BestReturn : 13.73662481414486\n",
      "TimeSinceStart : 704.099601984024\n",
      "Training Loss : 0.11542963981628418\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 133001\n",
      "mean reward (100 episodes) 17.605595\n",
      "best mean reward 17.605595\n",
      "running time 708.891993\n",
      "Train_EnvstepsSoFar : 133001\n",
      "Train_AverageReturn : 17.605594556809752\n",
      "Train_BestReturn : 17.605594556809752\n",
      "TimeSinceStart : 708.8919930458069\n",
      "Training Loss : 0.2471742033958435\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 134001\n",
      "mean reward (100 episodes) 21.689829\n",
      "best mean reward 21.689829\n",
      "running time 713.768733\n",
      "Train_EnvstepsSoFar : 134001\n",
      "Train_AverageReturn : 21.689828899224036\n",
      "Train_BestReturn : 21.689828899224036\n",
      "TimeSinceStart : 713.7687327861786\n",
      "Training Loss : 0.42475828528404236\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 135001\n",
      "mean reward (100 episodes) 23.197573\n",
      "best mean reward 23.197573\n",
      "running time 718.610701\n",
      "Train_EnvstepsSoFar : 135001\n",
      "Train_AverageReturn : 23.197573374737498\n",
      "Train_BestReturn : 23.197573374737498\n",
      "TimeSinceStart : 718.610701084137\n",
      "Training Loss : 0.15426944196224213\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 136001\n",
      "mean reward (100 episodes) 22.059611\n",
      "best mean reward 23.197573\n",
      "running time 723.390793\n",
      "Train_EnvstepsSoFar : 136001\n",
      "Train_AverageReturn : 22.059610738035737\n",
      "Train_BestReturn : 23.197573374737498\n",
      "TimeSinceStart : 723.3907928466797\n",
      "Training Loss : 0.17720593512058258\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 137001\n",
      "mean reward (100 episodes) 23.590463\n",
      "best mean reward 23.590463\n",
      "running time 727.661979\n",
      "Train_EnvstepsSoFar : 137001\n",
      "Train_AverageReturn : 23.590463119031597\n",
      "Train_BestReturn : 23.590463119031597\n",
      "TimeSinceStart : 727.6619789600372\n",
      "Training Loss : 0.26715442538261414\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 138001\n",
      "mean reward (100 episodes) 27.167004\n",
      "best mean reward 27.167004\n",
      "running time 732.196004\n",
      "Train_EnvstepsSoFar : 138001\n",
      "Train_AverageReturn : 27.16700391994794\n",
      "Train_BestReturn : 27.16700391994794\n",
      "TimeSinceStart : 732.196004152298\n",
      "Training Loss : 0.07171683758497238\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 139001\n",
      "mean reward (100 episodes) 31.476575\n",
      "best mean reward 31.476575\n",
      "running time 736.323733\n",
      "Train_EnvstepsSoFar : 139001\n",
      "Train_AverageReturn : 31.476575180732056\n",
      "Train_BestReturn : 31.476575180732056\n",
      "TimeSinceStart : 736.3237328529358\n",
      "Training Loss : 0.7987120151519775\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 140001\n",
      "mean reward (100 episodes) 37.717623\n",
      "best mean reward 37.717623\n",
      "running time 740.310157\n",
      "Train_EnvstepsSoFar : 140001\n",
      "Train_AverageReturn : 37.717622803378994\n",
      "Train_BestReturn : 37.717622803378994\n",
      "TimeSinceStart : 740.3101570606232\n",
      "Training Loss : 0.20729315280914307\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 141001\n",
      "mean reward (100 episodes) 41.195740\n",
      "best mean reward 41.195740\n",
      "running time 744.355337\n",
      "Train_EnvstepsSoFar : 141001\n",
      "Train_AverageReturn : 41.19573980051339\n",
      "Train_BestReturn : 41.19573980051339\n",
      "TimeSinceStart : 744.3553369045258\n",
      "Training Loss : 0.11929908394813538\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 142001\n",
      "mean reward (100 episodes) 42.169069\n",
      "best mean reward 42.169069\n",
      "running time 749.341927\n",
      "Train_EnvstepsSoFar : 142001\n",
      "Train_AverageReturn : 42.16906886535652\n",
      "Train_BestReturn : 42.16906886535652\n",
      "TimeSinceStart : 749.3419268131256\n",
      "Training Loss : 0.06193796545267105\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 143001\n",
      "mean reward (100 episodes) 42.310433\n",
      "best mean reward 42.310433\n",
      "running time 753.827182\n",
      "Train_EnvstepsSoFar : 143001\n",
      "Train_AverageReturn : 42.310433490449356\n",
      "Train_BestReturn : 42.310433490449356\n",
      "TimeSinceStart : 753.8271820545197\n",
      "Training Loss : 0.17506957054138184\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 144001\n",
      "mean reward (100 episodes) 43.501181\n",
      "best mean reward 43.501181\n",
      "running time 758.075173\n",
      "Train_EnvstepsSoFar : 144001\n",
      "Train_AverageReturn : 43.501180516330706\n",
      "Train_BestReturn : 43.501180516330706\n",
      "TimeSinceStart : 758.0751731395721\n",
      "Training Loss : 0.10435141623020172\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 145001\n",
      "mean reward (100 episodes) 46.570251\n",
      "best mean reward 46.570251\n",
      "running time 762.007484\n",
      "Train_EnvstepsSoFar : 145001\n",
      "Train_AverageReturn : 46.5702510951995\n",
      "Train_BestReturn : 46.5702510951995\n",
      "TimeSinceStart : 762.007483959198\n",
      "Training Loss : 0.9870084524154663\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 146001\n",
      "mean reward (100 episodes) 52.179908\n",
      "best mean reward 52.179908\n",
      "running time 766.166070\n",
      "Train_EnvstepsSoFar : 146001\n",
      "Train_AverageReturn : 52.179908323898196\n",
      "Train_BestReturn : 52.179908323898196\n",
      "TimeSinceStart : 766.166069984436\n",
      "Training Loss : 1.8175920248031616\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 147001\n",
      "mean reward (100 episodes) 59.100309\n",
      "best mean reward 59.100309\n",
      "running time 770.490749\n",
      "Train_EnvstepsSoFar : 147001\n",
      "Train_AverageReturn : 59.10030856190542\n",
      "Train_BestReturn : 59.10030856190542\n",
      "TimeSinceStart : 770.4907488822937\n",
      "Training Loss : 0.20867612957954407\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 148001\n",
      "mean reward (100 episodes) 60.815173\n",
      "best mean reward 60.815173\n",
      "running time 774.985170\n",
      "Train_EnvstepsSoFar : 148001\n",
      "Train_AverageReturn : 60.81517266348683\n",
      "Train_BestReturn : 60.81517266348683\n",
      "TimeSinceStart : 774.9851698875427\n",
      "Training Loss : 0.10088504105806351\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 149001\n",
      "mean reward (100 episodes) 63.789636\n",
      "best mean reward 63.789636\n",
      "running time 779.131008\n",
      "Train_EnvstepsSoFar : 149001\n",
      "Train_AverageReturn : 63.789635644749524\n",
      "Train_BestReturn : 63.789635644749524\n",
      "TimeSinceStart : 779.1310079097748\n",
      "Training Loss : 3.2947468757629395\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 150001\n",
      "mean reward (100 episodes) 63.082970\n",
      "best mean reward 63.789636\n",
      "running time 783.525314\n",
      "Train_EnvstepsSoFar : 150001\n",
      "Train_AverageReturn : 63.08297012185266\n",
      "Train_BestReturn : 63.789635644749524\n",
      "TimeSinceStart : 783.5253140926361\n",
      "Training Loss : 0.48043230175971985\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 151001\n",
      "mean reward (100 episodes) 63.678259\n",
      "best mean reward 63.789636\n",
      "running time 788.437517\n",
      "Train_EnvstepsSoFar : 151001\n",
      "Train_AverageReturn : 63.67825914676343\n",
      "Train_BestReturn : 63.789635644749524\n",
      "TimeSinceStart : 788.4375169277191\n",
      "Training Loss : 2.581671953201294\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 152001\n",
      "mean reward (100 episodes) 68.387196\n",
      "best mean reward 68.387196\n",
      "running time 793.189515\n",
      "Train_EnvstepsSoFar : 152001\n",
      "Train_AverageReturn : 68.3871957019127\n",
      "Train_BestReturn : 68.3871957019127\n",
      "TimeSinceStart : 793.189514875412\n",
      "Training Loss : 0.33414483070373535\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 153001\n",
      "mean reward (100 episodes) 72.543760\n",
      "best mean reward 72.543760\n",
      "running time 797.581950\n",
      "Train_EnvstepsSoFar : 153001\n",
      "Train_AverageReturn : 72.54376040516702\n",
      "Train_BestReturn : 72.54376040516702\n",
      "TimeSinceStart : 797.5819499492645\n",
      "Training Loss : 0.0990171730518341\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 154001\n",
      "mean reward (100 episodes) 74.323287\n",
      "best mean reward 74.323287\n",
      "running time 803.616503\n",
      "Train_EnvstepsSoFar : 154001\n",
      "Train_AverageReturn : 74.32328683194834\n",
      "Train_BestReturn : 74.32328683194834\n",
      "TimeSinceStart : 803.6165030002594\n",
      "Training Loss : 0.5441285371780396\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 155001\n",
      "mean reward (100 episodes) 74.275597\n",
      "best mean reward 74.323287\n",
      "running time 809.242753\n",
      "Train_EnvstepsSoFar : 155001\n",
      "Train_AverageReturn : 74.27559693282782\n",
      "Train_BestReturn : 74.32328683194834\n",
      "TimeSinceStart : 809.2427530288696\n",
      "Training Loss : 0.38130900263786316\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 156001\n",
      "mean reward (100 episodes) 71.992641\n",
      "best mean reward 74.323287\n",
      "running time 813.204600\n",
      "Train_EnvstepsSoFar : 156001\n",
      "Train_AverageReturn : 71.99264053689033\n",
      "Train_BestReturn : 74.32328683194834\n",
      "TimeSinceStart : 813.2046000957489\n",
      "Training Loss : 0.19269712269306183\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 157001\n",
      "mean reward (100 episodes) 72.500449\n",
      "best mean reward 74.323287\n",
      "running time 818.534687\n",
      "Train_EnvstepsSoFar : 157001\n",
      "Train_AverageReturn : 72.50044879779058\n",
      "Train_BestReturn : 74.32328683194834\n",
      "TimeSinceStart : 818.5346870422363\n",
      "Training Loss : 0.20763909816741943\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 158001\n",
      "mean reward (100 episodes) 70.505796\n",
      "best mean reward 74.323287\n",
      "running time 825.870799\n",
      "Train_EnvstepsSoFar : 158001\n",
      "Train_AverageReturn : 70.50579597979836\n",
      "Train_BestReturn : 74.32328683194834\n",
      "TimeSinceStart : 825.8707990646362\n",
      "Training Loss : 0.27095893025398254\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 159001\n",
      "mean reward (100 episodes) 76.415164\n",
      "best mean reward 76.415164\n",
      "running time 830.693148\n",
      "Train_EnvstepsSoFar : 159001\n",
      "Train_AverageReturn : 76.4151642337894\n",
      "Train_BestReturn : 76.4151642337894\n",
      "TimeSinceStart : 830.6931478977203\n",
      "Training Loss : 0.09098558127880096\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 160001\n",
      "mean reward (100 episodes) 76.382193\n",
      "best mean reward 76.415164\n",
      "running time 838.180869\n",
      "Train_EnvstepsSoFar : 160001\n",
      "Train_AverageReturn : 76.38219273638882\n",
      "Train_BestReturn : 76.4151642337894\n",
      "TimeSinceStart : 838.1808688640594\n",
      "Training Loss : 0.08922838419675827\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 161001\n",
      "mean reward (100 episodes) 77.522190\n",
      "best mean reward 77.522190\n",
      "running time 845.075427\n",
      "Train_EnvstepsSoFar : 161001\n",
      "Train_AverageReturn : 77.52219038271728\n",
      "Train_BestReturn : 77.52219038271728\n",
      "TimeSinceStart : 845.0754270553589\n",
      "Training Loss : 0.12261749804019928\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 162001\n",
      "mean reward (100 episodes) 78.713637\n",
      "best mean reward 78.713637\n",
      "running time 850.755758\n",
      "Train_EnvstepsSoFar : 162001\n",
      "Train_AverageReturn : 78.71363721596343\n",
      "Train_BestReturn : 78.71363721596343\n",
      "TimeSinceStart : 850.7557580471039\n",
      "Training Loss : 0.07758583128452301\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 163001\n",
      "mean reward (100 episodes) 76.871166\n",
      "best mean reward 78.713637\n",
      "running time 854.934956\n",
      "Train_EnvstepsSoFar : 163001\n",
      "Train_AverageReturn : 76.87116566915124\n",
      "Train_BestReturn : 78.71363721596343\n",
      "TimeSinceStart : 854.934956073761\n",
      "Training Loss : 0.21536009013652802\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 164001\n",
      "mean reward (100 episodes) 76.085918\n",
      "best mean reward 78.713637\n",
      "running time 859.866635\n",
      "Train_EnvstepsSoFar : 164001\n",
      "Train_AverageReturn : 76.08591827203803\n",
      "Train_BestReturn : 78.71363721596343\n",
      "TimeSinceStart : 859.8666350841522\n",
      "Training Loss : 0.40132442116737366\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 165001\n",
      "mean reward (100 episodes) 74.494105\n",
      "best mean reward 78.713637\n",
      "running time 863.856808\n",
      "Train_EnvstepsSoFar : 165001\n",
      "Train_AverageReturn : 74.49410452856655\n",
      "Train_BestReturn : 78.71363721596343\n",
      "TimeSinceStart : 863.8568079471588\n",
      "Training Loss : 0.1251329630613327\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 166001\n",
      "mean reward (100 episodes) 79.974001\n",
      "best mean reward 79.974001\n",
      "running time 867.694488\n",
      "Train_EnvstepsSoFar : 166001\n",
      "Train_AverageReturn : 79.97400061027086\n",
      "Train_BestReturn : 79.97400061027086\n",
      "TimeSinceStart : 867.6944880485535\n",
      "Training Loss : 0.1537792831659317\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 167001\n",
      "mean reward (100 episodes) 80.103987\n",
      "best mean reward 80.103987\n",
      "running time 871.526670\n",
      "Train_EnvstepsSoFar : 167001\n",
      "Train_AverageReturn : 80.10398711325634\n",
      "Train_BestReturn : 80.10398711325634\n",
      "TimeSinceStart : 871.5266699790955\n",
      "Training Loss : 0.4988243877887726\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 168001\n",
      "mean reward (100 episodes) 81.613048\n",
      "best mean reward 81.613048\n",
      "running time 875.914170\n",
      "Train_EnvstepsSoFar : 168001\n",
      "Train_AverageReturn : 81.6130483360056\n",
      "Train_BestReturn : 81.6130483360056\n",
      "TimeSinceStart : 875.9141700267792\n",
      "Training Loss : 0.19582723081111908\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 169001\n",
      "mean reward (100 episodes) 79.060669\n",
      "best mean reward 81.613048\n",
      "running time 879.800534\n",
      "Train_EnvstepsSoFar : 169001\n",
      "Train_AverageReturn : 79.06066939938552\n",
      "Train_BestReturn : 81.6130483360056\n",
      "TimeSinceStart : 879.8005340099335\n",
      "Training Loss : 1.8846570253372192\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 170001\n",
      "mean reward (100 episodes) 78.004628\n",
      "best mean reward 81.613048\n",
      "running time 883.925921\n",
      "Train_EnvstepsSoFar : 170001\n",
      "Train_AverageReturn : 78.00462805100224\n",
      "Train_BestReturn : 81.6130483360056\n",
      "TimeSinceStart : 883.9259209632874\n",
      "Training Loss : 1.2707573175430298\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 171001\n",
      "mean reward (100 episodes) 77.904460\n",
      "best mean reward 81.613048\n",
      "running time 887.823826\n",
      "Train_EnvstepsSoFar : 171001\n",
      "Train_AverageReturn : 77.90446032370059\n",
      "Train_BestReturn : 81.6130483360056\n",
      "TimeSinceStart : 887.8238258361816\n",
      "Training Loss : 0.13742202520370483\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 172001\n",
      "mean reward (100 episodes) 80.537242\n",
      "best mean reward 81.613048\n",
      "running time 891.702370\n",
      "Train_EnvstepsSoFar : 172001\n",
      "Train_AverageReturn : 80.53724208772857\n",
      "Train_BestReturn : 81.6130483360056\n",
      "TimeSinceStart : 891.70236992836\n",
      "Training Loss : 1.0892399549484253\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 173001\n",
      "mean reward (100 episodes) 85.492784\n",
      "best mean reward 85.492784\n",
      "running time 895.421942\n",
      "Train_EnvstepsSoFar : 173001\n",
      "Train_AverageReturn : 85.49278449162773\n",
      "Train_BestReturn : 85.49278449162773\n",
      "TimeSinceStart : 895.4219419956207\n",
      "Training Loss : 5.084014892578125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 174001\n",
      "mean reward (100 episodes) 86.842746\n",
      "best mean reward 86.842746\n",
      "running time 899.475009\n",
      "Train_EnvstepsSoFar : 174001\n",
      "Train_AverageReturn : 86.84274597498903\n",
      "Train_BestReturn : 86.84274597498903\n",
      "TimeSinceStart : 899.4750089645386\n",
      "Training Loss : 0.053349707275629044\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 175001\n",
      "mean reward (100 episodes) 84.382525\n",
      "best mean reward 86.842746\n",
      "running time 903.148600\n",
      "Train_EnvstepsSoFar : 175001\n",
      "Train_AverageReturn : 84.38252519284615\n",
      "Train_BestReturn : 86.84274597498903\n",
      "TimeSinceStart : 903.148600101471\n",
      "Training Loss : 1.345078468322754\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 176001\n",
      "mean reward (100 episodes) 85.476825\n",
      "best mean reward 86.842746\n",
      "running time 907.355575\n",
      "Train_EnvstepsSoFar : 176001\n",
      "Train_AverageReturn : 85.47682500200185\n",
      "Train_BestReturn : 86.84274597498903\n",
      "TimeSinceStart : 907.3555750846863\n",
      "Training Loss : 0.18086223304271698\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 177001\n",
      "mean reward (100 episodes) 82.725973\n",
      "best mean reward 86.842746\n",
      "running time 911.534378\n",
      "Train_EnvstepsSoFar : 177001\n",
      "Train_AverageReturn : 82.72597283533241\n",
      "Train_BestReturn : 86.84274597498903\n",
      "TimeSinceStart : 911.5343780517578\n",
      "Training Loss : 0.5216193199157715\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 178001\n",
      "mean reward (100 episodes) 82.120211\n",
      "best mean reward 86.842746\n",
      "running time 915.954831\n",
      "Train_EnvstepsSoFar : 178001\n",
      "Train_AverageReturn : 82.12021102167765\n",
      "Train_BestReturn : 86.84274597498903\n",
      "TimeSinceStart : 915.954831123352\n",
      "Training Loss : 0.1993280053138733\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 179001\n",
      "mean reward (100 episodes) 79.923745\n",
      "best mean reward 86.842746\n",
      "running time 920.335622\n",
      "Train_EnvstepsSoFar : 179001\n",
      "Train_AverageReturn : 79.92374504565244\n",
      "Train_BestReturn : 86.84274597498903\n",
      "TimeSinceStart : 920.3356218338013\n",
      "Training Loss : 0.08293302357196808\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 180001\n",
      "mean reward (100 episodes) 78.573330\n",
      "best mean reward 86.842746\n",
      "running time 924.844859\n",
      "Train_EnvstepsSoFar : 180001\n",
      "Train_AverageReturn : 78.57333020830326\n",
      "Train_BestReturn : 86.84274597498903\n",
      "TimeSinceStart : 924.8448588848114\n",
      "Training Loss : 0.14637207984924316\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 181001\n",
      "mean reward (100 episodes) 78.683137\n",
      "best mean reward 86.842746\n",
      "running time 929.069211\n",
      "Train_EnvstepsSoFar : 181001\n",
      "Train_AverageReturn : 78.68313740225834\n",
      "Train_BestReturn : 86.84274597498903\n",
      "TimeSinceStart : 929.069210767746\n",
      "Training Loss : 1.8339416980743408\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 182001\n",
      "mean reward (100 episodes) 79.750122\n",
      "best mean reward 86.842746\n",
      "running time 933.136332\n",
      "Train_EnvstepsSoFar : 182001\n",
      "Train_AverageReturn : 79.75012229453858\n",
      "Train_BestReturn : 86.84274597498903\n",
      "TimeSinceStart : 933.1363320350647\n",
      "Training Loss : 0.1313309371471405\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 183001\n",
      "mean reward (100 episodes) 78.428129\n",
      "best mean reward 86.842746\n",
      "running time 938.170705\n",
      "Train_EnvstepsSoFar : 183001\n",
      "Train_AverageReturn : 78.42812891914075\n",
      "Train_BestReturn : 86.84274597498903\n",
      "TimeSinceStart : 938.1707048416138\n",
      "Training Loss : 0.34440332651138306\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 184001\n",
      "mean reward (100 episodes) 78.103361\n",
      "best mean reward 86.842746\n",
      "running time 942.821909\n",
      "Train_EnvstepsSoFar : 184001\n",
      "Train_AverageReturn : 78.10336124852998\n",
      "Train_BestReturn : 86.84274597498903\n",
      "TimeSinceStart : 942.8219089508057\n",
      "Training Loss : 3.702895164489746\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 185001\n",
      "mean reward (100 episodes) 77.046101\n",
      "best mean reward 86.842746\n",
      "running time 948.357827\n",
      "Train_EnvstepsSoFar : 185001\n",
      "Train_AverageReturn : 77.04610106908952\n",
      "Train_BestReturn : 86.84274597498903\n",
      "TimeSinceStart : 948.3578269481659\n",
      "Training Loss : 0.26285091042518616\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 186001\n",
      "mean reward (100 episodes) 79.366258\n",
      "best mean reward 86.842746\n",
      "running time 953.872671\n",
      "Train_EnvstepsSoFar : 186001\n",
      "Train_AverageReturn : 79.36625810820428\n",
      "Train_BestReturn : 86.84274597498903\n",
      "TimeSinceStart : 953.8726708889008\n",
      "Training Loss : 2.8740487098693848\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 187001\n",
      "mean reward (100 episodes) 85.282872\n",
      "best mean reward 86.842746\n",
      "running time 958.059585\n",
      "Train_EnvstepsSoFar : 187001\n",
      "Train_AverageReturn : 85.28287171429177\n",
      "Train_BestReturn : 86.84274597498903\n",
      "TimeSinceStart : 958.0595850944519\n",
      "Training Loss : 0.11965539306402206\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 188001\n",
      "mean reward (100 episodes) 85.313895\n",
      "best mean reward 86.842746\n",
      "running time 962.202102\n",
      "Train_EnvstepsSoFar : 188001\n",
      "Train_AverageReturn : 85.31389468088356\n",
      "Train_BestReturn : 86.84274597498903\n",
      "TimeSinceStart : 962.2021019458771\n",
      "Training Loss : 0.05088907852768898\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 189001\n",
      "mean reward (100 episodes) 87.167591\n",
      "best mean reward 87.167591\n",
      "running time 966.720031\n",
      "Train_EnvstepsSoFar : 189001\n",
      "Train_AverageReturn : 87.16759139226646\n",
      "Train_BestReturn : 87.16759139226646\n",
      "TimeSinceStart : 966.7200310230255\n",
      "Training Loss : 1.5531864166259766\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 190001\n",
      "mean reward (100 episodes) 86.382861\n",
      "best mean reward 87.167591\n",
      "running time 972.562922\n",
      "Train_EnvstepsSoFar : 190001\n",
      "Train_AverageReturn : 86.38286117395025\n",
      "Train_BestReturn : 87.16759139226646\n",
      "TimeSinceStart : 972.562922000885\n",
      "Training Loss : 1.4919819831848145\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 191001\n",
      "mean reward (100 episodes) 88.808934\n",
      "best mean reward 88.808934\n",
      "running time 977.013530\n",
      "Train_EnvstepsSoFar : 191001\n",
      "Train_AverageReturn : 88.80893372511214\n",
      "Train_BestReturn : 88.80893372511214\n",
      "TimeSinceStart : 977.0135300159454\n",
      "Training Loss : 0.09634791314601898\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 192001\n",
      "mean reward (100 episodes) 81.444715\n",
      "best mean reward 88.808934\n",
      "running time 980.953387\n",
      "Train_EnvstepsSoFar : 192001\n",
      "Train_AverageReturn : 81.44471493107243\n",
      "Train_BestReturn : 88.80893372511214\n",
      "TimeSinceStart : 980.9533870220184\n",
      "Training Loss : 0.2208367884159088\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 193001\n",
      "mean reward (100 episodes) 80.468174\n",
      "best mean reward 88.808934\n",
      "running time 985.883024\n",
      "Train_EnvstepsSoFar : 193001\n",
      "Train_AverageReturn : 80.46817418477366\n",
      "Train_BestReturn : 88.80893372511214\n",
      "TimeSinceStart : 985.8830237388611\n",
      "Training Loss : 0.12727606296539307\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 194001\n",
      "mean reward (100 episodes) 83.021849\n",
      "best mean reward 88.808934\n",
      "running time 990.602387\n",
      "Train_EnvstepsSoFar : 194001\n",
      "Train_AverageReturn : 83.02184933982952\n",
      "Train_BestReturn : 88.80893372511214\n",
      "TimeSinceStart : 990.6023869514465\n",
      "Training Loss : 0.9388798475265503\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 195001\n",
      "mean reward (100 episodes) 84.367447\n",
      "best mean reward 88.808934\n",
      "running time 995.556334\n",
      "Train_EnvstepsSoFar : 195001\n",
      "Train_AverageReturn : 84.36744706182881\n",
      "Train_BestReturn : 88.80893372511214\n",
      "TimeSinceStart : 995.5563340187073\n",
      "Training Loss : 0.22271773219108582\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 196001\n",
      "mean reward (100 episodes) 89.230140\n",
      "best mean reward 89.230140\n",
      "running time 999.636224\n",
      "Train_EnvstepsSoFar : 196001\n",
      "Train_AverageReturn : 89.23014048491176\n",
      "Train_BestReturn : 89.23014048491176\n",
      "TimeSinceStart : 999.6362240314484\n",
      "Training Loss : 0.259754478931427\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 197001\n",
      "mean reward (100 episodes) 88.044690\n",
      "best mean reward 89.230140\n",
      "running time 1004.008225\n",
      "Train_EnvstepsSoFar : 197001\n",
      "Train_AverageReturn : 88.04468996399311\n",
      "Train_BestReturn : 89.23014048491176\n",
      "TimeSinceStart : 1004.0082249641418\n",
      "Training Loss : 1.154132604598999\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 198001\n",
      "mean reward (100 episodes) 85.292581\n",
      "best mean reward 89.230140\n",
      "running time 1008.194887\n",
      "Train_EnvstepsSoFar : 198001\n",
      "Train_AverageReturn : 85.2925807066483\n",
      "Train_BestReturn : 89.23014048491176\n",
      "TimeSinceStart : 1008.1948869228363\n",
      "Training Loss : 0.2006130963563919\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 199001\n",
      "mean reward (100 episodes) 82.189352\n",
      "best mean reward 89.230140\n",
      "running time 1012.666418\n",
      "Train_EnvstepsSoFar : 199001\n",
      "Train_AverageReturn : 82.18935194862208\n",
      "Train_BestReturn : 89.23014048491176\n",
      "TimeSinceStart : 1012.666417837143\n",
      "Training Loss : 0.33117225766181946\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 200001\n",
      "mean reward (100 episodes) 85.294887\n",
      "best mean reward 89.230140\n",
      "running time 1016.840754\n",
      "Train_EnvstepsSoFar : 200001\n",
      "Train_AverageReturn : 85.29488701310082\n",
      "Train_BestReturn : 89.23014048491176\n",
      "TimeSinceStart : 1016.8407537937164\n",
      "Training Loss : 0.46384164690971375\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 201001\n",
      "mean reward (100 episodes) 81.781474\n",
      "best mean reward 89.230140\n",
      "running time 1021.155846\n",
      "Train_EnvstepsSoFar : 201001\n",
      "Train_AverageReturn : 81.78147396181625\n",
      "Train_BestReturn : 89.23014048491176\n",
      "TimeSinceStart : 1021.1558458805084\n",
      "Training Loss : 0.25088804960250854\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 202001\n",
      "mean reward (100 episodes) 81.790284\n",
      "best mean reward 89.230140\n",
      "running time 1027.139699\n",
      "Train_EnvstepsSoFar : 202001\n",
      "Train_AverageReturn : 81.7902837324009\n",
      "Train_BestReturn : 89.23014048491176\n",
      "TimeSinceStart : 1027.1396989822388\n",
      "Training Loss : 0.8646146655082703\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 203001\n",
      "mean reward (100 episodes) 80.059350\n",
      "best mean reward 89.230140\n",
      "running time 1032.862441\n",
      "Train_EnvstepsSoFar : 203001\n",
      "Train_AverageReturn : 80.05934976172743\n",
      "Train_BestReturn : 89.23014048491176\n",
      "TimeSinceStart : 1032.8624408245087\n",
      "Training Loss : 0.2577880322933197\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 204001\n",
      "mean reward (100 episodes) 80.290744\n",
      "best mean reward 89.230140\n",
      "running time 1040.392219\n",
      "Train_EnvstepsSoFar : 204001\n",
      "Train_AverageReturn : 80.29074426714322\n",
      "Train_BestReturn : 89.23014048491176\n",
      "TimeSinceStart : 1040.3922188282013\n",
      "Training Loss : 0.5426439046859741\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 205001\n",
      "mean reward (100 episodes) 81.661393\n",
      "best mean reward 89.230140\n",
      "running time 1048.241943\n",
      "Train_EnvstepsSoFar : 205001\n",
      "Train_AverageReturn : 81.6613925972737\n",
      "Train_BestReturn : 89.23014048491176\n",
      "TimeSinceStart : 1048.2419431209564\n",
      "Training Loss : 0.8219277858734131\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 206001\n",
      "mean reward (100 episodes) 83.476408\n",
      "best mean reward 89.230140\n",
      "running time 1053.569839\n",
      "Train_EnvstepsSoFar : 206001\n",
      "Train_AverageReturn : 83.47640762727939\n",
      "Train_BestReturn : 89.23014048491176\n",
      "TimeSinceStart : 1053.569839000702\n",
      "Training Loss : 1.4598655700683594\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 207001\n",
      "mean reward (100 episodes) 83.314141\n",
      "best mean reward 89.230140\n",
      "running time 1058.194180\n",
      "Train_EnvstepsSoFar : 207001\n",
      "Train_AverageReturn : 83.31414119339597\n",
      "Train_BestReturn : 89.23014048491176\n",
      "TimeSinceStart : 1058.1941800117493\n",
      "Training Loss : 1.0015898942947388\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 208001\n",
      "mean reward (100 episodes) 89.438188\n",
      "best mean reward 89.438188\n",
      "running time 1062.076745\n",
      "Train_EnvstepsSoFar : 208001\n",
      "Train_AverageReturn : 89.4381881218353\n",
      "Train_BestReturn : 89.4381881218353\n",
      "TimeSinceStart : 1062.0767450332642\n",
      "Training Loss : 0.5402583479881287\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 209001\n",
      "mean reward (100 episodes) 84.996368\n",
      "best mean reward 89.438188\n",
      "running time 1066.033581\n",
      "Train_EnvstepsSoFar : 209001\n",
      "Train_AverageReturn : 84.9963682938269\n",
      "Train_BestReturn : 89.4381881218353\n",
      "TimeSinceStart : 1066.0335810184479\n",
      "Training Loss : 0.1888076514005661\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 210001\n",
      "mean reward (100 episodes) 83.119935\n",
      "best mean reward 89.438188\n",
      "running time 1071.730139\n",
      "Train_EnvstepsSoFar : 210001\n",
      "Train_AverageReturn : 83.11993488080675\n",
      "Train_BestReturn : 89.4381881218353\n",
      "TimeSinceStart : 1071.7301387786865\n",
      "Training Loss : 1.6964352130889893\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 211001\n",
      "mean reward (100 episodes) 82.558133\n",
      "best mean reward 89.438188\n",
      "running time 1076.149323\n",
      "Train_EnvstepsSoFar : 211001\n",
      "Train_AverageReturn : 82.55813320577718\n",
      "Train_BestReturn : 89.4381881218353\n",
      "TimeSinceStart : 1076.1493229866028\n",
      "Training Loss : 0.4899733066558838\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 212001\n",
      "mean reward (100 episodes) 82.621815\n",
      "best mean reward 89.438188\n",
      "running time 1080.347941\n",
      "Train_EnvstepsSoFar : 212001\n",
      "Train_AverageReturn : 82.62181503744138\n",
      "Train_BestReturn : 89.4381881218353\n",
      "TimeSinceStart : 1080.347941160202\n",
      "Training Loss : 0.41634172201156616\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 213001\n",
      "mean reward (100 episodes) 82.521867\n",
      "best mean reward 89.438188\n",
      "running time 1085.445978\n",
      "Train_EnvstepsSoFar : 213001\n",
      "Train_AverageReturn : 82.52186746112051\n",
      "Train_BestReturn : 89.4381881218353\n",
      "TimeSinceStart : 1085.4459781646729\n",
      "Training Loss : 1.537217140197754\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 214001\n",
      "mean reward (100 episodes) 83.611038\n",
      "best mean reward 89.438188\n",
      "running time 1089.582769\n",
      "Train_EnvstepsSoFar : 214001\n",
      "Train_AverageReturn : 83.61103764266446\n",
      "Train_BestReturn : 89.4381881218353\n",
      "TimeSinceStart : 1089.5827689170837\n",
      "Training Loss : 0.3685391843318939\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 215001\n",
      "mean reward (100 episodes) 83.497457\n",
      "best mean reward 89.438188\n",
      "running time 1095.929100\n",
      "Train_EnvstepsSoFar : 215001\n",
      "Train_AverageReturn : 83.4974567221302\n",
      "Train_BestReturn : 89.4381881218353\n",
      "TimeSinceStart : 1095.929100036621\n",
      "Training Loss : 1.5248074531555176\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 216001\n",
      "mean reward (100 episodes) 88.726236\n",
      "best mean reward 89.438188\n",
      "running time 1099.902173\n",
      "Train_EnvstepsSoFar : 216001\n",
      "Train_AverageReturn : 88.72623626516327\n",
      "Train_BestReturn : 89.4381881218353\n",
      "TimeSinceStart : 1099.9021728038788\n",
      "Training Loss : 0.220673069357872\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 217001\n",
      "mean reward (100 episodes) 90.072912\n",
      "best mean reward 90.072912\n",
      "running time 1104.009425\n",
      "Train_EnvstepsSoFar : 217001\n",
      "Train_AverageReturn : 90.07291170550988\n",
      "Train_BestReturn : 90.07291170550988\n",
      "TimeSinceStart : 1104.0094249248505\n",
      "Training Loss : 0.1764093041419983\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 218001\n",
      "mean reward (100 episodes) 91.069510\n",
      "best mean reward 91.069510\n",
      "running time 1108.232940\n",
      "Train_EnvstepsSoFar : 218001\n",
      "Train_AverageReturn : 91.06951013357042\n",
      "Train_BestReturn : 91.06951013357042\n",
      "TimeSinceStart : 1108.2329399585724\n",
      "Training Loss : 0.4770888388156891\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 219001\n",
      "mean reward (100 episodes) 90.811153\n",
      "best mean reward 91.069510\n",
      "running time 1112.409356\n",
      "Train_EnvstepsSoFar : 219001\n",
      "Train_AverageReturn : 90.81115273910589\n",
      "Train_BestReturn : 91.06951013357042\n",
      "TimeSinceStart : 1112.40935587883\n",
      "Training Loss : 0.09405101090669632\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 220001\n",
      "mean reward (100 episodes) 98.693026\n",
      "best mean reward 98.693026\n",
      "running time 1116.826088\n",
      "Train_EnvstepsSoFar : 220001\n",
      "Train_AverageReturn : 98.69302625627078\n",
      "Train_BestReturn : 98.69302625627078\n",
      "TimeSinceStart : 1116.8260879516602\n",
      "Training Loss : 1.2139617204666138\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 221001\n",
      "mean reward (100 episodes) 97.991051\n",
      "best mean reward 98.693026\n",
      "running time 1121.696675\n",
      "Train_EnvstepsSoFar : 221001\n",
      "Train_AverageReturn : 97.99105074316908\n",
      "Train_BestReturn : 98.69302625627078\n",
      "TimeSinceStart : 1121.6966750621796\n",
      "Training Loss : 0.11028698086738586\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 222001\n",
      "mean reward (100 episodes) 96.994585\n",
      "best mean reward 98.693026\n",
      "running time 1125.691708\n",
      "Train_EnvstepsSoFar : 222001\n",
      "Train_AverageReturn : 96.99458534013742\n",
      "Train_BestReturn : 98.69302625627078\n",
      "TimeSinceStart : 1125.6917078495026\n",
      "Training Loss : 2.785250186920166\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 223001\n",
      "mean reward (100 episodes) 93.375134\n",
      "best mean reward 98.693026\n",
      "running time 1130.634815\n",
      "Train_EnvstepsSoFar : 223001\n",
      "Train_AverageReturn : 93.3751341217598\n",
      "Train_BestReturn : 98.69302625627078\n",
      "TimeSinceStart : 1130.6348149776459\n",
      "Training Loss : 0.09456116706132889\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 224001\n",
      "mean reward (100 episodes) 93.319719\n",
      "best mean reward 98.693026\n",
      "running time 1134.838024\n",
      "Train_EnvstepsSoFar : 224001\n",
      "Train_AverageReturn : 93.3197191578034\n",
      "Train_BestReturn : 98.69302625627078\n",
      "TimeSinceStart : 1134.8380239009857\n",
      "Training Loss : 2.964484453201294\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 225001\n",
      "mean reward (100 episodes) 91.256384\n",
      "best mean reward 98.693026\n",
      "running time 1139.770336\n",
      "Train_EnvstepsSoFar : 225001\n",
      "Train_AverageReturn : 91.25638391026926\n",
      "Train_BestReturn : 98.69302625627078\n",
      "TimeSinceStart : 1139.7703359127045\n",
      "Training Loss : 0.16164672374725342\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 226001\n",
      "mean reward (100 episodes) 90.399627\n",
      "best mean reward 98.693026\n",
      "running time 1143.935593\n",
      "Train_EnvstepsSoFar : 226001\n",
      "Train_AverageReturn : 90.39962731347461\n",
      "Train_BestReturn : 98.69302625627078\n",
      "TimeSinceStart : 1143.9355928897858\n",
      "Training Loss : 0.16606158018112183\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 227001\n",
      "mean reward (100 episodes) 88.941267\n",
      "best mean reward 98.693026\n",
      "running time 1149.234742\n",
      "Train_EnvstepsSoFar : 227001\n",
      "Train_AverageReturn : 88.94126719725973\n",
      "Train_BestReturn : 98.69302625627078\n",
      "TimeSinceStart : 1149.2347419261932\n",
      "Training Loss : 0.22862258553504944\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 228001\n",
      "mean reward (100 episodes) 90.655385\n",
      "best mean reward 98.693026\n",
      "running time 1153.688587\n",
      "Train_EnvstepsSoFar : 228001\n",
      "Train_AverageReturn : 90.65538484731283\n",
      "Train_BestReturn : 98.69302625627078\n",
      "TimeSinceStart : 1153.6885869503021\n",
      "Training Loss : 0.18240198493003845\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 229001\n",
      "mean reward (100 episodes) 91.350059\n",
      "best mean reward 98.693026\n",
      "running time 1158.703071\n",
      "Train_EnvstepsSoFar : 229001\n",
      "Train_AverageReturn : 91.35005944921059\n",
      "Train_BestReturn : 98.69302625627078\n",
      "TimeSinceStart : 1158.7030708789825\n",
      "Training Loss : 1.1854549646377563\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 230001\n",
      "mean reward (100 episodes) 91.910448\n",
      "best mean reward 98.693026\n",
      "running time 1162.972343\n",
      "Train_EnvstepsSoFar : 230001\n",
      "Train_AverageReturn : 91.91044812209918\n",
      "Train_BestReturn : 98.69302625627078\n",
      "TimeSinceStart : 1162.972342967987\n",
      "Training Loss : 1.3980865478515625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 231001\n",
      "mean reward (100 episodes) 91.668699\n",
      "best mean reward 98.693026\n",
      "running time 1166.820122\n",
      "Train_EnvstepsSoFar : 231001\n",
      "Train_AverageReturn : 91.66869895763215\n",
      "Train_BestReturn : 98.69302625627078\n",
      "TimeSinceStart : 1166.8201220035553\n",
      "Training Loss : 1.4824429750442505\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 232001\n",
      "mean reward (100 episodes) 85.578432\n",
      "best mean reward 98.693026\n",
      "running time 1170.916032\n",
      "Train_EnvstepsSoFar : 232001\n",
      "Train_AverageReturn : 85.57843248138941\n",
      "Train_BestReturn : 98.69302625627078\n",
      "TimeSinceStart : 1170.916032075882\n",
      "Training Loss : 0.509216845035553\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 233001\n",
      "mean reward (100 episodes) 87.189878\n",
      "best mean reward 98.693026\n",
      "running time 1175.061925\n",
      "Train_EnvstepsSoFar : 233001\n",
      "Train_AverageReturn : 87.18987845980622\n",
      "Train_BestReturn : 98.69302625627078\n",
      "TimeSinceStart : 1175.0619251728058\n",
      "Training Loss : 0.5621466040611267\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 234001\n",
      "mean reward (100 episodes) 86.957753\n",
      "best mean reward 98.693026\n",
      "running time 1179.956529\n",
      "Train_EnvstepsSoFar : 234001\n",
      "Train_AverageReturn : 86.95775260352875\n",
      "Train_BestReturn : 98.69302625627078\n",
      "TimeSinceStart : 1179.9565289020538\n",
      "Training Loss : 0.21028867363929749\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 235001\n",
      "mean reward (100 episodes) 88.065522\n",
      "best mean reward 98.693026\n",
      "running time 1184.313819\n",
      "Train_EnvstepsSoFar : 235001\n",
      "Train_AverageReturn : 88.06552173353664\n",
      "Train_BestReturn : 98.69302625627078\n",
      "TimeSinceStart : 1184.3138189315796\n",
      "Training Loss : 1.1236929893493652\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 236001\n",
      "mean reward (100 episodes) 90.545735\n",
      "best mean reward 98.693026\n",
      "running time 1188.012116\n",
      "Train_EnvstepsSoFar : 236001\n",
      "Train_AverageReturn : 90.54573485997395\n",
      "Train_BestReturn : 98.69302625627078\n",
      "TimeSinceStart : 1188.0121159553528\n",
      "Training Loss : 3.0196824073791504\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 237001\n",
      "mean reward (100 episodes) 93.214928\n",
      "best mean reward 98.693026\n",
      "running time 1192.393702\n",
      "Train_EnvstepsSoFar : 237001\n",
      "Train_AverageReturn : 93.21492761317346\n",
      "Train_BestReturn : 98.69302625627078\n",
      "TimeSinceStart : 1192.3937017917633\n",
      "Training Loss : 0.33598557114601135\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 238001\n",
      "mean reward (100 episodes) 95.371814\n",
      "best mean reward 98.693026\n",
      "running time 1197.229009\n",
      "Train_EnvstepsSoFar : 238001\n",
      "Train_AverageReturn : 95.37181383532085\n",
      "Train_BestReturn : 98.69302625627078\n",
      "TimeSinceStart : 1197.2290089130402\n",
      "Training Loss : 0.1325669288635254\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 239001\n",
      "mean reward (100 episodes) 98.150109\n",
      "best mean reward 98.693026\n",
      "running time 1201.565512\n",
      "Train_EnvstepsSoFar : 239001\n",
      "Train_AverageReturn : 98.15010871391833\n",
      "Train_BestReturn : 98.69302625627078\n",
      "TimeSinceStart : 1201.5655119419098\n",
      "Training Loss : 0.6261691451072693\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 240001\n",
      "mean reward (100 episodes) 94.300280\n",
      "best mean reward 98.693026\n",
      "running time 1205.527784\n",
      "Train_EnvstepsSoFar : 240001\n",
      "Train_AverageReturn : 94.30027955406142\n",
      "Train_BestReturn : 98.69302625627078\n",
      "TimeSinceStart : 1205.527783870697\n",
      "Training Loss : 0.7194593548774719\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 241001\n",
      "mean reward (100 episodes) 93.944454\n",
      "best mean reward 98.693026\n",
      "running time 1209.615976\n",
      "Train_EnvstepsSoFar : 241001\n",
      "Train_AverageReturn : 93.944454229916\n",
      "Train_BestReturn : 98.69302625627078\n",
      "TimeSinceStart : 1209.615975856781\n",
      "Training Loss : 0.09231064468622208\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 242001\n",
      "mean reward (100 episodes) 90.726534\n",
      "best mean reward 98.693026\n",
      "running time 1213.949722\n",
      "Train_EnvstepsSoFar : 242001\n",
      "Train_AverageReturn : 90.72653355717341\n",
      "Train_BestReturn : 98.69302625627078\n",
      "TimeSinceStart : 1213.9497220516205\n",
      "Training Loss : 2.1804072856903076\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 243001\n",
      "mean reward (100 episodes) 88.537104\n",
      "best mean reward 98.693026\n",
      "running time 1218.594825\n",
      "Train_EnvstepsSoFar : 243001\n",
      "Train_AverageReturn : 88.53710426609027\n",
      "Train_BestReturn : 98.69302625627078\n",
      "TimeSinceStart : 1218.5948250293732\n",
      "Training Loss : 0.3157498836517334\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 244001\n",
      "mean reward (100 episodes) 91.922268\n",
      "best mean reward 98.693026\n",
      "running time 1222.981770\n",
      "Train_EnvstepsSoFar : 244001\n",
      "Train_AverageReturn : 91.92226757732989\n",
      "Train_BestReturn : 98.69302625627078\n",
      "TimeSinceStart : 1222.9817700386047\n",
      "Training Loss : 2.1278109550476074\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 245001\n",
      "mean reward (100 episodes) 90.160819\n",
      "best mean reward 98.693026\n",
      "running time 1227.045586\n",
      "Train_EnvstepsSoFar : 245001\n",
      "Train_AverageReturn : 90.16081883670144\n",
      "Train_BestReturn : 98.69302625627078\n",
      "TimeSinceStart : 1227.0455858707428\n",
      "Training Loss : 0.2021283060312271\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 246001\n",
      "mean reward (100 episodes) 97.882965\n",
      "best mean reward 98.693026\n",
      "running time 1231.058826\n",
      "Train_EnvstepsSoFar : 246001\n",
      "Train_AverageReturn : 97.88296486504306\n",
      "Train_BestReturn : 98.69302625627078\n",
      "TimeSinceStart : 1231.058825969696\n",
      "Training Loss : 0.13477888703346252\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 247001\n",
      "mean reward (100 episodes) 102.256423\n",
      "best mean reward 102.256423\n",
      "running time 1235.162273\n",
      "Train_EnvstepsSoFar : 247001\n",
      "Train_AverageReturn : 102.2564226871152\n",
      "Train_BestReturn : 102.2564226871152\n",
      "TimeSinceStart : 1235.1622729301453\n",
      "Training Loss : 0.7072610855102539\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 248001\n",
      "mean reward (100 episodes) 106.397063\n",
      "best mean reward 106.397063\n",
      "running time 1239.475829\n",
      "Train_EnvstepsSoFar : 248001\n",
      "Train_AverageReturn : 106.3970630969773\n",
      "Train_BestReturn : 106.3970630969773\n",
      "TimeSinceStart : 1239.475828886032\n",
      "Training Loss : 0.9309095740318298\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 249001\n",
      "mean reward (100 episodes) 107.236988\n",
      "best mean reward 107.236988\n",
      "running time 1244.379302\n",
      "Train_EnvstepsSoFar : 249001\n",
      "Train_AverageReturn : 107.23698763848921\n",
      "Train_BestReturn : 107.23698763848921\n",
      "TimeSinceStart : 1244.3793017864227\n",
      "Training Loss : 0.4299614429473877\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 250001\n",
      "mean reward (100 episodes) 103.040400\n",
      "best mean reward 107.236988\n",
      "running time 1250.188926\n",
      "Train_EnvstepsSoFar : 250001\n",
      "Train_AverageReturn : 103.04039980079487\n",
      "Train_BestReturn : 107.23698763848921\n",
      "TimeSinceStart : 1250.1889259815216\n",
      "Training Loss : 3.762547492980957\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 251001\n",
      "mean reward (100 episodes) 100.933189\n",
      "best mean reward 107.236988\n",
      "running time 1254.697185\n",
      "Train_EnvstepsSoFar : 251001\n",
      "Train_AverageReturn : 100.93318914294618\n",
      "Train_BestReturn : 107.23698763848921\n",
      "TimeSinceStart : 1254.6971850395203\n",
      "Training Loss : 0.41401711106300354\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 252001\n",
      "mean reward (100 episodes) 100.825832\n",
      "best mean reward 107.236988\n",
      "running time 1263.770600\n",
      "Train_EnvstepsSoFar : 252001\n",
      "Train_AverageReturn : 100.82583232734429\n",
      "Train_BestReturn : 107.23698763848921\n",
      "TimeSinceStart : 1263.7705998420715\n",
      "Training Loss : 0.7116551399230957\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 253001\n",
      "mean reward (100 episodes) 98.058698\n",
      "best mean reward 107.236988\n",
      "running time 1267.818923\n",
      "Train_EnvstepsSoFar : 253001\n",
      "Train_AverageReturn : 98.05869768275704\n",
      "Train_BestReturn : 107.23698763848921\n",
      "TimeSinceStart : 1267.818922996521\n",
      "Training Loss : 0.5678244233131409\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 254001\n",
      "mean reward (100 episodes) 98.293443\n",
      "best mean reward 107.236988\n",
      "running time 1272.672038\n",
      "Train_EnvstepsSoFar : 254001\n",
      "Train_AverageReturn : 98.29344307310409\n",
      "Train_BestReturn : 107.23698763848921\n",
      "TimeSinceStart : 1272.6720378398895\n",
      "Training Loss : 0.45567598938941956\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 255001\n",
      "mean reward (100 episodes) 99.434065\n",
      "best mean reward 107.236988\n",
      "running time 1277.984282\n",
      "Train_EnvstepsSoFar : 255001\n",
      "Train_AverageReturn : 99.43406482465804\n",
      "Train_BestReturn : 107.23698763848921\n",
      "TimeSinceStart : 1277.9842820167542\n",
      "Training Loss : 0.237761452794075\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 256001\n",
      "mean reward (100 episodes) 98.374361\n",
      "best mean reward 107.236988\n",
      "running time 1285.116032\n",
      "Train_EnvstepsSoFar : 256001\n",
      "Train_AverageReturn : 98.3743606909325\n",
      "Train_BestReturn : 107.23698763848921\n",
      "TimeSinceStart : 1285.116031885147\n",
      "Training Loss : 0.22871147096157074\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 257001\n",
      "mean reward (100 episodes) 102.761861\n",
      "best mean reward 107.236988\n",
      "running time 1289.461854\n",
      "Train_EnvstepsSoFar : 257001\n",
      "Train_AverageReturn : 102.76186112540387\n",
      "Train_BestReturn : 107.23698763848921\n",
      "TimeSinceStart : 1289.461853981018\n",
      "Training Loss : 0.09369105845689774\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 258001\n",
      "mean reward (100 episodes) 102.857102\n",
      "best mean reward 107.236988\n",
      "running time 1293.726242\n",
      "Train_EnvstepsSoFar : 258001\n",
      "Train_AverageReturn : 102.85710202293608\n",
      "Train_BestReturn : 107.23698763848921\n",
      "TimeSinceStart : 1293.7262420654297\n",
      "Training Loss : 0.6339370012283325\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 259001\n",
      "mean reward (100 episodes) 104.925331\n",
      "best mean reward 107.236988\n",
      "running time 1297.834379\n",
      "Train_EnvstepsSoFar : 259001\n",
      "Train_AverageReturn : 104.92533100949635\n",
      "Train_BestReturn : 107.23698763848921\n",
      "TimeSinceStart : 1297.8343789577484\n",
      "Training Loss : 0.1007181853055954\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 260001\n",
      "mean reward (100 episodes) 105.803615\n",
      "best mean reward 107.236988\n",
      "running time 1301.907913\n",
      "Train_EnvstepsSoFar : 260001\n",
      "Train_AverageReturn : 105.80361540007897\n",
      "Train_BestReturn : 107.23698763848921\n",
      "TimeSinceStart : 1301.9079129695892\n",
      "Training Loss : 0.8555486798286438\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 261001\n",
      "mean reward (100 episodes) 104.329495\n",
      "best mean reward 107.236988\n",
      "running time 1306.981649\n",
      "Train_EnvstepsSoFar : 261001\n",
      "Train_AverageReturn : 104.3294948693717\n",
      "Train_BestReturn : 107.23698763848921\n",
      "TimeSinceStart : 1306.9816489219666\n",
      "Training Loss : 0.5109690427780151\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 262001\n",
      "mean reward (100 episodes) 102.915394\n",
      "best mean reward 107.236988\n",
      "running time 1311.047134\n",
      "Train_EnvstepsSoFar : 262001\n",
      "Train_AverageReturn : 102.91539351079413\n",
      "Train_BestReturn : 107.23698763848921\n",
      "TimeSinceStart : 1311.047133922577\n",
      "Training Loss : 0.29324138164520264\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 263001\n",
      "mean reward (100 episodes) 100.760712\n",
      "best mean reward 107.236988\n",
      "running time 1317.056131\n",
      "Train_EnvstepsSoFar : 263001\n",
      "Train_AverageReturn : 100.76071150643473\n",
      "Train_BestReturn : 107.23698763848921\n",
      "TimeSinceStart : 1317.0561311244965\n",
      "Training Loss : 0.1337103545665741\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 264001\n",
      "mean reward (100 episodes) 104.263279\n",
      "best mean reward 107.236988\n",
      "running time 1321.097759\n",
      "Train_EnvstepsSoFar : 264001\n",
      "Train_AverageReturn : 104.26327942496833\n",
      "Train_BestReturn : 107.23698763848921\n",
      "TimeSinceStart : 1321.0977590084076\n",
      "Training Loss : 0.22416728734970093\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 265001\n",
      "mean reward (100 episodes) 104.205578\n",
      "best mean reward 107.236988\n",
      "running time 1325.756159\n",
      "Train_EnvstepsSoFar : 265001\n",
      "Train_AverageReturn : 104.2055783556103\n",
      "Train_BestReturn : 107.23698763848921\n",
      "TimeSinceStart : 1325.7561588287354\n",
      "Training Loss : 0.7165878415107727\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 266001\n",
      "mean reward (100 episodes) 102.391480\n",
      "best mean reward 107.236988\n",
      "running time 1329.940194\n",
      "Train_EnvstepsSoFar : 266001\n",
      "Train_AverageReturn : 102.39147988279281\n",
      "Train_BestReturn : 107.23698763848921\n",
      "TimeSinceStart : 1329.9401941299438\n",
      "Training Loss : 0.9186074733734131\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 267001\n",
      "mean reward (100 episodes) 101.551784\n",
      "best mean reward 107.236988\n",
      "running time 1335.443373\n",
      "Train_EnvstepsSoFar : 267001\n",
      "Train_AverageReturn : 101.55178382494731\n",
      "Train_BestReturn : 107.23698763848921\n",
      "TimeSinceStart : 1335.443372964859\n",
      "Training Loss : 0.19025319814682007\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 268001\n",
      "mean reward (100 episodes) 100.954943\n",
      "best mean reward 107.236988\n",
      "running time 1339.286687\n",
      "Train_EnvstepsSoFar : 268001\n",
      "Train_AverageReturn : 100.95494289426284\n",
      "Train_BestReturn : 107.23698763848921\n",
      "TimeSinceStart : 1339.2866868972778\n",
      "Training Loss : 0.15799440443515778\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 269001\n",
      "mean reward (100 episodes) 98.898197\n",
      "best mean reward 107.236988\n",
      "running time 1343.392612\n",
      "Train_EnvstepsSoFar : 269001\n",
      "Train_AverageReturn : 98.89819713098262\n",
      "Train_BestReturn : 107.23698763848921\n",
      "TimeSinceStart : 1343.3926119804382\n",
      "Training Loss : 0.1514664739370346\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 270001\n",
      "mean reward (100 episodes) 101.599550\n",
      "best mean reward 107.236988\n",
      "running time 1347.329419\n",
      "Train_EnvstepsSoFar : 270001\n",
      "Train_AverageReturn : 101.59955034880328\n",
      "Train_BestReturn : 107.23698763848921\n",
      "TimeSinceStart : 1347.3294188976288\n",
      "Training Loss : 0.561678409576416\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 271001\n",
      "mean reward (100 episodes) 103.212138\n",
      "best mean reward 107.236988\n",
      "running time 1351.321923\n",
      "Train_EnvstepsSoFar : 271001\n",
      "Train_AverageReturn : 103.21213769679864\n",
      "Train_BestReturn : 107.23698763848921\n",
      "TimeSinceStart : 1351.3219230175018\n",
      "Training Loss : 0.2638418674468994\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 272001\n",
      "mean reward (100 episodes) 105.748782\n",
      "best mean reward 107.236988\n",
      "running time 1355.605109\n",
      "Train_EnvstepsSoFar : 272001\n",
      "Train_AverageReturn : 105.74878171670629\n",
      "Train_BestReturn : 107.23698763848921\n",
      "TimeSinceStart : 1355.6051089763641\n",
      "Training Loss : 1.6009665727615356\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 273001\n",
      "mean reward (100 episodes) 107.912809\n",
      "best mean reward 107.912809\n",
      "running time 1360.999982\n",
      "Train_EnvstepsSoFar : 273001\n",
      "Train_AverageReturn : 107.91280864283075\n",
      "Train_BestReturn : 107.91280864283075\n",
      "TimeSinceStart : 1360.9999821186066\n",
      "Training Loss : 0.200679212808609\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 274001\n",
      "mean reward (100 episodes) 107.882829\n",
      "best mean reward 107.912809\n",
      "running time 1365.091401\n",
      "Train_EnvstepsSoFar : 274001\n",
      "Train_AverageReturn : 107.88282885423423\n",
      "Train_BestReturn : 107.91280864283075\n",
      "TimeSinceStart : 1365.09140086174\n",
      "Training Loss : 2.461109161376953\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 275001\n",
      "mean reward (100 episodes) 101.193883\n",
      "best mean reward 107.912809\n",
      "running time 1368.724295\n",
      "Train_EnvstepsSoFar : 275001\n",
      "Train_AverageReturn : 101.19388322370783\n",
      "Train_BestReturn : 107.91280864283075\n",
      "TimeSinceStart : 1368.7242949008942\n",
      "Training Loss : 0.1490136831998825\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 276001\n",
      "mean reward (100 episodes) 104.578314\n",
      "best mean reward 107.912809\n",
      "running time 1372.836519\n",
      "Train_EnvstepsSoFar : 276001\n",
      "Train_AverageReturn : 104.57831389261653\n",
      "Train_BestReturn : 107.91280864283075\n",
      "TimeSinceStart : 1372.8365187644958\n",
      "Training Loss : 0.11376181244850159\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 277001\n",
      "mean reward (100 episodes) 104.801032\n",
      "best mean reward 107.912809\n",
      "running time 1376.724850\n",
      "Train_EnvstepsSoFar : 277001\n",
      "Train_AverageReturn : 104.80103226839668\n",
      "Train_BestReturn : 107.91280864283075\n",
      "TimeSinceStart : 1376.7248499393463\n",
      "Training Loss : 1.3786461353302002\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 278001\n",
      "mean reward (100 episodes) 111.690154\n",
      "best mean reward 111.690154\n",
      "running time 1380.876976\n",
      "Train_EnvstepsSoFar : 278001\n",
      "Train_AverageReturn : 111.6901535067495\n",
      "Train_BestReturn : 111.6901535067495\n",
      "TimeSinceStart : 1380.876975774765\n",
      "Training Loss : 2.967527389526367\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 279001\n",
      "mean reward (100 episodes) 114.807769\n",
      "best mean reward 114.807769\n",
      "running time 1384.695264\n",
      "Train_EnvstepsSoFar : 279001\n",
      "Train_AverageReturn : 114.80776924343762\n",
      "Train_BestReturn : 114.80776924343762\n",
      "TimeSinceStart : 1384.6952638626099\n",
      "Training Loss : 0.1603119820356369\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 280001\n",
      "mean reward (100 episodes) 113.713982\n",
      "best mean reward 114.807769\n",
      "running time 1388.900990\n",
      "Train_EnvstepsSoFar : 280001\n",
      "Train_AverageReturn : 113.71398223897242\n",
      "Train_BestReturn : 114.80776924343762\n",
      "TimeSinceStart : 1388.9009900093079\n",
      "Training Loss : 1.5166350603103638\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 281001\n",
      "mean reward (100 episodes) 119.230809\n",
      "best mean reward 119.230809\n",
      "running time 1393.173169\n",
      "Train_EnvstepsSoFar : 281001\n",
      "Train_AverageReturn : 119.23080949441015\n",
      "Train_BestReturn : 119.23080949441015\n",
      "TimeSinceStart : 1393.1731688976288\n",
      "Training Loss : 0.27311354875564575\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 282001\n",
      "mean reward (100 episodes) 118.980740\n",
      "best mean reward 119.230809\n",
      "running time 1398.471662\n",
      "Train_EnvstepsSoFar : 282001\n",
      "Train_AverageReturn : 118.9807401068743\n",
      "Train_BestReturn : 119.23080949441015\n",
      "TimeSinceStart : 1398.4716620445251\n",
      "Training Loss : 0.4458162784576416\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 283001\n",
      "mean reward (100 episodes) 120.998597\n",
      "best mean reward 120.998597\n",
      "running time 1402.844044\n",
      "Train_EnvstepsSoFar : 283001\n",
      "Train_AverageReturn : 120.99859735789231\n",
      "Train_BestReturn : 120.99859735789231\n",
      "TimeSinceStart : 1402.844043970108\n",
      "Training Loss : 0.888636589050293\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 284001\n",
      "mean reward (100 episodes) 120.890689\n",
      "best mean reward 120.998597\n",
      "running time 1407.836986\n",
      "Train_EnvstepsSoFar : 284001\n",
      "Train_AverageReturn : 120.89068907343548\n",
      "Train_BestReturn : 120.99859735789231\n",
      "TimeSinceStart : 1407.836986064911\n",
      "Training Loss : 0.2852066159248352\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 285001\n",
      "mean reward (100 episodes) 122.620785\n",
      "best mean reward 122.620785\n",
      "running time 1413.673728\n",
      "Train_EnvstepsSoFar : 285001\n",
      "Train_AverageReturn : 122.62078504529595\n",
      "Train_BestReturn : 122.62078504529595\n",
      "TimeSinceStart : 1413.6737279891968\n",
      "Training Loss : 0.8168720006942749\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 286001\n",
      "mean reward (100 episodes) 125.207140\n",
      "best mean reward 125.207140\n",
      "running time 1419.152452\n",
      "Train_EnvstepsSoFar : 286001\n",
      "Train_AverageReturn : 125.20713955089415\n",
      "Train_BestReturn : 125.20713955089415\n",
      "TimeSinceStart : 1419.152451992035\n",
      "Training Loss : 3.05960750579834\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 287001\n",
      "mean reward (100 episodes) 125.502864\n",
      "best mean reward 125.502864\n",
      "running time 1427.797555\n",
      "Train_EnvstepsSoFar : 287001\n",
      "Train_AverageReturn : 125.50286352521879\n",
      "Train_BestReturn : 125.50286352521879\n",
      "TimeSinceStart : 1427.7975549697876\n",
      "Training Loss : 0.9022566676139832\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 288001\n",
      "mean reward (100 episodes) 124.618015\n",
      "best mean reward 125.502864\n",
      "running time 1436.395109\n",
      "Train_EnvstepsSoFar : 288001\n",
      "Train_AverageReturn : 124.61801535884723\n",
      "Train_BestReturn : 125.50286352521879\n",
      "TimeSinceStart : 1436.3951091766357\n",
      "Training Loss : 0.3134676516056061\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 289001\n",
      "mean reward (100 episodes) 125.783311\n",
      "best mean reward 125.783311\n",
      "running time 1442.472992\n",
      "Train_EnvstepsSoFar : 289001\n",
      "Train_AverageReturn : 125.78331104945627\n",
      "Train_BestReturn : 125.78331104945627\n",
      "TimeSinceStart : 1442.4729919433594\n",
      "Training Loss : 0.3396463394165039\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 290001\n",
      "mean reward (100 episodes) 125.400452\n",
      "best mean reward 125.783311\n",
      "running time 1449.317252\n",
      "Train_EnvstepsSoFar : 290001\n",
      "Train_AverageReturn : 125.40045243844017\n",
      "Train_BestReturn : 125.78331104945627\n",
      "TimeSinceStart : 1449.3172521591187\n",
      "Training Loss : 0.15838004648685455\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 291001\n",
      "mean reward (100 episodes) 127.422617\n",
      "best mean reward 127.422617\n",
      "running time 1453.523656\n",
      "Train_EnvstepsSoFar : 291001\n",
      "Train_AverageReturn : 127.42261663497726\n",
      "Train_BestReturn : 127.42261663497726\n",
      "TimeSinceStart : 1453.5236558914185\n",
      "Training Loss : 0.4985593855381012\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 292001\n",
      "mean reward (100 episodes) 128.426695\n",
      "best mean reward 128.426695\n",
      "running time 1459.500708\n",
      "Train_EnvstepsSoFar : 292001\n",
      "Train_AverageReturn : 128.42669476011156\n",
      "Train_BestReturn : 128.42669476011156\n",
      "TimeSinceStart : 1459.50070810318\n",
      "Training Loss : 1.0801539421081543\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 293001\n",
      "mean reward (100 episodes) 129.034768\n",
      "best mean reward 129.034768\n",
      "running time 1465.660877\n",
      "Train_EnvstepsSoFar : 293001\n",
      "Train_AverageReturn : 129.0347682052425\n",
      "Train_BestReturn : 129.0347682052425\n",
      "TimeSinceStart : 1465.6608769893646\n",
      "Training Loss : 1.0886626243591309\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 294001\n",
      "mean reward (100 episodes) 128.049273\n",
      "best mean reward 129.034768\n",
      "running time 1469.565235\n",
      "Train_EnvstepsSoFar : 294001\n",
      "Train_AverageReturn : 128.0492733127955\n",
      "Train_BestReturn : 129.0347682052425\n",
      "TimeSinceStart : 1469.5652348995209\n",
      "Training Loss : 4.9558210372924805\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 295001\n",
      "mean reward (100 episodes) 132.250974\n",
      "best mean reward 132.250974\n",
      "running time 1473.405418\n",
      "Train_EnvstepsSoFar : 295001\n",
      "Train_AverageReturn : 132.25097415448255\n",
      "Train_BestReturn : 132.25097415448255\n",
      "TimeSinceStart : 1473.405417919159\n",
      "Training Loss : 1.7300565242767334\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 296001\n",
      "mean reward (100 episodes) 135.472706\n",
      "best mean reward 135.472706\n",
      "running time 1477.140867\n",
      "Train_EnvstepsSoFar : 296001\n",
      "Train_AverageReturn : 135.47270576227484\n",
      "Train_BestReturn : 135.47270576227484\n",
      "TimeSinceStart : 1477.1408669948578\n",
      "Training Loss : 0.14052310585975647\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 297001\n",
      "mean reward (100 episodes) 136.851126\n",
      "best mean reward 136.851126\n",
      "running time 1480.807809\n",
      "Train_EnvstepsSoFar : 297001\n",
      "Train_AverageReturn : 136.8511261812021\n",
      "Train_BestReturn : 136.8511261812021\n",
      "TimeSinceStart : 1480.8078088760376\n",
      "Training Loss : 0.474625825881958\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 298001\n",
      "mean reward (100 episodes) 133.411256\n",
      "best mean reward 136.851126\n",
      "running time 1484.645795\n",
      "Train_EnvstepsSoFar : 298001\n",
      "Train_AverageReturn : 133.4112561541634\n",
      "Train_BestReturn : 136.8511261812021\n",
      "TimeSinceStart : 1484.6457951068878\n",
      "Training Loss : 0.8796972036361694\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 299001\n",
      "mean reward (100 episodes) 130.830482\n",
      "best mean reward 136.851126\n",
      "running time 1489.184239\n",
      "Train_EnvstepsSoFar : 299001\n",
      "Train_AverageReturn : 130.83048249687172\n",
      "Train_BestReturn : 136.8511261812021\n",
      "TimeSinceStart : 1489.1842391490936\n",
      "Training Loss : 0.6025581359863281\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 300001\n",
      "mean reward (100 episodes) 127.614571\n",
      "best mean reward 136.851126\n",
      "running time 1492.971054\n",
      "Train_EnvstepsSoFar : 300001\n",
      "Train_AverageReturn : 127.61457123667805\n",
      "Train_BestReturn : 136.8511261812021\n",
      "TimeSinceStart : 1492.9710540771484\n",
      "Training Loss : 0.3744361698627472\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 301001\n",
      "mean reward (100 episodes) 125.569807\n",
      "best mean reward 136.851126\n",
      "running time 1497.078213\n",
      "Train_EnvstepsSoFar : 301001\n",
      "Train_AverageReturn : 125.56980666971405\n",
      "Train_BestReturn : 136.8511261812021\n",
      "TimeSinceStart : 1497.078212738037\n",
      "Training Loss : 0.41715720295906067\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 302001\n",
      "mean reward (100 episodes) 130.230700\n",
      "best mean reward 136.851126\n",
      "running time 1500.755052\n",
      "Train_EnvstepsSoFar : 302001\n",
      "Train_AverageReturn : 130.23070004941178\n",
      "Train_BestReturn : 136.8511261812021\n",
      "TimeSinceStart : 1500.7550520896912\n",
      "Training Loss : 0.1345711201429367\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 303001\n",
      "mean reward (100 episodes) 134.431199\n",
      "best mean reward 136.851126\n",
      "running time 1504.458905\n",
      "Train_EnvstepsSoFar : 303001\n",
      "Train_AverageReturn : 134.43119915522132\n",
      "Train_BestReturn : 136.8511261812021\n",
      "TimeSinceStart : 1504.4589049816132\n",
      "Training Loss : 0.21718759834766388\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 304001\n",
      "mean reward (100 episodes) 141.643881\n",
      "best mean reward 141.643881\n",
      "running time 1508.170427\n",
      "Train_EnvstepsSoFar : 304001\n",
      "Train_AverageReturn : 141.64388144943734\n",
      "Train_BestReturn : 141.64388144943734\n",
      "TimeSinceStart : 1508.1704268455505\n",
      "Training Loss : 0.5892577767372131\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 305001\n",
      "mean reward (100 episodes) 142.396374\n",
      "best mean reward 142.396374\n",
      "running time 1511.954394\n",
      "Train_EnvstepsSoFar : 305001\n",
      "Train_AverageReturn : 142.39637435359896\n",
      "Train_BestReturn : 142.39637435359896\n",
      "TimeSinceStart : 1511.9543941020966\n",
      "Training Loss : 2.302568197250366\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 306001\n",
      "mean reward (100 episodes) 142.706172\n",
      "best mean reward 142.706172\n",
      "running time 1515.756134\n",
      "Train_EnvstepsSoFar : 306001\n",
      "Train_AverageReturn : 142.7061724744285\n",
      "Train_BestReturn : 142.7061724744285\n",
      "TimeSinceStart : 1515.7561337947845\n",
      "Training Loss : 0.6415659189224243\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 307001\n",
      "mean reward (100 episodes) 145.374134\n",
      "best mean reward 145.374134\n",
      "running time 1519.354163\n",
      "Train_EnvstepsSoFar : 307001\n",
      "Train_AverageReturn : 145.37413381555686\n",
      "Train_BestReturn : 145.37413381555686\n",
      "TimeSinceStart : 1519.3541629314423\n",
      "Training Loss : 0.7461428046226501\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 308001\n",
      "mean reward (100 episodes) 148.268216\n",
      "best mean reward 148.268216\n",
      "running time 1523.129419\n",
      "Train_EnvstepsSoFar : 308001\n",
      "Train_AverageReturn : 148.2682160660391\n",
      "Train_BestReturn : 148.2682160660391\n",
      "TimeSinceStart : 1523.1294190883636\n",
      "Training Loss : 0.2035110592842102\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 309001\n",
      "mean reward (100 episodes) 151.294436\n",
      "best mean reward 151.294436\n",
      "running time 1527.112998\n",
      "Train_EnvstepsSoFar : 309001\n",
      "Train_AverageReturn : 151.294435537452\n",
      "Train_BestReturn : 151.294435537452\n",
      "TimeSinceStart : 1527.1129977703094\n",
      "Training Loss : 0.23220670223236084\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 310001\n",
      "mean reward (100 episodes) 150.647748\n",
      "best mean reward 151.294436\n",
      "running time 1530.930380\n",
      "Train_EnvstepsSoFar : 310001\n",
      "Train_AverageReturn : 150.64774750814811\n",
      "Train_BestReturn : 151.294435537452\n",
      "TimeSinceStart : 1530.9303801059723\n",
      "Training Loss : 1.0380855798721313\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 311001\n",
      "mean reward (100 episodes) 146.891408\n",
      "best mean reward 151.294436\n",
      "running time 1534.934497\n",
      "Train_EnvstepsSoFar : 311001\n",
      "Train_AverageReturn : 146.89140771946438\n",
      "Train_BestReturn : 151.294435537452\n",
      "TimeSinceStart : 1534.9344968795776\n",
      "Training Loss : 0.24575018882751465\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 312001\n",
      "mean reward (100 episodes) 142.543742\n",
      "best mean reward 151.294436\n",
      "running time 1538.704239\n",
      "Train_EnvstepsSoFar : 312001\n",
      "Train_AverageReturn : 142.5437422076516\n",
      "Train_BestReturn : 151.294435537452\n",
      "TimeSinceStart : 1538.7042391300201\n",
      "Training Loss : 0.40241214632987976\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 313001\n",
      "mean reward (100 episodes) 143.956008\n",
      "best mean reward 151.294436\n",
      "running time 1542.606688\n",
      "Train_EnvstepsSoFar : 313001\n",
      "Train_AverageReturn : 143.9560080832109\n",
      "Train_BestReturn : 151.294435537452\n",
      "TimeSinceStart : 1542.6066880226135\n",
      "Training Loss : 0.27585655450820923\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 314001\n",
      "mean reward (100 episodes) 143.777730\n",
      "best mean reward 151.294436\n",
      "running time 1546.725252\n",
      "Train_EnvstepsSoFar : 314001\n",
      "Train_AverageReturn : 143.7777302920979\n",
      "Train_BestReturn : 151.294435537452\n",
      "TimeSinceStart : 1546.7252521514893\n",
      "Training Loss : 4.780524730682373\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 315001\n",
      "mean reward (100 episodes) 144.328879\n",
      "best mean reward 151.294436\n",
      "running time 1550.527994\n",
      "Train_EnvstepsSoFar : 315001\n",
      "Train_AverageReturn : 144.3288791631076\n",
      "Train_BestReturn : 151.294435537452\n",
      "TimeSinceStart : 1550.5279941558838\n",
      "Training Loss : 0.1750425398349762\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 316001\n",
      "mean reward (100 episodes) 147.808757\n",
      "best mean reward 151.294436\n",
      "running time 1554.906645\n",
      "Train_EnvstepsSoFar : 316001\n",
      "Train_AverageReturn : 147.80875709925857\n",
      "Train_BestReturn : 151.294435537452\n",
      "TimeSinceStart : 1554.9066450595856\n",
      "Training Loss : 0.13474895060062408\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 317001\n",
      "mean reward (100 episodes) 147.544038\n",
      "best mean reward 151.294436\n",
      "running time 1558.966629\n",
      "Train_EnvstepsSoFar : 317001\n",
      "Train_AverageReturn : 147.54403821351727\n",
      "Train_BestReturn : 151.294435537452\n",
      "TimeSinceStart : 1558.9666290283203\n",
      "Training Loss : 0.617888867855072\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 318001\n",
      "mean reward (100 episodes) 149.474139\n",
      "best mean reward 151.294436\n",
      "running time 1562.726366\n",
      "Train_EnvstepsSoFar : 318001\n",
      "Train_AverageReturn : 149.474138719292\n",
      "Train_BestReturn : 151.294435537452\n",
      "TimeSinceStart : 1562.7263658046722\n",
      "Training Loss : 2.3065507411956787\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 319001\n",
      "mean reward (100 episodes) 143.346553\n",
      "best mean reward 151.294436\n",
      "running time 1566.554368\n",
      "Train_EnvstepsSoFar : 319001\n",
      "Train_AverageReturn : 143.34655288742172\n",
      "Train_BestReturn : 151.294435537452\n",
      "TimeSinceStart : 1566.554368019104\n",
      "Training Loss : 1.1845017671585083\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 320001\n",
      "mean reward (100 episodes) 139.775496\n",
      "best mean reward 151.294436\n",
      "running time 1570.919694\n",
      "Train_EnvstepsSoFar : 320001\n",
      "Train_AverageReturn : 139.775495914883\n",
      "Train_BestReturn : 151.294435537452\n",
      "TimeSinceStart : 1570.9196939468384\n",
      "Training Loss : 2.2271857261657715\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 321001\n",
      "mean reward (100 episodes) 144.361501\n",
      "best mean reward 151.294436\n",
      "running time 1574.651762\n",
      "Train_EnvstepsSoFar : 321001\n",
      "Train_AverageReturn : 144.36150072964097\n",
      "Train_BestReturn : 151.294435537452\n",
      "TimeSinceStart : 1574.6517617702484\n",
      "Training Loss : 0.5693115592002869\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 322001\n",
      "mean reward (100 episodes) 140.098561\n",
      "best mean reward 151.294436\n",
      "running time 1578.549531\n",
      "Train_EnvstepsSoFar : 322001\n",
      "Train_AverageReturn : 140.09856067050706\n",
      "Train_BestReturn : 151.294435537452\n",
      "TimeSinceStart : 1578.5495307445526\n",
      "Training Loss : 0.8000121712684631\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 323001\n",
      "mean reward (100 episodes) 141.971682\n",
      "best mean reward 151.294436\n",
      "running time 1582.340695\n",
      "Train_EnvstepsSoFar : 323001\n",
      "Train_AverageReturn : 141.9716822564267\n",
      "Train_BestReturn : 151.294435537452\n",
      "TimeSinceStart : 1582.3406949043274\n",
      "Training Loss : 0.2975695729255676\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 324001\n",
      "mean reward (100 episodes) 142.573264\n",
      "best mean reward 151.294436\n",
      "running time 1585.943295\n",
      "Train_EnvstepsSoFar : 324001\n",
      "Train_AverageReturn : 142.57326395999135\n",
      "Train_BestReturn : 151.294435537452\n",
      "TimeSinceStart : 1585.9432950019836\n",
      "Training Loss : 1.3929719924926758\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 325001\n",
      "mean reward (100 episodes) 139.575785\n",
      "best mean reward 151.294436\n",
      "running time 1589.610002\n",
      "Train_EnvstepsSoFar : 325001\n",
      "Train_AverageReturn : 139.57578481310847\n",
      "Train_BestReturn : 151.294435537452\n",
      "TimeSinceStart : 1589.6100018024445\n",
      "Training Loss : 0.13427405059337616\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 326001\n",
      "mean reward (100 episodes) 137.661579\n",
      "best mean reward 151.294436\n",
      "running time 1593.307905\n",
      "Train_EnvstepsSoFar : 326001\n",
      "Train_AverageReturn : 137.66157887429338\n",
      "Train_BestReturn : 151.294435537452\n",
      "TimeSinceStart : 1593.307904958725\n",
      "Training Loss : 0.24721719324588776\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 327001\n",
      "mean reward (100 episodes) 139.313902\n",
      "best mean reward 151.294436\n",
      "running time 1597.014844\n",
      "Train_EnvstepsSoFar : 327001\n",
      "Train_AverageReturn : 139.3139020092948\n",
      "Train_BestReturn : 151.294435537452\n",
      "TimeSinceStart : 1597.0148439407349\n",
      "Training Loss : 0.20345699787139893\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 328001\n",
      "mean reward (100 episodes) 141.620733\n",
      "best mean reward 151.294436\n",
      "running time 1600.734772\n",
      "Train_EnvstepsSoFar : 328001\n",
      "Train_AverageReturn : 141.6207332717853\n",
      "Train_BestReturn : 151.294435537452\n",
      "TimeSinceStart : 1600.7347719669342\n",
      "Training Loss : 1.6571223735809326\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 329001\n",
      "mean reward (100 episodes) 141.967273\n",
      "best mean reward 151.294436\n",
      "running time 1604.344976\n",
      "Train_EnvstepsSoFar : 329001\n",
      "Train_AverageReturn : 141.9672726386817\n",
      "Train_BestReturn : 151.294435537452\n",
      "TimeSinceStart : 1604.3449759483337\n",
      "Training Loss : 0.310335636138916\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 330001\n",
      "mean reward (100 episodes) 144.110504\n",
      "best mean reward 151.294436\n",
      "running time 1608.035932\n",
      "Train_EnvstepsSoFar : 330001\n",
      "Train_AverageReturn : 144.11050396810106\n",
      "Train_BestReturn : 151.294435537452\n",
      "TimeSinceStart : 1608.0359318256378\n",
      "Training Loss : 1.6699399948120117\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 331001\n",
      "mean reward (100 episodes) 148.786609\n",
      "best mean reward 151.294436\n",
      "running time 1611.706395\n",
      "Train_EnvstepsSoFar : 331001\n",
      "Train_AverageReturn : 148.7866086954649\n",
      "Train_BestReturn : 151.294435537452\n",
      "TimeSinceStart : 1611.7063949108124\n",
      "Training Loss : 3.0236353874206543\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 332001\n",
      "mean reward (100 episodes) 152.809589\n",
      "best mean reward 152.809589\n",
      "running time 1615.341688\n",
      "Train_EnvstepsSoFar : 332001\n",
      "Train_AverageReturn : 152.8095886017503\n",
      "Train_BestReturn : 152.8095886017503\n",
      "TimeSinceStart : 1615.3416879177094\n",
      "Training Loss : 0.7515867352485657\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 333001\n",
      "mean reward (100 episodes) 154.684757\n",
      "best mean reward 154.684757\n",
      "running time 1619.010936\n",
      "Train_EnvstepsSoFar : 333001\n",
      "Train_AverageReturn : 154.68475680919536\n",
      "Train_BestReturn : 154.68475680919536\n",
      "TimeSinceStart : 1619.0109360218048\n",
      "Training Loss : 0.12977096438407898\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 334001\n",
      "mean reward (100 episodes) 161.902514\n",
      "best mean reward 161.902514\n",
      "running time 1622.678916\n",
      "Train_EnvstepsSoFar : 334001\n",
      "Train_AverageReturn : 161.90251389195845\n",
      "Train_BestReturn : 161.90251389195845\n",
      "TimeSinceStart : 1622.678915977478\n",
      "Training Loss : 1.6430859565734863\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 335001\n",
      "mean reward (100 episodes) 160.281365\n",
      "best mean reward 161.902514\n",
      "running time 1626.336778\n",
      "Train_EnvstepsSoFar : 335001\n",
      "Train_AverageReturn : 160.28136549113654\n",
      "Train_BestReturn : 161.90251389195845\n",
      "TimeSinceStart : 1626.3367779254913\n",
      "Training Loss : 0.16269363462924957\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 336001\n",
      "mean reward (100 episodes) 166.167337\n",
      "best mean reward 166.167337\n",
      "running time 1629.998416\n",
      "Train_EnvstepsSoFar : 336001\n",
      "Train_AverageReturn : 166.1673369362377\n",
      "Train_BestReturn : 166.1673369362377\n",
      "TimeSinceStart : 1629.9984159469604\n",
      "Training Loss : 0.7762815356254578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 337001\n",
      "mean reward (100 episodes) 163.101719\n",
      "best mean reward 166.167337\n",
      "running time 1633.707456\n",
      "Train_EnvstepsSoFar : 337001\n",
      "Train_AverageReturn : 163.10171880437264\n",
      "Train_BestReturn : 166.1673369362377\n",
      "TimeSinceStart : 1633.7074558734894\n",
      "Training Loss : 0.1022069901227951\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 338001\n",
      "mean reward (100 episodes) 160.719790\n",
      "best mean reward 166.167337\n",
      "running time 1637.378884\n",
      "Train_EnvstepsSoFar : 338001\n",
      "Train_AverageReturn : 160.7197895777591\n",
      "Train_BestReturn : 166.1673369362377\n",
      "TimeSinceStart : 1637.3788840770721\n",
      "Training Loss : 0.190801203250885\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 339001\n",
      "mean reward (100 episodes) 161.505905\n",
      "best mean reward 166.167337\n",
      "running time 1641.038642\n",
      "Train_EnvstepsSoFar : 339001\n",
      "Train_AverageReturn : 161.50590479405415\n",
      "Train_BestReturn : 166.1673369362377\n",
      "TimeSinceStart : 1641.0386419296265\n",
      "Training Loss : 0.07412020862102509\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 340001\n",
      "mean reward (100 episodes) 164.608094\n",
      "best mean reward 166.167337\n",
      "running time 1644.660938\n",
      "Train_EnvstepsSoFar : 340001\n",
      "Train_AverageReturn : 164.6080942095437\n",
      "Train_BestReturn : 166.1673369362377\n",
      "TimeSinceStart : 1644.6609380245209\n",
      "Training Loss : 0.6686381101608276\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 341001\n",
      "mean reward (100 episodes) 166.118038\n",
      "best mean reward 166.167337\n",
      "running time 1648.378166\n",
      "Train_EnvstepsSoFar : 341001\n",
      "Train_AverageReturn : 166.1180375221154\n",
      "Train_BestReturn : 166.1673369362377\n",
      "TimeSinceStart : 1648.378165960312\n",
      "Training Loss : 1.7478432655334473\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 342001\n",
      "mean reward (100 episodes) 169.148691\n",
      "best mean reward 169.148691\n",
      "running time 1651.976290\n",
      "Train_EnvstepsSoFar : 342001\n",
      "Train_AverageReturn : 169.14869073015078\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1651.976289987564\n",
      "Training Loss : 0.15474587678909302\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 343001\n",
      "mean reward (100 episodes) 167.212573\n",
      "best mean reward 169.148691\n",
      "running time 1655.542216\n",
      "Train_EnvstepsSoFar : 343001\n",
      "Train_AverageReturn : 167.21257267800152\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1655.5422158241272\n",
      "Training Loss : 0.5481130480766296\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 344001\n",
      "mean reward (100 episodes) 167.557673\n",
      "best mean reward 169.148691\n",
      "running time 1659.129345\n",
      "Train_EnvstepsSoFar : 344001\n",
      "Train_AverageReturn : 167.55767341321516\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1659.1293449401855\n",
      "Training Loss : 3.3308377265930176\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 345001\n",
      "mean reward (100 episodes) 162.063768\n",
      "best mean reward 169.148691\n",
      "running time 1662.754945\n",
      "Train_EnvstepsSoFar : 345001\n",
      "Train_AverageReturn : 162.06376774875068\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1662.7549450397491\n",
      "Training Loss : 0.21334293484687805\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 346001\n",
      "mean reward (100 episodes) 160.105304\n",
      "best mean reward 169.148691\n",
      "running time 1666.377735\n",
      "Train_EnvstepsSoFar : 346001\n",
      "Train_AverageReturn : 160.10530389767425\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1666.3777348995209\n",
      "Training Loss : 0.2196986973285675\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 347001\n",
      "mean reward (100 episodes) 158.480403\n",
      "best mean reward 169.148691\n",
      "running time 1670.105461\n",
      "Train_EnvstepsSoFar : 347001\n",
      "Train_AverageReturn : 158.4804026844592\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1670.105460882187\n",
      "Training Loss : 0.26061710715293884\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 348001\n",
      "mean reward (100 episodes) 154.985660\n",
      "best mean reward 169.148691\n",
      "running time 1673.696727\n",
      "Train_EnvstepsSoFar : 348001\n",
      "Train_AverageReturn : 154.98565971205144\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1673.6967270374298\n",
      "Training Loss : 0.22096994519233704\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 349001\n",
      "mean reward (100 episodes) 156.396892\n",
      "best mean reward 169.148691\n",
      "running time 1677.300476\n",
      "Train_EnvstepsSoFar : 349001\n",
      "Train_AverageReturn : 156.39689184236155\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1677.3004758358002\n",
      "Training Loss : 1.1097139120101929\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 350001\n",
      "mean reward (100 episodes) 153.498048\n",
      "best mean reward 169.148691\n",
      "running time 1680.890225\n",
      "Train_EnvstepsSoFar : 350001\n",
      "Train_AverageReturn : 153.4980483943466\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1680.8902249336243\n",
      "Training Loss : 0.23216162621974945\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 351001\n",
      "mean reward (100 episodes) 151.481716\n",
      "best mean reward 169.148691\n",
      "running time 1684.460534\n",
      "Train_EnvstepsSoFar : 351001\n",
      "Train_AverageReturn : 151.48171579652092\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1684.4605340957642\n",
      "Training Loss : 0.11135745793581009\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 352001\n",
      "mean reward (100 episodes) 159.468791\n",
      "best mean reward 169.148691\n",
      "running time 1688.045994\n",
      "Train_EnvstepsSoFar : 352001\n",
      "Train_AverageReturn : 159.4687908457822\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1688.0459940433502\n",
      "Training Loss : 0.9930565357208252\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 353001\n",
      "mean reward (100 episodes) 155.861051\n",
      "best mean reward 169.148691\n",
      "running time 1691.813794\n",
      "Train_EnvstepsSoFar : 353001\n",
      "Train_AverageReturn : 155.86105078677724\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1691.8137938976288\n",
      "Training Loss : 0.115390345454216\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 354001\n",
      "mean reward (100 episodes) 150.171135\n",
      "best mean reward 169.148691\n",
      "running time 1695.406076\n",
      "Train_EnvstepsSoFar : 354001\n",
      "Train_AverageReturn : 150.17113501237148\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1695.4060759544373\n",
      "Training Loss : 0.23524975776672363\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 355001\n",
      "mean reward (100 episodes) 150.940458\n",
      "best mean reward 169.148691\n",
      "running time 1698.999471\n",
      "Train_EnvstepsSoFar : 355001\n",
      "Train_AverageReturn : 150.9404582904342\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1698.9994711875916\n",
      "Training Loss : 0.17096781730651855\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 356001\n",
      "mean reward (100 episodes) 148.048174\n",
      "best mean reward 169.148691\n",
      "running time 1702.593574\n",
      "Train_EnvstepsSoFar : 356001\n",
      "Train_AverageReturn : 148.04817427644136\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1702.59357380867\n",
      "Training Loss : 15.079570770263672\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 357001\n",
      "mean reward (100 episodes) 142.942866\n",
      "best mean reward 169.148691\n",
      "running time 1706.210906\n",
      "Train_EnvstepsSoFar : 357001\n",
      "Train_AverageReturn : 142.94286624546814\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1706.2109060287476\n",
      "Training Loss : 0.2448432296514511\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 358001\n",
      "mean reward (100 episodes) 147.040681\n",
      "best mean reward 169.148691\n",
      "running time 1709.789082\n",
      "Train_EnvstepsSoFar : 358001\n",
      "Train_AverageReturn : 147.04068096646836\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1709.7890820503235\n",
      "Training Loss : 0.4090459942817688\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 359001\n",
      "mean reward (100 episodes) 145.072659\n",
      "best mean reward 169.148691\n",
      "running time 1713.373385\n",
      "Train_EnvstepsSoFar : 359001\n",
      "Train_AverageReturn : 145.0726594937029\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1713.3733849525452\n",
      "Training Loss : 0.4569365680217743\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 360001\n",
      "mean reward (100 episodes) 139.977337\n",
      "best mean reward 169.148691\n",
      "running time 1716.966740\n",
      "Train_EnvstepsSoFar : 360001\n",
      "Train_AverageReturn : 139.97733737958524\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1716.9667398929596\n",
      "Training Loss : 0.11134292185306549\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 361001\n",
      "mean reward (100 episodes) 131.371576\n",
      "best mean reward 169.148691\n",
      "running time 1720.530148\n",
      "Train_EnvstepsSoFar : 361001\n",
      "Train_AverageReturn : 131.37157584868544\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1720.5301480293274\n",
      "Training Loss : 1.143958568572998\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 362001\n",
      "mean reward (100 episodes) 121.412854\n",
      "best mean reward 169.148691\n",
      "running time 1724.113763\n",
      "Train_EnvstepsSoFar : 362001\n",
      "Train_AverageReturn : 121.41285375705802\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1724.1137630939484\n",
      "Training Loss : 0.06573998928070068\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 363001\n",
      "mean reward (100 episodes) 121.400398\n",
      "best mean reward 169.148691\n",
      "running time 1727.721507\n",
      "Train_EnvstepsSoFar : 363001\n",
      "Train_AverageReturn : 121.40039789396535\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1727.7215070724487\n",
      "Training Loss : 0.9194782972335815\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 364001\n",
      "mean reward (100 episodes) 113.804294\n",
      "best mean reward 169.148691\n",
      "running time 1731.510228\n",
      "Train_EnvstepsSoFar : 364001\n",
      "Train_AverageReturn : 113.80429382860748\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1731.5102279186249\n",
      "Training Loss : 1.7073488235473633\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 365001\n",
      "mean reward (100 episodes) 109.158556\n",
      "best mean reward 169.148691\n",
      "running time 1735.526414\n",
      "Train_EnvstepsSoFar : 365001\n",
      "Train_AverageReturn : 109.15855588121731\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1735.52641415596\n",
      "Training Loss : 2.863403558731079\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 366001\n",
      "mean reward (100 episodes) 102.727435\n",
      "best mean reward 169.148691\n",
      "running time 1739.396925\n",
      "Train_EnvstepsSoFar : 366001\n",
      "Train_AverageReturn : 102.72743510117817\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1739.3969249725342\n",
      "Training Loss : 0.068606436252594\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 367001\n",
      "mean reward (100 episodes) 97.198175\n",
      "best mean reward 169.148691\n",
      "running time 1742.969578\n",
      "Train_EnvstepsSoFar : 367001\n",
      "Train_AverageReturn : 97.19817460417553\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1742.9695780277252\n",
      "Training Loss : 1.76031494140625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 368001\n",
      "mean reward (100 episodes) 92.366209\n",
      "best mean reward 169.148691\n",
      "running time 1746.580168\n",
      "Train_EnvstepsSoFar : 368001\n",
      "Train_AverageReturn : 92.36620936202503\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1746.5801680088043\n",
      "Training Loss : 0.09034833312034607\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 369001\n",
      "mean reward (100 episodes) 92.612540\n",
      "best mean reward 169.148691\n",
      "running time 1750.138364\n",
      "Train_EnvstepsSoFar : 369001\n",
      "Train_AverageReturn : 92.61253959946166\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1750.1383638381958\n",
      "Training Loss : 1.4640686511993408\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 370001\n",
      "mean reward (100 episodes) 89.182532\n",
      "best mean reward 169.148691\n",
      "running time 1753.711164\n",
      "Train_EnvstepsSoFar : 370001\n",
      "Train_AverageReturn : 89.18253191614531\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1753.7111637592316\n",
      "Training Loss : 0.9818751215934753\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 371001\n",
      "mean reward (100 episodes) 87.222049\n",
      "best mean reward 169.148691\n",
      "running time 1757.255414\n",
      "Train_EnvstepsSoFar : 371001\n",
      "Train_AverageReturn : 87.2220485500586\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1757.2554140090942\n",
      "Training Loss : 0.08515217155218124\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 372001\n",
      "mean reward (100 episodes) 86.252036\n",
      "best mean reward 169.148691\n",
      "running time 1760.832396\n",
      "Train_EnvstepsSoFar : 372001\n",
      "Train_AverageReturn : 86.2520361232925\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1760.832396030426\n",
      "Training Loss : 0.2222416251897812\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 373001\n",
      "mean reward (100 episodes) 80.189903\n",
      "best mean reward 169.148691\n",
      "running time 1764.427721\n",
      "Train_EnvstepsSoFar : 373001\n",
      "Train_AverageReturn : 80.18990264503287\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1764.4277210235596\n",
      "Training Loss : 0.0916561707854271\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 374001\n",
      "mean reward (100 episodes) 84.799759\n",
      "best mean reward 169.148691\n",
      "running time 1768.011065\n",
      "Train_EnvstepsSoFar : 374001\n",
      "Train_AverageReturn : 84.7997586941989\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1768.011065006256\n",
      "Training Loss : 0.17435112595558167\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 375001\n",
      "mean reward (100 episodes) 77.316423\n",
      "best mean reward 169.148691\n",
      "running time 1771.575374\n",
      "Train_EnvstepsSoFar : 375001\n",
      "Train_AverageReturn : 77.31642283084\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1771.5753741264343\n",
      "Training Loss : 1.6774591207504272\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 376001\n",
      "mean reward (100 episodes) 69.182987\n",
      "best mean reward 169.148691\n",
      "running time 1775.199096\n",
      "Train_EnvstepsSoFar : 376001\n",
      "Train_AverageReturn : 69.1829869502503\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1775.1990959644318\n",
      "Training Loss : 0.40065255761146545\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 377001\n",
      "mean reward (100 episodes) 68.078809\n",
      "best mean reward 169.148691\n",
      "running time 1778.755923\n",
      "Train_EnvstepsSoFar : 377001\n",
      "Train_AverageReturn : 68.07880856797085\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1778.755922794342\n",
      "Training Loss : 0.1806587278842926\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 378001\n",
      "mean reward (100 episodes) 64.094599\n",
      "best mean reward 169.148691\n",
      "running time 1782.345318\n",
      "Train_EnvstepsSoFar : 378001\n",
      "Train_AverageReturn : 64.09459894819098\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1782.3453178405762\n",
      "Training Loss : 0.09114307165145874\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 379001\n",
      "mean reward (100 episodes) 69.804899\n",
      "best mean reward 169.148691\n",
      "running time 1785.899065\n",
      "Train_EnvstepsSoFar : 379001\n",
      "Train_AverageReturn : 69.80489857064934\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1785.8990650177002\n",
      "Training Loss : 0.2862831950187683\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 380001\n",
      "mean reward (100 episodes) 67.130533\n",
      "best mean reward 169.148691\n",
      "running time 1789.496233\n",
      "Train_EnvstepsSoFar : 380001\n",
      "Train_AverageReturn : 67.13053304959585\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1789.4962329864502\n",
      "Training Loss : 0.21033886075019836\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 381001\n",
      "mean reward (100 episodes) 69.264997\n",
      "best mean reward 169.148691\n",
      "running time 1793.120000\n",
      "Train_EnvstepsSoFar : 381001\n",
      "Train_AverageReturn : 69.2649970978801\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1793.119999885559\n",
      "Training Loss : 0.11849690973758698\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 382001\n",
      "mean reward (100 episodes) 71.126934\n",
      "best mean reward 169.148691\n",
      "running time 1796.738592\n",
      "Train_EnvstepsSoFar : 382001\n",
      "Train_AverageReturn : 71.1269335044984\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1796.7385919094086\n",
      "Training Loss : 0.9562792778015137\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 383001\n",
      "mean reward (100 episodes) 67.230597\n",
      "best mean reward 169.148691\n",
      "running time 1800.297986\n",
      "Train_EnvstepsSoFar : 383001\n",
      "Train_AverageReturn : 67.23059660297058\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1800.2979860305786\n",
      "Training Loss : 0.5407491326332092\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 384001\n",
      "mean reward (100 episodes) 67.456695\n",
      "best mean reward 169.148691\n",
      "running time 1803.863750\n",
      "Train_EnvstepsSoFar : 384001\n",
      "Train_AverageReturn : 67.45669485540117\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1803.863749742508\n",
      "Training Loss : 0.10438671708106995\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 385001\n",
      "mean reward (100 episodes) 61.638783\n",
      "best mean reward 169.148691\n",
      "running time 1807.494111\n",
      "Train_EnvstepsSoFar : 385001\n",
      "Train_AverageReturn : 61.63878316886198\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1807.4941108226776\n",
      "Training Loss : 0.2773411273956299\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 386001\n",
      "mean reward (100 episodes) 67.280372\n",
      "best mean reward 169.148691\n",
      "running time 1811.040276\n",
      "Train_EnvstepsSoFar : 386001\n",
      "Train_AverageReturn : 67.28037192992191\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1811.0402760505676\n",
      "Training Loss : 0.6635953783988953\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 387001\n",
      "mean reward (100 episodes) 63.303095\n",
      "best mean reward 169.148691\n",
      "running time 1814.645539\n",
      "Train_EnvstepsSoFar : 387001\n",
      "Train_AverageReturn : 63.3030954161811\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1814.6455390453339\n",
      "Training Loss : 0.4884265065193176\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 388001\n",
      "mean reward (100 episodes) 72.212779\n",
      "best mean reward 169.148691\n",
      "running time 1818.195476\n",
      "Train_EnvstepsSoFar : 388001\n",
      "Train_AverageReturn : 72.21277924412638\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1818.1954758167267\n",
      "Training Loss : 1.309867024421692\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 389001\n",
      "mean reward (100 episodes) 76.199907\n",
      "best mean reward 169.148691\n",
      "running time 1821.713519\n",
      "Train_EnvstepsSoFar : 389001\n",
      "Train_AverageReturn : 76.19990699127409\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1821.7135190963745\n",
      "Training Loss : 1.0896224975585938\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 390001\n",
      "mean reward (100 episodes) 84.032451\n",
      "best mean reward 169.148691\n",
      "running time 1825.285841\n",
      "Train_EnvstepsSoFar : 390001\n",
      "Train_AverageReturn : 84.0324513082534\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1825.2858407497406\n",
      "Training Loss : 0.0777888223528862\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 391001\n",
      "mean reward (100 episodes) 87.371799\n",
      "best mean reward 169.148691\n",
      "running time 1828.920238\n",
      "Train_EnvstepsSoFar : 391001\n",
      "Train_AverageReturn : 87.37179917523652\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1828.920238018036\n",
      "Training Loss : 0.1103786751627922\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 392001\n",
      "mean reward (100 episodes) 85.707775\n",
      "best mean reward 169.148691\n",
      "running time 1832.460870\n",
      "Train_EnvstepsSoFar : 392001\n",
      "Train_AverageReturn : 85.7077751031048\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1832.4608697891235\n",
      "Training Loss : 0.17409664392471313\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 393001\n",
      "mean reward (100 episodes) 88.443143\n",
      "best mean reward 169.148691\n",
      "running time 1836.001034\n",
      "Train_EnvstepsSoFar : 393001\n",
      "Train_AverageReturn : 88.44314335540132\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1836.0010340213776\n",
      "Training Loss : 0.35232919454574585\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 394001\n",
      "mean reward (100 episodes) 85.772579\n",
      "best mean reward 169.148691\n",
      "running time 1839.560522\n",
      "Train_EnvstepsSoFar : 394001\n",
      "Train_AverageReturn : 85.77257936660432\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1839.5605218410492\n",
      "Training Loss : 1.0672008991241455\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 395001\n",
      "mean reward (100 episodes) 88.664069\n",
      "best mean reward 169.148691\n",
      "running time 1843.085909\n",
      "Train_EnvstepsSoFar : 395001\n",
      "Train_AverageReturn : 88.66406933496852\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1843.0859088897705\n",
      "Training Loss : 1.273773431777954\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 396001\n",
      "mean reward (100 episodes) 90.819466\n",
      "best mean reward 169.148691\n",
      "running time 1846.687963\n",
      "Train_EnvstepsSoFar : 396001\n",
      "Train_AverageReturn : 90.8194656109477\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1846.687962770462\n",
      "Training Loss : 0.31389811635017395\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 397001\n",
      "mean reward (100 episodes) 89.862877\n",
      "best mean reward 169.148691\n",
      "running time 1850.352423\n",
      "Train_EnvstepsSoFar : 397001\n",
      "Train_AverageReturn : 89.86287685724754\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1850.352422952652\n",
      "Training Loss : 0.18445786833763123\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 398001\n",
      "mean reward (100 episodes) 96.581128\n",
      "best mean reward 169.148691\n",
      "running time 1853.923320\n",
      "Train_EnvstepsSoFar : 398001\n",
      "Train_AverageReturn : 96.58112775514819\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1853.923320055008\n",
      "Training Loss : 1.991804838180542\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 399001\n",
      "mean reward (100 episodes) 103.507663\n",
      "best mean reward 169.148691\n",
      "running time 1857.457874\n",
      "Train_EnvstepsSoFar : 399001\n",
      "Train_AverageReturn : 103.5076629773567\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1857.4578738212585\n",
      "Training Loss : 1.5725816488265991\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 400001\n",
      "mean reward (100 episodes) 95.848969\n",
      "best mean reward 169.148691\n",
      "running time 1861.103984\n",
      "Train_EnvstepsSoFar : 400001\n",
      "Train_AverageReturn : 95.84896858844164\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1861.1039838790894\n",
      "Training Loss : 0.9627918601036072\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 401001\n",
      "mean reward (100 episodes) 96.568460\n",
      "best mean reward 169.148691\n",
      "running time 1864.666819\n",
      "Train_EnvstepsSoFar : 401001\n",
      "Train_AverageReturn : 96.56845998669826\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1864.6668190956116\n",
      "Training Loss : 0.14606566727161407\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 402001\n",
      "mean reward (100 episodes) 98.661562\n",
      "best mean reward 169.148691\n",
      "running time 1868.213466\n",
      "Train_EnvstepsSoFar : 402001\n",
      "Train_AverageReturn : 98.66156161711189\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1868.21346616745\n",
      "Training Loss : 0.2660028636455536\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 403001\n",
      "mean reward (100 episodes) 90.087953\n",
      "best mean reward 169.148691\n",
      "running time 1871.822812\n",
      "Train_EnvstepsSoFar : 403001\n",
      "Train_AverageReturn : 90.08795270062663\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1871.8228118419647\n",
      "Training Loss : 0.27063509821891785\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 404001\n",
      "mean reward (100 episodes) 90.878493\n",
      "best mean reward 169.148691\n",
      "running time 1875.360139\n",
      "Train_EnvstepsSoFar : 404001\n",
      "Train_AverageReturn : 90.87849317579483\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1875.3601388931274\n",
      "Training Loss : 0.4855605959892273\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 405001\n",
      "mean reward (100 episodes) 92.318673\n",
      "best mean reward 169.148691\n",
      "running time 1878.898858\n",
      "Train_EnvstepsSoFar : 405001\n",
      "Train_AverageReturn : 92.31867331420528\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1878.898857831955\n",
      "Training Loss : 0.8947563171386719\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 406001\n",
      "mean reward (100 episodes) 87.539728\n",
      "best mean reward 169.148691\n",
      "running time 1882.485595\n",
      "Train_EnvstepsSoFar : 406001\n",
      "Train_AverageReturn : 87.53972799625537\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1882.4855947494507\n",
      "Training Loss : 0.4355836510658264\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 407001\n",
      "mean reward (100 episodes) 82.775663\n",
      "best mean reward 169.148691\n",
      "running time 1886.020665\n",
      "Train_EnvstepsSoFar : 407001\n",
      "Train_AverageReturn : 82.77566252258644\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1886.0206651687622\n",
      "Training Loss : 0.11956676840782166\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 408001\n",
      "mean reward (100 episodes) 78.391852\n",
      "best mean reward 169.148691\n",
      "running time 1889.520150\n",
      "Train_EnvstepsSoFar : 408001\n",
      "Train_AverageReturn : 78.39185152517005\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1889.5201499462128\n",
      "Training Loss : 0.9051802158355713\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 409001\n",
      "mean reward (100 episodes) 82.433275\n",
      "best mean reward 169.148691\n",
      "running time 1893.075316\n",
      "Train_EnvstepsSoFar : 409001\n",
      "Train_AverageReturn : 82.43327491133245\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1893.075315952301\n",
      "Training Loss : 0.14332440495491028\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 410001\n",
      "mean reward (100 episodes) 77.178047\n",
      "best mean reward 169.148691\n",
      "running time 1896.633032\n",
      "Train_EnvstepsSoFar : 410001\n",
      "Train_AverageReturn : 77.17804721476024\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1896.6330318450928\n",
      "Training Loss : 4.5730977058410645\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 411001\n",
      "mean reward (100 episodes) 77.162245\n",
      "best mean reward 169.148691\n",
      "running time 1900.178376\n",
      "Train_EnvstepsSoFar : 411001\n",
      "Train_AverageReturn : 77.16224473398303\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1900.1783759593964\n",
      "Training Loss : 2.116037368774414\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 412001\n",
      "mean reward (100 episodes) 81.685929\n",
      "best mean reward 169.148691\n",
      "running time 1903.703663\n",
      "Train_EnvstepsSoFar : 412001\n",
      "Train_AverageReturn : 81.68592935771693\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1903.703663110733\n",
      "Training Loss : 0.13505347073078156\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 413001\n",
      "mean reward (100 episodes) 75.817190\n",
      "best mean reward 169.148691\n",
      "running time 1907.222930\n",
      "Train_EnvstepsSoFar : 413001\n",
      "Train_AverageReturn : 75.81718960365586\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1907.2229299545288\n",
      "Training Loss : 8.278558731079102\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 414001\n",
      "mean reward (100 episodes) 73.866531\n",
      "best mean reward 169.148691\n",
      "running time 1911.198006\n",
      "Train_EnvstepsSoFar : 414001\n",
      "Train_AverageReturn : 73.866531345441\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1911.198005914688\n",
      "Training Loss : 0.25142738223075867\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 415001\n",
      "mean reward (100 episodes) 78.980690\n",
      "best mean reward 169.148691\n",
      "running time 1914.908522\n",
      "Train_EnvstepsSoFar : 415001\n",
      "Train_AverageReturn : 78.98068998189548\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1914.9085218906403\n",
      "Training Loss : 4.112930774688721\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 416001\n",
      "mean reward (100 episodes) 78.799788\n",
      "best mean reward 169.148691\n",
      "running time 1918.474589\n",
      "Train_EnvstepsSoFar : 416001\n",
      "Train_AverageReturn : 78.79978810397955\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1918.4745888710022\n",
      "Training Loss : 3.589016914367676\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 417001\n",
      "mean reward (100 episodes) 78.330766\n",
      "best mean reward 169.148691\n",
      "running time 1921.999799\n",
      "Train_EnvstepsSoFar : 417001\n",
      "Train_AverageReturn : 78.33076554901689\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1921.9997990131378\n",
      "Training Loss : 2.9355528354644775\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 418001\n",
      "mean reward (100 episodes) 89.209009\n",
      "best mean reward 169.148691\n",
      "running time 1925.545426\n",
      "Train_EnvstepsSoFar : 418001\n",
      "Train_AverageReturn : 89.20900895642355\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1925.5454258918762\n",
      "Training Loss : 0.14318394660949707\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 419001\n",
      "mean reward (100 episodes) 87.452388\n",
      "best mean reward 169.148691\n",
      "running time 1929.109791\n",
      "Train_EnvstepsSoFar : 419001\n",
      "Train_AverageReturn : 87.4523875462353\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1929.1097910404205\n",
      "Training Loss : 0.3318968415260315\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 420001\n",
      "mean reward (100 episodes) 85.504618\n",
      "best mean reward 169.148691\n",
      "running time 1932.682210\n",
      "Train_EnvstepsSoFar : 420001\n",
      "Train_AverageReturn : 85.50461754721698\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1932.682209968567\n",
      "Training Loss : 0.4193407893180847\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 421001\n",
      "mean reward (100 episodes) 84.718012\n",
      "best mean reward 169.148691\n",
      "running time 1936.268491\n",
      "Train_EnvstepsSoFar : 421001\n",
      "Train_AverageReturn : 84.71801174873085\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1936.2684910297394\n",
      "Training Loss : 0.24396510422229767\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 422001\n",
      "mean reward (100 episodes) 82.965001\n",
      "best mean reward 169.148691\n",
      "running time 1939.854733\n",
      "Train_EnvstepsSoFar : 422001\n",
      "Train_AverageReturn : 82.96500104693358\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1939.854732990265\n",
      "Training Loss : 0.264805406332016\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 423001\n",
      "mean reward (100 episodes) 87.448875\n",
      "best mean reward 169.148691\n",
      "running time 1943.402298\n",
      "Train_EnvstepsSoFar : 423001\n",
      "Train_AverageReturn : 87.44887546022335\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1943.4022979736328\n",
      "Training Loss : 0.33945316076278687\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 424001\n",
      "mean reward (100 episodes) 82.771252\n",
      "best mean reward 169.148691\n",
      "running time 1946.988360\n",
      "Train_EnvstepsSoFar : 424001\n",
      "Train_AverageReturn : 82.77125160958484\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1946.988359928131\n",
      "Training Loss : 0.15707272291183472\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 425001\n",
      "mean reward (100 episodes) 80.098098\n",
      "best mean reward 169.148691\n",
      "running time 1950.526788\n",
      "Train_EnvstepsSoFar : 425001\n",
      "Train_AverageReturn : 80.09809774491266\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1950.5267879962921\n",
      "Training Loss : 0.6697553396224976\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 426001\n",
      "mean reward (100 episodes) 78.196914\n",
      "best mean reward 169.148691\n",
      "running time 1954.068161\n",
      "Train_EnvstepsSoFar : 426001\n",
      "Train_AverageReturn : 78.19691365615051\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1954.0681610107422\n",
      "Training Loss : 0.3905058801174164\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 427001\n",
      "mean reward (100 episodes) 75.429474\n",
      "best mean reward 169.148691\n",
      "running time 1957.981681\n",
      "Train_EnvstepsSoFar : 427001\n",
      "Train_AverageReturn : 75.42947416281301\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1957.9816808700562\n",
      "Training Loss : 0.19302314519882202\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 428001\n",
      "mean reward (100 episodes) 71.650066\n",
      "best mean reward 169.148691\n",
      "running time 1961.532027\n",
      "Train_EnvstepsSoFar : 428001\n",
      "Train_AverageReturn : 71.65006645657853\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1961.5320267677307\n",
      "Training Loss : 0.16192075610160828\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 429001\n",
      "mean reward (100 episodes) 71.295934\n",
      "best mean reward 169.148691\n",
      "running time 1965.111262\n",
      "Train_EnvstepsSoFar : 429001\n",
      "Train_AverageReturn : 71.29593384249421\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1965.111261844635\n",
      "Training Loss : 0.32197147607803345\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 430001\n",
      "mean reward (100 episodes) 68.744503\n",
      "best mean reward 169.148691\n",
      "running time 1968.828213\n",
      "Train_EnvstepsSoFar : 430001\n",
      "Train_AverageReturn : 68.74450335438472\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1968.8282129764557\n",
      "Training Loss : 0.13860617578029633\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 431001\n",
      "mean reward (100 episodes) 63.912780\n",
      "best mean reward 169.148691\n",
      "running time 1974.336897\n",
      "Train_EnvstepsSoFar : 431001\n",
      "Train_AverageReturn : 63.91278027278006\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1974.3368968963623\n",
      "Training Loss : 0.473566472530365\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 432001\n",
      "mean reward (100 episodes) 65.886829\n",
      "best mean reward 169.148691\n",
      "running time 1978.070389\n",
      "Train_EnvstepsSoFar : 432001\n",
      "Train_AverageReturn : 65.88682898315706\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1978.070389032364\n",
      "Training Loss : 11.575053215026855\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 433001\n",
      "mean reward (100 episodes) 68.411936\n",
      "best mean reward 169.148691\n",
      "running time 1981.674471\n",
      "Train_EnvstepsSoFar : 433001\n",
      "Train_AverageReturn : 68.41193609577722\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1981.6744709014893\n",
      "Training Loss : 0.26903659105300903\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 434001\n",
      "mean reward (100 episodes) 62.553878\n",
      "best mean reward 169.148691\n",
      "running time 1985.314174\n",
      "Train_EnvstepsSoFar : 434001\n",
      "Train_AverageReturn : 62.55387800899155\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1985.3141739368439\n",
      "Training Loss : 0.4338352680206299\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 435001\n",
      "mean reward (100 episodes) 64.388116\n",
      "best mean reward 169.148691\n",
      "running time 1988.918339\n",
      "Train_EnvstepsSoFar : 435001\n",
      "Train_AverageReturn : 64.38811595395394\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1988.9183390140533\n",
      "Training Loss : 0.10285988450050354\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 436000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 436001\n",
      "mean reward (100 episodes) 72.416711\n",
      "best mean reward 169.148691\n",
      "running time 1992.795394\n",
      "Train_EnvstepsSoFar : 436001\n",
      "Train_AverageReturn : 72.416711322131\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1992.7953939437866\n",
      "Training Loss : 1.2731579542160034\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 437001\n",
      "mean reward (100 episodes) 78.975796\n",
      "best mean reward 169.148691\n",
      "running time 1996.447686\n",
      "Train_EnvstepsSoFar : 437001\n",
      "Train_AverageReturn : 78.97579559792422\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 1996.447685956955\n",
      "Training Loss : 0.23366093635559082\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 438001\n",
      "mean reward (100 episodes) 89.168852\n",
      "best mean reward 169.148691\n",
      "running time 2000.165910\n",
      "Train_EnvstepsSoFar : 438001\n",
      "Train_AverageReturn : 89.16885245916636\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2000.1659100055695\n",
      "Training Loss : 1.5142337083816528\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 439001\n",
      "mean reward (100 episodes) 91.564859\n",
      "best mean reward 169.148691\n",
      "running time 2003.899823\n",
      "Train_EnvstepsSoFar : 439001\n",
      "Train_AverageReturn : 91.56485928184168\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2003.8998229503632\n",
      "Training Loss : 5.4386749267578125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 440001\n",
      "mean reward (100 episodes) 88.067629\n",
      "best mean reward 169.148691\n",
      "running time 2007.788706\n",
      "Train_EnvstepsSoFar : 440001\n",
      "Train_AverageReturn : 88.06762934687077\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2007.7887060642242\n",
      "Training Loss : 0.27056074142456055\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 441001\n",
      "mean reward (100 episodes) 86.331953\n",
      "best mean reward 169.148691\n",
      "running time 2011.494321\n",
      "Train_EnvstepsSoFar : 441001\n",
      "Train_AverageReturn : 86.33195327959181\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2011.4943208694458\n",
      "Training Loss : 0.28674185276031494\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 442001\n",
      "mean reward (100 episodes) 85.411715\n",
      "best mean reward 169.148691\n",
      "running time 2015.121930\n",
      "Train_EnvstepsSoFar : 442001\n",
      "Train_AverageReturn : 85.41171490856276\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2015.121929883957\n",
      "Training Loss : 0.23688049614429474\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 443001\n",
      "mean reward (100 episodes) 86.177092\n",
      "best mean reward 169.148691\n",
      "running time 2018.706450\n",
      "Train_EnvstepsSoFar : 443001\n",
      "Train_AverageReturn : 86.17709224784267\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2018.7064499855042\n",
      "Training Loss : 0.2629730999469757\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 444001\n",
      "mean reward (100 episodes) 85.511613\n",
      "best mean reward 169.148691\n",
      "running time 2022.321227\n",
      "Train_EnvstepsSoFar : 444001\n",
      "Train_AverageReturn : 85.51161274649935\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2022.3212270736694\n",
      "Training Loss : 0.18655993044376373\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 445001\n",
      "mean reward (100 episodes) 91.804273\n",
      "best mean reward 169.148691\n",
      "running time 2025.989278\n",
      "Train_EnvstepsSoFar : 445001\n",
      "Train_AverageReturn : 91.80427319868969\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2025.9892778396606\n",
      "Training Loss : 0.29468584060668945\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 446001\n",
      "mean reward (100 episodes) 87.624038\n",
      "best mean reward 169.148691\n",
      "running time 2029.584872\n",
      "Train_EnvstepsSoFar : 446001\n",
      "Train_AverageReturn : 87.62403769972421\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2029.58487200737\n",
      "Training Loss : 0.28535452485084534\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 447001\n",
      "mean reward (100 episodes) 88.771373\n",
      "best mean reward 169.148691\n",
      "running time 2033.387268\n",
      "Train_EnvstepsSoFar : 447001\n",
      "Train_AverageReturn : 88.7713730376409\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2033.3872680664062\n",
      "Training Loss : 0.15573567152023315\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 448001\n",
      "mean reward (100 episodes) 87.827488\n",
      "best mean reward 169.148691\n",
      "running time 2037.334048\n",
      "Train_EnvstepsSoFar : 448001\n",
      "Train_AverageReturn : 87.82748777820014\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2037.3340480327606\n",
      "Training Loss : 0.10262210667133331\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 449001\n",
      "mean reward (100 episodes) 81.831005\n",
      "best mean reward 169.148691\n",
      "running time 2041.505372\n",
      "Train_EnvstepsSoFar : 449001\n",
      "Train_AverageReturn : 81.83100473487741\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2041.5053718090057\n",
      "Training Loss : 6.2428364753723145\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 450001\n",
      "mean reward (100 episodes) 75.275559\n",
      "best mean reward 169.148691\n",
      "running time 2045.075948\n",
      "Train_EnvstepsSoFar : 450001\n",
      "Train_AverageReturn : 75.27555944691284\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2045.0759479999542\n",
      "Training Loss : 0.3372078835964203\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 451001\n",
      "mean reward (100 episodes) 75.394011\n",
      "best mean reward 169.148691\n",
      "running time 2048.608089\n",
      "Train_EnvstepsSoFar : 451001\n",
      "Train_AverageReturn : 75.39401077391089\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2048.6080889701843\n",
      "Training Loss : 0.15151475369930267\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 452001\n",
      "mean reward (100 episodes) 74.822450\n",
      "best mean reward 169.148691\n",
      "running time 2052.222219\n",
      "Train_EnvstepsSoFar : 452001\n",
      "Train_AverageReturn : 74.82244983223971\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2052.222218990326\n",
      "Training Loss : 12.884632110595703\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 453001\n",
      "mean reward (100 episodes) 72.116092\n",
      "best mean reward 169.148691\n",
      "running time 2055.742053\n",
      "Train_EnvstepsSoFar : 453001\n",
      "Train_AverageReturn : 72.1160918514282\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2055.7420530319214\n",
      "Training Loss : 0.8438106775283813\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 454001\n",
      "mean reward (100 episodes) 71.436156\n",
      "best mean reward 169.148691\n",
      "running time 2059.350188\n",
      "Train_EnvstepsSoFar : 454001\n",
      "Train_AverageReturn : 71.43615611164304\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2059.3501880168915\n",
      "Training Loss : 2.3947536945343018\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 455001\n",
      "mean reward (100 episodes) 72.601160\n",
      "best mean reward 169.148691\n",
      "running time 2062.935911\n",
      "Train_EnvstepsSoFar : 455001\n",
      "Train_AverageReturn : 72.6011595614178\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2062.9359109401703\n",
      "Training Loss : 6.7246503829956055\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 456001\n",
      "mean reward (100 episodes) 71.774779\n",
      "best mean reward 169.148691\n",
      "running time 2066.531515\n",
      "Train_EnvstepsSoFar : 456001\n",
      "Train_AverageReturn : 71.77477930117016\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2066.5315148830414\n",
      "Training Loss : 1.4623035192489624\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 457001\n",
      "mean reward (100 episodes) 75.639238\n",
      "best mean reward 169.148691\n",
      "running time 2070.078679\n",
      "Train_EnvstepsSoFar : 457001\n",
      "Train_AverageReturn : 75.63923790381199\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2070.078679084778\n",
      "Training Loss : 0.5138974189758301\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 458001\n",
      "mean reward (100 episodes) 73.798802\n",
      "best mean reward 169.148691\n",
      "running time 2073.614854\n",
      "Train_EnvstepsSoFar : 458001\n",
      "Train_AverageReturn : 73.79880169002433\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2073.6148538589478\n",
      "Training Loss : 1.2217614650726318\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 459001\n",
      "mean reward (100 episodes) 67.108215\n",
      "best mean reward 169.148691\n",
      "running time 2077.201058\n",
      "Train_EnvstepsSoFar : 459001\n",
      "Train_AverageReturn : 67.10821534018015\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2077.201057910919\n",
      "Training Loss : 0.23107509315013885\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 460001\n",
      "mean reward (100 episodes) 69.086412\n",
      "best mean reward 169.148691\n",
      "running time 2080.746259\n",
      "Train_EnvstepsSoFar : 460001\n",
      "Train_AverageReturn : 69.08641229418407\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2080.7462589740753\n",
      "Training Loss : 0.31588441133499146\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 461001\n",
      "mean reward (100 episodes) 70.121101\n",
      "best mean reward 169.148691\n",
      "running time 2084.275821\n",
      "Train_EnvstepsSoFar : 461001\n",
      "Train_AverageReturn : 70.12110051755506\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2084.2758209705353\n",
      "Training Loss : 0.8127598166465759\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 462001\n",
      "mean reward (100 episodes) 57.375680\n",
      "best mean reward 169.148691\n",
      "running time 2087.870441\n",
      "Train_EnvstepsSoFar : 462001\n",
      "Train_AverageReturn : 57.37568018034362\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2087.8704409599304\n",
      "Training Loss : 2.596540689468384\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 463001\n",
      "mean reward (100 episodes) 52.099643\n",
      "best mean reward 169.148691\n",
      "running time 2091.450607\n",
      "Train_EnvstepsSoFar : 463001\n",
      "Train_AverageReturn : 52.099642950784755\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2091.450607061386\n",
      "Training Loss : 0.2785457670688629\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 464001\n",
      "mean reward (100 episodes) 52.427419\n",
      "best mean reward 169.148691\n",
      "running time 2095.026012\n",
      "Train_EnvstepsSoFar : 464001\n",
      "Train_AverageReturn : 52.42741855255556\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2095.026011943817\n",
      "Training Loss : 1.1829718351364136\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 465001\n",
      "mean reward (100 episodes) 50.697470\n",
      "best mean reward 169.148691\n",
      "running time 2098.555534\n",
      "Train_EnvstepsSoFar : 465001\n",
      "Train_AverageReturn : 50.69747000979316\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2098.5555341243744\n",
      "Training Loss : 1.5802923440933228\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 466001\n",
      "mean reward (100 episodes) 50.822402\n",
      "best mean reward 169.148691\n",
      "running time 2102.387278\n",
      "Train_EnvstepsSoFar : 466001\n",
      "Train_AverageReturn : 50.822402204353885\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2102.3872780799866\n",
      "Training Loss : 1.7395257949829102\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 467001\n",
      "mean reward (100 episodes) 41.756782\n",
      "best mean reward 169.148691\n",
      "running time 2106.004713\n",
      "Train_EnvstepsSoFar : 467001\n",
      "Train_AverageReturn : 41.75678238019208\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2106.004712820053\n",
      "Training Loss : 0.2882915139198303\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 468001\n",
      "mean reward (100 episodes) 37.866630\n",
      "best mean reward 169.148691\n",
      "running time 2109.520895\n",
      "Train_EnvstepsSoFar : 468001\n",
      "Train_AverageReturn : 37.866630375910724\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2109.5208950042725\n",
      "Training Loss : 0.19863350689411163\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 469001\n",
      "mean reward (100 episodes) 32.217967\n",
      "best mean reward 169.148691\n",
      "running time 2113.129924\n",
      "Train_EnvstepsSoFar : 469001\n",
      "Train_AverageReturn : 32.21796664193032\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2113.129924058914\n",
      "Training Loss : 3.0731327533721924\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 470001\n",
      "mean reward (100 episodes) 32.283948\n",
      "best mean reward 169.148691\n",
      "running time 2116.771660\n",
      "Train_EnvstepsSoFar : 470001\n",
      "Train_AverageReturn : 32.283948177470734\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2116.771660089493\n",
      "Training Loss : 6.1226091384887695\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 471001\n",
      "mean reward (100 episodes) 31.401382\n",
      "best mean reward 169.148691\n",
      "running time 2120.382500\n",
      "Train_EnvstepsSoFar : 471001\n",
      "Train_AverageReturn : 31.401381609161344\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2120.3825001716614\n",
      "Training Loss : 0.8693124055862427\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 472001\n",
      "mean reward (100 episodes) 36.626643\n",
      "best mean reward 169.148691\n",
      "running time 2123.942150\n",
      "Train_EnvstepsSoFar : 472001\n",
      "Train_AverageReturn : 36.6266430800572\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2123.942149877548\n",
      "Training Loss : 0.47748953104019165\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 473001\n",
      "mean reward (100 episodes) 37.988402\n",
      "best mean reward 169.148691\n",
      "running time 2127.588473\n",
      "Train_EnvstepsSoFar : 473001\n",
      "Train_AverageReturn : 37.98840195594021\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2127.5884730815887\n",
      "Training Loss : 0.2086479216814041\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 474001\n",
      "mean reward (100 episodes) 45.538444\n",
      "best mean reward 169.148691\n",
      "running time 2131.235222\n",
      "Train_EnvstepsSoFar : 474001\n",
      "Train_AverageReturn : 45.538444010544254\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2131.235221862793\n",
      "Training Loss : 0.7870895862579346\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 475001\n",
      "mean reward (100 episodes) 55.379777\n",
      "best mean reward 169.148691\n",
      "running time 2135.183723\n",
      "Train_EnvstepsSoFar : 475001\n",
      "Train_AverageReturn : 55.379777142381364\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2135.18372297287\n",
      "Training Loss : 0.9033538103103638\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 476001\n",
      "mean reward (100 episodes) 48.162470\n",
      "best mean reward 169.148691\n",
      "running time 2139.173558\n",
      "Train_EnvstepsSoFar : 476001\n",
      "Train_AverageReturn : 48.1624698880923\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2139.17355799675\n",
      "Training Loss : 1.5576369762420654\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 477001\n",
      "mean reward (100 episodes) 53.820680\n",
      "best mean reward 169.148691\n",
      "running time 2142.849020\n",
      "Train_EnvstepsSoFar : 477001\n",
      "Train_AverageReturn : 53.82068046207335\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2142.849019765854\n",
      "Training Loss : 0.2134108692407608\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 478001\n",
      "mean reward (100 episodes) 53.139488\n",
      "best mean reward 169.148691\n",
      "running time 2146.459615\n",
      "Train_EnvstepsSoFar : 478001\n",
      "Train_AverageReturn : 53.139488152809065\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2146.4596149921417\n",
      "Training Loss : 0.38155847787857056\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 479001\n",
      "mean reward (100 episodes) 60.334679\n",
      "best mean reward 169.148691\n",
      "running time 2150.108748\n",
      "Train_EnvstepsSoFar : 479001\n",
      "Train_AverageReturn : 60.33467946377329\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2150.108747959137\n",
      "Training Loss : 0.9549145698547363\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 480001\n",
      "mean reward (100 episodes) 56.157850\n",
      "best mean reward 169.148691\n",
      "running time 2153.754123\n",
      "Train_EnvstepsSoFar : 480001\n",
      "Train_AverageReturn : 56.15785000785919\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2153.7541229724884\n",
      "Training Loss : 1.0466383695602417\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 481001\n",
      "mean reward (100 episodes) 54.051263\n",
      "best mean reward 169.148691\n",
      "running time 2157.333324\n",
      "Train_EnvstepsSoFar : 481001\n",
      "Train_AverageReturn : 54.05126321024648\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2157.333323955536\n",
      "Training Loss : 4.00198221206665\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 482001\n",
      "mean reward (100 episodes) 54.527593\n",
      "best mean reward 169.148691\n",
      "running time 2161.036554\n",
      "Train_EnvstepsSoFar : 482001\n",
      "Train_AverageReturn : 54.52759256748067\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2161.0365538597107\n",
      "Training Loss : 0.18096798658370972\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 483001\n",
      "mean reward (100 episodes) 47.452154\n",
      "best mean reward 169.148691\n",
      "running time 2164.677322\n",
      "Train_EnvstepsSoFar : 483001\n",
      "Train_AverageReturn : 47.45215386338827\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2164.677321910858\n",
      "Training Loss : 0.7070006728172302\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 484001\n",
      "mean reward (100 episodes) 41.404335\n",
      "best mean reward 169.148691\n",
      "running time 2168.461993\n",
      "Train_EnvstepsSoFar : 484001\n",
      "Train_AverageReturn : 41.404334954205424\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2168.4619929790497\n",
      "Training Loss : 5.41443395614624\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 485001\n",
      "mean reward (100 episodes) 25.665736\n",
      "best mean reward 169.148691\n",
      "running time 2172.330587\n",
      "Train_EnvstepsSoFar : 485001\n",
      "Train_AverageReturn : 25.66573622045037\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2172.330586910248\n",
      "Training Loss : 3.0312705039978027\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 486001\n",
      "mean reward (100 episodes) 19.656990\n",
      "best mean reward 169.148691\n",
      "running time 2176.510720\n",
      "Train_EnvstepsSoFar : 486001\n",
      "Train_AverageReturn : 19.656989911676916\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2176.510720014572\n",
      "Training Loss : 0.5926234722137451\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 487001\n",
      "mean reward (100 episodes) 10.868733\n",
      "best mean reward 169.148691\n",
      "running time 2180.277053\n",
      "Train_EnvstepsSoFar : 487001\n",
      "Train_AverageReturn : 10.868732564439288\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2180.2770528793335\n",
      "Training Loss : 0.4942762851715088\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 488001\n",
      "mean reward (100 episodes) 14.390385\n",
      "best mean reward 169.148691\n",
      "running time 2184.009177\n",
      "Train_EnvstepsSoFar : 488001\n",
      "Train_AverageReturn : 14.390385248254377\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2184.009176969528\n",
      "Training Loss : 0.9373720288276672\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 489001\n",
      "mean reward (100 episodes) 13.405915\n",
      "best mean reward 169.148691\n",
      "running time 2187.970092\n",
      "Train_EnvstepsSoFar : 489001\n",
      "Train_AverageReturn : 13.405915351494773\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2187.9700920581818\n",
      "Training Loss : 0.15372885763645172\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 490001\n",
      "mean reward (100 episodes) 11.891418\n",
      "best mean reward 169.148691\n",
      "running time 2191.567481\n",
      "Train_EnvstepsSoFar : 490001\n",
      "Train_AverageReturn : 11.891418237072969\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2191.5674810409546\n",
      "Training Loss : 2.5129401683807373\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 491001\n",
      "mean reward (100 episodes) 5.861566\n",
      "best mean reward 169.148691\n",
      "running time 2195.264797\n",
      "Train_EnvstepsSoFar : 491001\n",
      "Train_AverageReturn : 5.861565578908419\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2195.264796972275\n",
      "Training Loss : 7.31583833694458\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 492001\n",
      "mean reward (100 episodes) 5.652764\n",
      "best mean reward 169.148691\n",
      "running time 2198.788488\n",
      "Train_EnvstepsSoFar : 492001\n",
      "Train_AverageReturn : 5.6527638911152955\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2198.7884879112244\n",
      "Training Loss : 1.9563831090927124\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 493001\n",
      "mean reward (100 episodes) 6.522820\n",
      "best mean reward 169.148691\n",
      "running time 2202.599694\n",
      "Train_EnvstepsSoFar : 493001\n",
      "Train_AverageReturn : 6.522820446329466\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2202.5996940135956\n",
      "Training Loss : 0.31801721453666687\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 494001\n",
      "mean reward (100 episodes) 0.746578\n",
      "best mean reward 169.148691\n",
      "running time 2206.180045\n",
      "Train_EnvstepsSoFar : 494001\n",
      "Train_AverageReturn : 0.7465777041429302\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2206.1800451278687\n",
      "Training Loss : 6.046526908874512\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 495001\n",
      "mean reward (100 episodes) -1.337606\n",
      "best mean reward 169.148691\n",
      "running time 2210.063898\n",
      "Train_EnvstepsSoFar : 495001\n",
      "Train_AverageReturn : -1.3376060313523295\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2210.063898086548\n",
      "Training Loss : 4.68954610824585\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 496001\n",
      "mean reward (100 episodes) -5.170716\n",
      "best mean reward 169.148691\n",
      "running time 2214.205436\n",
      "Train_EnvstepsSoFar : 496001\n",
      "Train_AverageReturn : -5.1707164218940065\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2214.2054357528687\n",
      "Training Loss : 0.25012660026550293\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 497001\n",
      "mean reward (100 episodes) -6.993976\n",
      "best mean reward 169.148691\n",
      "running time 2217.854645\n",
      "Train_EnvstepsSoFar : 497001\n",
      "Train_AverageReturn : -6.993976254590435\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2217.854645013809\n",
      "Training Loss : 0.8710522651672363\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 498001\n",
      "mean reward (100 episodes) -9.635977\n",
      "best mean reward 169.148691\n",
      "running time 2221.394747\n",
      "Train_EnvstepsSoFar : 498001\n",
      "Train_AverageReturn : -9.635976960194059\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2221.3947467803955\n",
      "Training Loss : 0.4892421066761017\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 499000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 499001\n",
      "mean reward (100 episodes) -16.054239\n",
      "best mean reward 169.148691\n",
      "running time 2225.801362\n",
      "Train_EnvstepsSoFar : 499001\n",
      "Train_AverageReturn : -16.05423931436404\n",
      "Train_BestReturn : 169.14869073015078\n",
      "TimeSinceStart : 2225.80136179924\n",
      "Training Loss : 1.1187138557434082\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Running DQN experiment with seed 1\n",
      "########################\n",
      "logging outputs to  logs/dqn/LunarLander/double_dqn/seed1\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "LunarLander-v3\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1\n",
      "mean reward (100 episodes) nan\n",
      "best mean reward -inf\n",
      "running time 0.000578\n",
      "Train_EnvstepsSoFar : 1\n",
      "TimeSinceStart : 0.0005776882171630859\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1001\n",
      "mean reward (100 episodes) -262.515855\n",
      "best mean reward -inf\n",
      "running time 0.651440\n",
      "Train_EnvstepsSoFar : 1001\n",
      "Train_AverageReturn : -262.5158551758566\n",
      "TimeSinceStart : 0.6514396667480469\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2001\n",
      "mean reward (100 episodes) -278.027661\n",
      "best mean reward -inf\n",
      "running time 5.067975\n",
      "Train_EnvstepsSoFar : 2001\n",
      "Train_AverageReturn : -278.0276608248407\n",
      "TimeSinceStart : 5.067974805831909\n",
      "Training Loss : 0.38780146837234497\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3001\n",
      "mean reward (100 episodes) -272.635477\n",
      "best mean reward -inf\n",
      "running time 10.055960\n",
      "Train_EnvstepsSoFar : 3001\n",
      "Train_AverageReturn : -272.63547731461483\n",
      "TimeSinceStart : 10.055959701538086\n",
      "Training Loss : 3.5459330081939697\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4001\n",
      "mean reward (100 episodes) -279.667857\n",
      "best mean reward -inf\n",
      "running time 15.281756\n",
      "Train_EnvstepsSoFar : 4001\n",
      "Train_AverageReturn : -279.6678565327213\n",
      "TimeSinceStart : 15.281755924224854\n",
      "Training Loss : 0.3268077075481415\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5001\n",
      "mean reward (100 episodes) -282.326601\n",
      "best mean reward -inf\n",
      "running time 18.989027\n",
      "Train_EnvstepsSoFar : 5001\n",
      "Train_AverageReturn : -282.32660100883976\n",
      "TimeSinceStart : 18.98902678489685\n",
      "Training Loss : 0.36736780405044556\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6001\n",
      "mean reward (100 episodes) -270.877934\n",
      "best mean reward -inf\n",
      "running time 22.570692\n",
      "Train_EnvstepsSoFar : 6001\n",
      "Train_AverageReturn : -270.87793400378996\n",
      "TimeSinceStart : 22.57069182395935\n",
      "Training Loss : 0.21106667816638947\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7001\n",
      "mean reward (100 episodes) -253.713664\n",
      "best mean reward -inf\n",
      "running time 26.185009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 7001\n",
      "Train_AverageReturn : -253.7136636222047\n",
      "TimeSinceStart : 26.185008764266968\n",
      "Training Loss : 1.1927947998046875\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8001\n",
      "mean reward (100 episodes) -246.240848\n",
      "best mean reward -inf\n",
      "running time 29.905900\n",
      "Train_EnvstepsSoFar : 8001\n",
      "Train_AverageReturn : -246.2408483433308\n",
      "TimeSinceStart : 29.90590000152588\n",
      "Training Loss : 3.269710063934326\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9001\n",
      "mean reward (100 episodes) -240.547197\n",
      "best mean reward -inf\n",
      "running time 33.820899\n",
      "Train_EnvstepsSoFar : 9001\n",
      "Train_AverageReturn : -240.54719675806547\n",
      "TimeSinceStart : 33.82089877128601\n",
      "Training Loss : 3.5026354789733887\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10001\n",
      "mean reward (100 episodes) -235.280430\n",
      "best mean reward -inf\n",
      "running time 37.781109\n",
      "Train_EnvstepsSoFar : 10001\n",
      "Train_AverageReturn : -235.28043006590815\n",
      "TimeSinceStart : 37.78110885620117\n",
      "Training Loss : 0.3426058292388916\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 11000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 11001\n",
      "mean reward (100 episodes) -234.419713\n",
      "best mean reward -inf\n",
      "running time 41.348845\n",
      "Train_EnvstepsSoFar : 11001\n",
      "Train_AverageReturn : -234.41971309453734\n",
      "TimeSinceStart : 41.34884476661682\n",
      "Training Loss : 0.216652512550354\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 12000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 12001\n",
      "mean reward (100 episodes) -232.659635\n",
      "best mean reward -inf\n",
      "running time 46.112367\n",
      "Train_EnvstepsSoFar : 12001\n",
      "Train_AverageReturn : -232.65963469190987\n",
      "TimeSinceStart : 46.112366676330566\n",
      "Training Loss : 0.46135061979293823\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 13000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 13001\n",
      "mean reward (100 episodes) -217.074582\n",
      "best mean reward -inf\n",
      "running time 50.307259\n",
      "Train_EnvstepsSoFar : 13001\n",
      "Train_AverageReturn : -217.0745820226903\n",
      "TimeSinceStart : 50.30725884437561\n",
      "Training Loss : 0.3495214879512787\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 14000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 14001\n",
      "mean reward (100 episodes) -210.810455\n",
      "best mean reward -inf\n",
      "running time 54.085125\n",
      "Train_EnvstepsSoFar : 14001\n",
      "Train_AverageReturn : -210.81045452733977\n",
      "TimeSinceStart : 54.08512473106384\n",
      "Training Loss : 3.544255495071411\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 15000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 15001\n",
      "mean reward (100 episodes) -207.327529\n",
      "best mean reward -inf\n",
      "running time 57.812661\n",
      "Train_EnvstepsSoFar : 15001\n",
      "Train_AverageReturn : -207.32752863642395\n",
      "TimeSinceStart : 57.812660932540894\n",
      "Training Loss : 1.239735722541809\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 16000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 16001\n",
      "mean reward (100 episodes) -206.464603\n",
      "best mean reward -inf\n",
      "running time 62.262832\n",
      "Train_EnvstepsSoFar : 16001\n",
      "Train_AverageReturn : -206.46460346864595\n",
      "TimeSinceStart : 62.262831687927246\n",
      "Training Loss : 0.42929038405418396\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 17000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 17001\n",
      "mean reward (100 episodes) -198.966347\n",
      "best mean reward -198.966347\n",
      "running time 66.342589\n",
      "Train_EnvstepsSoFar : 17001\n",
      "Train_AverageReturn : -198.96634653446262\n",
      "Train_BestReturn : -198.96634653446262\n",
      "TimeSinceStart : 66.34258890151978\n",
      "Training Loss : 1.0433337688446045\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 18000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 18001\n",
      "mean reward (100 episodes) -193.545151\n",
      "best mean reward -193.545151\n",
      "running time 70.526810\n",
      "Train_EnvstepsSoFar : 18001\n",
      "Train_AverageReturn : -193.54515130361722\n",
      "Train_BestReturn : -193.54515130361722\n",
      "TimeSinceStart : 70.52680969238281\n",
      "Training Loss : 0.6321118474006653\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 19000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 19001\n",
      "mean reward (100 episodes) -191.982768\n",
      "best mean reward -191.982768\n",
      "running time 76.305971\n",
      "Train_EnvstepsSoFar : 19001\n",
      "Train_AverageReturn : -191.9827676204304\n",
      "Train_BestReturn : -191.9827676204304\n",
      "TimeSinceStart : 76.3059709072113\n",
      "Training Loss : 0.4855596423149109\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 20001\n",
      "mean reward (100 episodes) -188.566734\n",
      "best mean reward -188.566734\n",
      "running time 81.238989\n",
      "Train_EnvstepsSoFar : 20001\n",
      "Train_AverageReturn : -188.56673427359993\n",
      "Train_BestReturn : -188.56673427359993\n",
      "TimeSinceStart : 81.2389886379242\n",
      "Training Loss : 0.6594617366790771\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 21000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 21001\n",
      "mean reward (100 episodes) -188.578095\n",
      "best mean reward -188.566734\n",
      "running time 85.537151\n",
      "Train_EnvstepsSoFar : 21001\n",
      "Train_AverageReturn : -188.5780945520243\n",
      "Train_BestReturn : -188.56673427359993\n",
      "TimeSinceStart : 85.53715085983276\n",
      "Training Loss : 2.8470940589904785\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 22000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 22001\n",
      "mean reward (100 episodes) -187.389250\n",
      "best mean reward -187.389250\n",
      "running time 90.521551\n",
      "Train_EnvstepsSoFar : 22001\n",
      "Train_AverageReturn : -187.38924959275647\n",
      "Train_BestReturn : -187.38924959275647\n",
      "TimeSinceStart : 90.52155065536499\n",
      "Training Loss : 0.8936426043510437\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 23000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 23001\n",
      "mean reward (100 episodes) -184.521599\n",
      "best mean reward -184.521599\n",
      "running time 95.410816\n",
      "Train_EnvstepsSoFar : 23001\n",
      "Train_AverageReturn : -184.52159875415472\n",
      "Train_BestReturn : -184.52159875415472\n",
      "TimeSinceStart : 95.41081595420837\n",
      "Training Loss : 0.4006327986717224\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 24000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 24001\n",
      "mean reward (100 episodes) -175.133478\n",
      "best mean reward -175.133478\n",
      "running time 100.217230\n",
      "Train_EnvstepsSoFar : 24001\n",
      "Train_AverageReturn : -175.13347822260104\n",
      "Train_BestReturn : -175.13347822260104\n",
      "TimeSinceStart : 100.21722984313965\n",
      "Training Loss : 0.5005121231079102\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 25000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 25001\n",
      "mean reward (100 episodes) -173.443844\n",
      "best mean reward -173.443844\n",
      "running time 110.311417\n",
      "Train_EnvstepsSoFar : 25001\n",
      "Train_AverageReturn : -173.4438437283564\n",
      "Train_BestReturn : -173.4438437283564\n",
      "TimeSinceStart : 110.31141662597656\n",
      "Training Loss : 6.853253364562988\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 26000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 26001\n",
      "mean reward (100 episodes) -171.467809\n",
      "best mean reward -171.467809\n",
      "running time 117.696153\n",
      "Train_EnvstepsSoFar : 26001\n",
      "Train_AverageReturn : -171.4678085888778\n",
      "Train_BestReturn : -171.4678085888778\n",
      "TimeSinceStart : 117.69615268707275\n",
      "Training Loss : 0.78769850730896\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 27000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 27001\n",
      "mean reward (100 episodes) -169.436576\n",
      "best mean reward -169.436576\n",
      "running time 122.304454\n",
      "Train_EnvstepsSoFar : 27001\n",
      "Train_AverageReturn : -169.4365756015569\n",
      "Train_BestReturn : -169.4365756015569\n",
      "TimeSinceStart : 122.30445384979248\n",
      "Training Loss : 0.6920902729034424\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 28000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 28001\n",
      "mean reward (100 episodes) -166.060279\n",
      "best mean reward -166.060279\n",
      "running time 129.921704\n",
      "Train_EnvstepsSoFar : 28001\n",
      "Train_AverageReturn : -166.0602785919924\n",
      "Train_BestReturn : -166.0602785919924\n",
      "TimeSinceStart : 129.9217038154602\n",
      "Training Loss : 0.6696727275848389\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 29000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 29001\n",
      "mean reward (100 episodes) -163.045874\n",
      "best mean reward -163.045874\n",
      "running time 135.319396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 29001\n",
      "Train_AverageReturn : -163.04587396234515\n",
      "Train_BestReturn : -163.04587396234515\n",
      "TimeSinceStart : 135.31939578056335\n",
      "Training Loss : 0.49166059494018555\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 30001\n",
      "mean reward (100 episodes) -160.602186\n",
      "best mean reward -160.602186\n",
      "running time 140.871974\n",
      "Train_EnvstepsSoFar : 30001\n",
      "Train_AverageReturn : -160.60218611198002\n",
      "Train_BestReturn : -160.60218611198002\n",
      "TimeSinceStart : 140.87197375297546\n",
      "Training Loss : 0.5718457102775574\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 31000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 31001\n",
      "mean reward (100 episodes) -159.587706\n",
      "best mean reward -159.587706\n",
      "running time 145.942649\n",
      "Train_EnvstepsSoFar : 31001\n",
      "Train_AverageReturn : -159.5877062871407\n",
      "Train_BestReturn : -159.5877062871407\n",
      "TimeSinceStart : 145.9426486492157\n",
      "Training Loss : 0.5407890677452087\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 32000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 32001\n",
      "mean reward (100 episodes) -160.214290\n",
      "best mean reward -159.587706\n",
      "running time 151.191586\n",
      "Train_EnvstepsSoFar : 32001\n",
      "Train_AverageReturn : -160.21429044532863\n",
      "Train_BestReturn : -159.5877062871407\n",
      "TimeSinceStart : 151.19158577919006\n",
      "Training Loss : 0.9370112419128418\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 33000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 33001\n",
      "mean reward (100 episodes) -160.517555\n",
      "best mean reward -159.587706\n",
      "running time 157.107636\n",
      "Train_EnvstepsSoFar : 33001\n",
      "Train_AverageReturn : -160.51755471417587\n",
      "Train_BestReturn : -159.5877062871407\n",
      "TimeSinceStart : 157.10763597488403\n",
      "Training Loss : 0.7150077819824219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 34000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 34001\n",
      "mean reward (100 episodes) -157.995268\n",
      "best mean reward -157.995268\n",
      "running time 162.046967\n",
      "Train_EnvstepsSoFar : 34001\n",
      "Train_AverageReturn : -157.99526822736442\n",
      "Train_BestReturn : -157.99526822736442\n",
      "TimeSinceStart : 162.04696702957153\n",
      "Training Loss : 0.7884668707847595\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 35000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 35001\n",
      "mean reward (100 episodes) -156.554723\n",
      "best mean reward -156.554723\n",
      "running time 167.268956\n",
      "Train_EnvstepsSoFar : 35001\n",
      "Train_AverageReturn : -156.5547232817878\n",
      "Train_BestReturn : -156.5547232817878\n",
      "TimeSinceStart : 167.26895570755005\n",
      "Training Loss : 0.7269879579544067\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 36000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 36001\n",
      "mean reward (100 episodes) -153.432704\n",
      "best mean reward -153.432704\n",
      "running time 174.005294\n",
      "Train_EnvstepsSoFar : 36001\n",
      "Train_AverageReturn : -153.43270366258247\n",
      "Train_BestReturn : -153.43270366258247\n",
      "TimeSinceStart : 174.00529384613037\n",
      "Training Loss : 0.4383866786956787\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 37000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 37001\n",
      "mean reward (100 episodes) -156.423049\n",
      "best mean reward -153.432704\n",
      "running time 181.306177\n",
      "Train_EnvstepsSoFar : 37001\n",
      "Train_AverageReturn : -156.4230489204503\n",
      "Train_BestReturn : -153.43270366258247\n",
      "TimeSinceStart : 181.30617690086365\n",
      "Training Loss : 0.3617667555809021\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 38000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 38001\n",
      "mean reward (100 episodes) -153.876511\n",
      "best mean reward -153.432704\n",
      "running time 186.152037\n",
      "Train_EnvstepsSoFar : 38001\n",
      "Train_AverageReturn : -153.8765107589352\n",
      "Train_BestReturn : -153.43270366258247\n",
      "TimeSinceStart : 186.15203666687012\n",
      "Training Loss : 0.5722267031669617\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 39000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 39001\n",
      "mean reward (100 episodes) -149.456301\n",
      "best mean reward -149.456301\n",
      "running time 191.225176\n",
      "Train_EnvstepsSoFar : 39001\n",
      "Train_AverageReturn : -149.45630130721574\n",
      "Train_BestReturn : -149.45630130721574\n",
      "TimeSinceStart : 191.22517561912537\n",
      "Training Loss : 0.22440925240516663\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 40001\n",
      "mean reward (100 episodes) -145.665990\n",
      "best mean reward -145.665990\n",
      "running time 196.953736\n",
      "Train_EnvstepsSoFar : 40001\n",
      "Train_AverageReturn : -145.66599037840146\n",
      "Train_BestReturn : -145.66599037840146\n",
      "TimeSinceStart : 196.95373582839966\n",
      "Training Loss : 0.2868545353412628\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 41000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 41001\n",
      "mean reward (100 episodes) -143.312971\n",
      "best mean reward -143.312971\n",
      "running time 202.311415\n",
      "Train_EnvstepsSoFar : 41001\n",
      "Train_AverageReturn : -143.31297111736612\n",
      "Train_BestReturn : -143.31297111736612\n",
      "TimeSinceStart : 202.3114149570465\n",
      "Training Loss : 1.9510869979858398\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 42000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 42001\n",
      "mean reward (100 episodes) -143.030408\n",
      "best mean reward -143.030408\n",
      "running time 207.715395\n",
      "Train_EnvstepsSoFar : 42001\n",
      "Train_AverageReturn : -143.03040809397118\n",
      "Train_BestReturn : -143.03040809397118\n",
      "TimeSinceStart : 207.7153947353363\n",
      "Training Loss : 0.5753909349441528\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 43000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 43001\n",
      "mean reward (100 episodes) -141.069174\n",
      "best mean reward -141.069174\n",
      "running time 212.220478\n",
      "Train_EnvstepsSoFar : 43001\n",
      "Train_AverageReturn : -141.06917424771223\n",
      "Train_BestReturn : -141.06917424771223\n",
      "TimeSinceStart : 212.22047781944275\n",
      "Training Loss : 0.8458133935928345\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 44000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 44001\n",
      "mean reward (100 episodes) -139.515007\n",
      "best mean reward -139.515007\n",
      "running time 216.610471\n",
      "Train_EnvstepsSoFar : 44001\n",
      "Train_AverageReturn : -139.51500737470772\n",
      "Train_BestReturn : -139.51500737470772\n",
      "TimeSinceStart : 216.61047077178955\n",
      "Training Loss : 1.1129179000854492\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 45000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 45001\n",
      "mean reward (100 episodes) -136.190448\n",
      "best mean reward -136.190448\n",
      "running time 221.237584\n",
      "Train_EnvstepsSoFar : 45001\n",
      "Train_AverageReturn : -136.19044760489192\n",
      "Train_BestReturn : -136.19044760489192\n",
      "TimeSinceStart : 221.23758387565613\n",
      "Training Loss : 0.43172889947891235\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 46000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 46001\n",
      "mean reward (100 episodes) -136.204106\n",
      "best mean reward -136.190448\n",
      "running time 225.579890\n",
      "Train_EnvstepsSoFar : 46001\n",
      "Train_AverageReturn : -136.20410565164948\n",
      "Train_BestReturn : -136.19044760489192\n",
      "TimeSinceStart : 225.5798897743225\n",
      "Training Loss : 0.8276176452636719\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 47000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 47001\n",
      "mean reward (100 episodes) -135.992049\n",
      "best mean reward -135.992049\n",
      "running time 230.219187\n",
      "Train_EnvstepsSoFar : 47001\n",
      "Train_AverageReturn : -135.99204924191906\n",
      "Train_BestReturn : -135.99204924191906\n",
      "TimeSinceStart : 230.2191867828369\n",
      "Training Loss : 0.4928207993507385\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 48000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 48001\n",
      "mean reward (100 episodes) -133.764975\n",
      "best mean reward -133.764975\n",
      "running time 234.902037\n",
      "Train_EnvstepsSoFar : 48001\n",
      "Train_AverageReturn : -133.76497541784556\n",
      "Train_BestReturn : -133.76497541784556\n",
      "TimeSinceStart : 234.9020369052887\n",
      "Training Loss : 1.1106138229370117\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 49000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 49001\n",
      "mean reward (100 episodes) -132.250832\n",
      "best mean reward -132.250832\n",
      "running time 239.490481\n",
      "Train_EnvstepsSoFar : 49001\n",
      "Train_AverageReturn : -132.25083228496814\n",
      "Train_BestReturn : -132.25083228496814\n",
      "TimeSinceStart : 239.4904808998108\n",
      "Training Loss : 0.5408241748809814\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 50001\n",
      "mean reward (100 episodes) -130.664030\n",
      "best mean reward -130.664030\n",
      "running time 244.389956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 50001\n",
      "Train_AverageReturn : -130.66403034153416\n",
      "Train_BestReturn : -130.66403034153416\n",
      "TimeSinceStart : 244.38995575904846\n",
      "Training Loss : 1.0754667520523071\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 51000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 51001\n",
      "mean reward (100 episodes) -130.562132\n",
      "best mean reward -130.562132\n",
      "running time 249.089722\n",
      "Train_EnvstepsSoFar : 51001\n",
      "Train_AverageReturn : -130.56213235136025\n",
      "Train_BestReturn : -130.56213235136025\n",
      "TimeSinceStart : 249.0897216796875\n",
      "Training Loss : 0.7338390350341797\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 52000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 52001\n",
      "mean reward (100 episodes) -130.521569\n",
      "best mean reward -130.521569\n",
      "running time 254.160743\n",
      "Train_EnvstepsSoFar : 52001\n",
      "Train_AverageReturn : -130.52156942601476\n",
      "Train_BestReturn : -130.52156942601476\n",
      "TimeSinceStart : 254.1607427597046\n",
      "Training Loss : 2.385704755783081\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 53000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 53001\n",
      "mean reward (100 episodes) -131.901002\n",
      "best mean reward -130.521569\n",
      "running time 258.888015\n",
      "Train_EnvstepsSoFar : 53001\n",
      "Train_AverageReturn : -131.90100246077168\n",
      "Train_BestReturn : -130.52156942601476\n",
      "TimeSinceStart : 258.888014793396\n",
      "Training Loss : 0.15149684250354767\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 54000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 54001\n",
      "mean reward (100 episodes) -131.047533\n",
      "best mean reward -130.521569\n",
      "running time 263.322674\n",
      "Train_EnvstepsSoFar : 54001\n",
      "Train_AverageReturn : -131.04753347304302\n",
      "Train_BestReturn : -130.52156942601476\n",
      "TimeSinceStart : 263.3226737976074\n",
      "Training Loss : 1.0524427890777588\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 55000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 55001\n",
      "mean reward (100 episodes) -129.572803\n",
      "best mean reward -129.572803\n",
      "running time 267.936435\n",
      "Train_EnvstepsSoFar : 55001\n",
      "Train_AverageReturn : -129.5728027737062\n",
      "Train_BestReturn : -129.5728027737062\n",
      "TimeSinceStart : 267.9364347457886\n",
      "Training Loss : 0.8836215734481812\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 56000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 56001\n",
      "mean reward (100 episodes) -128.678676\n",
      "best mean reward -128.678676\n",
      "running time 273.022429\n",
      "Train_EnvstepsSoFar : 56001\n",
      "Train_AverageReturn : -128.67867565970025\n",
      "Train_BestReturn : -128.67867565970025\n",
      "TimeSinceStart : 273.0224287509918\n",
      "Training Loss : 0.41352030634880066\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 57000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 57001\n",
      "mean reward (100 episodes) -127.678514\n",
      "best mean reward -127.678514\n",
      "running time 278.147458\n",
      "Train_EnvstepsSoFar : 57001\n",
      "Train_AverageReturn : -127.67851390549531\n",
      "Train_BestReturn : -127.67851390549531\n",
      "TimeSinceStart : 278.1474575996399\n",
      "Training Loss : 0.31850385665893555\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 58000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 58001\n",
      "mean reward (100 episodes) -127.179398\n",
      "best mean reward -127.179398\n",
      "running time 282.610309\n",
      "Train_EnvstepsSoFar : 58001\n",
      "Train_AverageReturn : -127.17939769813567\n",
      "Train_BestReturn : -127.17939769813567\n",
      "TimeSinceStart : 282.61030888557434\n",
      "Training Loss : 0.17960350215435028\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 59000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 59001\n",
      "mean reward (100 episodes) -125.155342\n",
      "best mean reward -125.155342\n",
      "running time 288.073293\n",
      "Train_EnvstepsSoFar : 59001\n",
      "Train_AverageReturn : -125.15534170586542\n",
      "Train_BestReturn : -125.15534170586542\n",
      "TimeSinceStart : 288.07329273223877\n",
      "Training Loss : 0.4639480710029602\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 60001\n",
      "mean reward (100 episodes) -123.736606\n",
      "best mean reward -123.736606\n",
      "running time 293.796459\n",
      "Train_EnvstepsSoFar : 60001\n",
      "Train_AverageReturn : -123.73660585144745\n",
      "Train_BestReturn : -123.73660585144745\n",
      "TimeSinceStart : 293.79645895957947\n",
      "Training Loss : 0.22510620951652527\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 61000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 61001\n",
      "mean reward (100 episodes) -123.783398\n",
      "best mean reward -123.736606\n",
      "running time 298.997508\n",
      "Train_EnvstepsSoFar : 61001\n",
      "Train_AverageReturn : -123.78339759476953\n",
      "Train_BestReturn : -123.73660585144745\n",
      "TimeSinceStart : 298.99750781059265\n",
      "Training Loss : 0.36032724380493164\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 62000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 62001\n",
      "mean reward (100 episodes) -120.381479\n",
      "best mean reward -120.381479\n",
      "running time 304.749380\n",
      "Train_EnvstepsSoFar : 62001\n",
      "Train_AverageReturn : -120.381478666869\n",
      "Train_BestReturn : -120.381478666869\n",
      "TimeSinceStart : 304.7493796348572\n",
      "Training Loss : 1.4408546686172485\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 63000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 63001\n",
      "mean reward (100 episodes) -120.563365\n",
      "best mean reward -120.381479\n",
      "running time 310.657812\n",
      "Train_EnvstepsSoFar : 63001\n",
      "Train_AverageReturn : -120.56336494647971\n",
      "Train_BestReturn : -120.381478666869\n",
      "TimeSinceStart : 310.6578118801117\n",
      "Training Loss : 0.7731438875198364\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 64000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 64001\n",
      "mean reward (100 episodes) -118.955459\n",
      "best mean reward -118.955459\n",
      "running time 315.351412\n",
      "Train_EnvstepsSoFar : 64001\n",
      "Train_AverageReturn : -118.95545864168858\n",
      "Train_BestReturn : -118.95545864168858\n",
      "TimeSinceStart : 315.351411819458\n",
      "Training Loss : 0.32699328660964966\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 65000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 65001\n",
      "mean reward (100 episodes) -117.672117\n",
      "best mean reward -117.672117\n",
      "running time 320.403202\n",
      "Train_EnvstepsSoFar : 65001\n",
      "Train_AverageReturn : -117.67211656195539\n",
      "Train_BestReturn : -117.67211656195539\n",
      "TimeSinceStart : 320.4032015800476\n",
      "Training Loss : 0.7830936908721924\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 66000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 66001\n",
      "mean reward (100 episodes) -117.203741\n",
      "best mean reward -117.203741\n",
      "running time 325.010859\n",
      "Train_EnvstepsSoFar : 66001\n",
      "Train_AverageReturn : -117.20374111903237\n",
      "Train_BestReturn : -117.20374111903237\n",
      "TimeSinceStart : 325.01085901260376\n",
      "Training Loss : 0.27126044034957886\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 67000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 67001\n",
      "mean reward (100 episodes) -115.566647\n",
      "best mean reward -115.566647\n",
      "running time 330.057248\n",
      "Train_EnvstepsSoFar : 67001\n",
      "Train_AverageReturn : -115.56664728046029\n",
      "Train_BestReturn : -115.56664728046029\n",
      "TimeSinceStart : 330.057247877121\n",
      "Training Loss : 0.22450627386569977\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 68000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 68001\n",
      "mean reward (100 episodes) -115.240676\n",
      "best mean reward -115.240676\n",
      "running time 334.929823\n",
      "Train_EnvstepsSoFar : 68001\n",
      "Train_AverageReturn : -115.24067625284506\n",
      "Train_BestReturn : -115.24067625284506\n",
      "TimeSinceStart : 334.92982292175293\n",
      "Training Loss : 0.6888023614883423\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 69000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 69001\n",
      "mean reward (100 episodes) -112.976973\n",
      "best mean reward -112.976973\n",
      "running time 339.684870\n",
      "Train_EnvstepsSoFar : 69001\n",
      "Train_AverageReturn : -112.97697346568734\n",
      "Train_BestReturn : -112.97697346568734\n",
      "TimeSinceStart : 339.68486976623535\n",
      "Training Loss : 0.3745884299278259\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 70001\n",
      "mean reward (100 episodes) -110.678473\n",
      "best mean reward -110.678473\n",
      "running time 344.864090\n",
      "Train_EnvstepsSoFar : 70001\n",
      "Train_AverageReturn : -110.67847287271393\n",
      "Train_BestReturn : -110.67847287271393\n",
      "TimeSinceStart : 344.86408972740173\n",
      "Training Loss : 0.6228480339050293\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 71000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 71001\n",
      "mean reward (100 episodes) -111.082380\n",
      "best mean reward -110.678473\n",
      "running time 349.644698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 71001\n",
      "Train_AverageReturn : -111.08238003941439\n",
      "Train_BestReturn : -110.67847287271393\n",
      "TimeSinceStart : 349.6446976661682\n",
      "Training Loss : 0.22718748450279236\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 72000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 72001\n",
      "mean reward (100 episodes) -110.198487\n",
      "best mean reward -110.198487\n",
      "running time 355.627127\n",
      "Train_EnvstepsSoFar : 72001\n",
      "Train_AverageReturn : -110.19848720294257\n",
      "Train_BestReturn : -110.19848720294257\n",
      "TimeSinceStart : 355.62712693214417\n",
      "Training Loss : 0.16120541095733643\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 73000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 73001\n",
      "mean reward (100 episodes) -109.561503\n",
      "best mean reward -109.561503\n",
      "running time 361.015009\n",
      "Train_EnvstepsSoFar : 73001\n",
      "Train_AverageReturn : -109.56150338952753\n",
      "Train_BestReturn : -109.56150338952753\n",
      "TimeSinceStart : 361.0150089263916\n",
      "Training Loss : 0.16836439073085785\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 74000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 74001\n",
      "mean reward (100 episodes) -109.052497\n",
      "best mean reward -109.052497\n",
      "running time 366.390034\n",
      "Train_EnvstepsSoFar : 74001\n",
      "Train_AverageReturn : -109.05249666257384\n",
      "Train_BestReturn : -109.05249666257384\n",
      "TimeSinceStart : 366.3900337219238\n",
      "Training Loss : 0.28537896275520325\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 75000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 75001\n",
      "mean reward (100 episodes) -107.165824\n",
      "best mean reward -107.165824\n",
      "running time 371.847267\n",
      "Train_EnvstepsSoFar : 75001\n",
      "Train_AverageReturn : -107.16582362686181\n",
      "Train_BestReturn : -107.16582362686181\n",
      "TimeSinceStart : 371.84726667404175\n",
      "Training Loss : 0.30257630348205566\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 76000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 76001\n",
      "mean reward (100 episodes) -100.094988\n",
      "best mean reward -100.094988\n",
      "running time 376.648486\n",
      "Train_EnvstepsSoFar : 76001\n",
      "Train_AverageReturn : -100.09498755116269\n",
      "Train_BestReturn : -100.09498755116269\n",
      "TimeSinceStart : 376.64848589897156\n",
      "Training Loss : 0.14045271277427673\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 77000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 77001\n",
      "mean reward (100 episodes) -98.697166\n",
      "best mean reward -98.697166\n",
      "running time 381.746962\n",
      "Train_EnvstepsSoFar : 77001\n",
      "Train_AverageReturn : -98.69716634860329\n",
      "Train_BestReturn : -98.69716634860329\n",
      "TimeSinceStart : 381.7469618320465\n",
      "Training Loss : 0.25021401047706604\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 78000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 78001\n",
      "mean reward (100 episodes) -97.337026\n",
      "best mean reward -97.337026\n",
      "running time 386.774706\n",
      "Train_EnvstepsSoFar : 78001\n",
      "Train_AverageReturn : -97.33702637323822\n",
      "Train_BestReturn : -97.33702637323822\n",
      "TimeSinceStart : 386.7747058868408\n",
      "Training Loss : 0.4057300090789795\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 79000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 79001\n",
      "mean reward (100 episodes) -97.578855\n",
      "best mean reward -97.337026\n",
      "running time 392.205792\n",
      "Train_EnvstepsSoFar : 79001\n",
      "Train_AverageReturn : -97.57885511764218\n",
      "Train_BestReturn : -97.33702637323822\n",
      "TimeSinceStart : 392.20579195022583\n",
      "Training Loss : 0.3399178087711334\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 80001\n",
      "mean reward (100 episodes) -97.304533\n",
      "best mean reward -97.304533\n",
      "running time 396.550636\n",
      "Train_EnvstepsSoFar : 80001\n",
      "Train_AverageReturn : -97.3045326912824\n",
      "Train_BestReturn : -97.3045326912824\n",
      "TimeSinceStart : 396.5506360530853\n",
      "Training Loss : 0.2377886176109314\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 81000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 81001\n",
      "mean reward (100 episodes) -97.408722\n",
      "best mean reward -97.304533\n",
      "running time 401.030019\n",
      "Train_EnvstepsSoFar : 81001\n",
      "Train_AverageReturn : -97.40872248717683\n",
      "Train_BestReturn : -97.3045326912824\n",
      "TimeSinceStart : 401.0300188064575\n",
      "Training Loss : 0.35166069865226746\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 82000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 82001\n",
      "mean reward (100 episodes) -97.833033\n",
      "best mean reward -97.304533\n",
      "running time 406.539975\n",
      "Train_EnvstepsSoFar : 82001\n",
      "Train_AverageReturn : -97.83303327850295\n",
      "Train_BestReturn : -97.3045326912824\n",
      "TimeSinceStart : 406.5399749279022\n",
      "Training Loss : 0.20957303047180176\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 83000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 83001\n",
      "mean reward (100 episodes) -98.776988\n",
      "best mean reward -97.304533\n",
      "running time 410.941298\n",
      "Train_EnvstepsSoFar : 83001\n",
      "Train_AverageReturn : -98.77698761479262\n",
      "Train_BestReturn : -97.3045326912824\n",
      "TimeSinceStart : 410.9412977695465\n",
      "Training Loss : 0.16624556481838226\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 84000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 84001\n",
      "mean reward (100 episodes) -98.914027\n",
      "best mean reward -97.304533\n",
      "running time 415.600289\n",
      "Train_EnvstepsSoFar : 84001\n",
      "Train_AverageReturn : -98.91402654115076\n",
      "Train_BestReturn : -97.3045326912824\n",
      "TimeSinceStart : 415.60028886795044\n",
      "Training Loss : 0.17957255244255066\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 85000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 85001\n",
      "mean reward (100 episodes) -97.007318\n",
      "best mean reward -97.007318\n",
      "running time 420.285168\n",
      "Train_EnvstepsSoFar : 85001\n",
      "Train_AverageReturn : -97.00731808300458\n",
      "Train_BestReturn : -97.00731808300458\n",
      "TimeSinceStart : 420.2851676940918\n",
      "Training Loss : 0.12105688452720642\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 86000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 86001\n",
      "mean reward (100 episodes) -99.397162\n",
      "best mean reward -97.007318\n",
      "running time 424.797643\n",
      "Train_EnvstepsSoFar : 86001\n",
      "Train_AverageReturn : -99.39716242133882\n",
      "Train_BestReturn : -97.00731808300458\n",
      "TimeSinceStart : 424.7976429462433\n",
      "Training Loss : 0.12793304026126862\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 87000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 87001\n",
      "mean reward (100 episodes) -96.965774\n",
      "best mean reward -96.965774\n",
      "running time 429.708951\n",
      "Train_EnvstepsSoFar : 87001\n",
      "Train_AverageReturn : -96.96577370382718\n",
      "Train_BestReturn : -96.96577370382718\n",
      "TimeSinceStart : 429.70895075798035\n",
      "Training Loss : 0.16821394860744476\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 88000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 88001\n",
      "mean reward (100 episodes) -93.654732\n",
      "best mean reward -93.654732\n",
      "running time 433.922249\n",
      "Train_EnvstepsSoFar : 88001\n",
      "Train_AverageReturn : -93.6547316688668\n",
      "Train_BestReturn : -93.6547316688668\n",
      "TimeSinceStart : 433.92224884033203\n",
      "Training Loss : 0.3252847194671631\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 89000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 89001\n",
      "mean reward (100 episodes) -88.450032\n",
      "best mean reward -88.450032\n",
      "running time 438.019976\n",
      "Train_EnvstepsSoFar : 89001\n",
      "Train_AverageReturn : -88.4500319167314\n",
      "Train_BestReturn : -88.4500319167314\n",
      "TimeSinceStart : 438.01997590065\n",
      "Training Loss : 0.4200243651866913\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 90001\n",
      "mean reward (100 episodes) -91.256826\n",
      "best mean reward -88.450032\n",
      "running time 442.879538\n",
      "Train_EnvstepsSoFar : 90001\n",
      "Train_AverageReturn : -91.25682552655752\n",
      "Train_BestReturn : -88.4500319167314\n",
      "TimeSinceStart : 442.87953782081604\n",
      "Training Loss : 0.220742866396904\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 91000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 91001\n",
      "mean reward (100 episodes) -87.693021\n",
      "best mean reward -87.693021\n",
      "running time 447.446460\n",
      "Train_EnvstepsSoFar : 91001\n",
      "Train_AverageReturn : -87.69302135602967\n",
      "Train_BestReturn : -87.69302135602967\n",
      "TimeSinceStart : 447.44645977020264\n",
      "Training Loss : 0.09583202004432678\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 92000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 92001\n",
      "mean reward (100 episodes) -85.755230\n",
      "best mean reward -85.755230\n",
      "running time 451.969613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 92001\n",
      "Train_AverageReturn : -85.7552297919261\n",
      "Train_BestReturn : -85.7552297919261\n",
      "TimeSinceStart : 451.9696125984192\n",
      "Training Loss : 0.14192558825016022\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 93000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 93001\n",
      "mean reward (100 episodes) -83.696931\n",
      "best mean reward -83.696931\n",
      "running time 457.148394\n",
      "Train_EnvstepsSoFar : 93001\n",
      "Train_AverageReturn : -83.6969311983095\n",
      "Train_BestReturn : -83.6969311983095\n",
      "TimeSinceStart : 457.1483938694\n",
      "Training Loss : 0.14871010184288025\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 94000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 94001\n",
      "mean reward (100 episodes) -82.290433\n",
      "best mean reward -82.290433\n",
      "running time 461.528254\n",
      "Train_EnvstepsSoFar : 94001\n",
      "Train_AverageReturn : -82.29043272408907\n",
      "Train_BestReturn : -82.29043272408907\n",
      "TimeSinceStart : 461.52825379371643\n",
      "Training Loss : 0.1443696916103363\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 95000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 95001\n",
      "mean reward (100 episodes) -82.298057\n",
      "best mean reward -82.290433\n",
      "running time 466.388468\n",
      "Train_EnvstepsSoFar : 95001\n",
      "Train_AverageReturn : -82.2980574347062\n",
      "Train_BestReturn : -82.29043272408907\n",
      "TimeSinceStart : 466.3884677886963\n",
      "Training Loss : 0.16578267514705658\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 96000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 96001\n",
      "mean reward (100 episodes) -77.723262\n",
      "best mean reward -77.723262\n",
      "running time 470.319412\n",
      "Train_EnvstepsSoFar : 96001\n",
      "Train_AverageReturn : -77.72326187986165\n",
      "Train_BestReturn : -77.72326187986165\n",
      "TimeSinceStart : 470.31941199302673\n",
      "Training Loss : 0.14296695590019226\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 97000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 97001\n",
      "mean reward (100 episodes) -77.422739\n",
      "best mean reward -77.422739\n",
      "running time 474.808251\n",
      "Train_EnvstepsSoFar : 97001\n",
      "Train_AverageReturn : -77.42273869080134\n",
      "Train_BestReturn : -77.42273869080134\n",
      "TimeSinceStart : 474.80825090408325\n",
      "Training Loss : 0.5520066022872925\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 98000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 98001\n",
      "mean reward (100 episodes) -74.863570\n",
      "best mean reward -74.863570\n",
      "running time 479.426846\n",
      "Train_EnvstepsSoFar : 98001\n",
      "Train_AverageReturn : -74.86357034770765\n",
      "Train_BestReturn : -74.86357034770765\n",
      "TimeSinceStart : 479.4268457889557\n",
      "Training Loss : 0.2147897332906723\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 99000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 99001\n",
      "mean reward (100 episodes) -73.142660\n",
      "best mean reward -73.142660\n",
      "running time 484.342866\n",
      "Train_EnvstepsSoFar : 99001\n",
      "Train_AverageReturn : -73.14265963753293\n",
      "Train_BestReturn : -73.14265963753293\n",
      "TimeSinceStart : 484.3428659439087\n",
      "Training Loss : 0.07688187062740326\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 100001\n",
      "mean reward (100 episodes) -69.823323\n",
      "best mean reward -69.823323\n",
      "running time 488.712836\n",
      "Train_EnvstepsSoFar : 100001\n",
      "Train_AverageReturn : -69.82332271487107\n",
      "Train_BestReturn : -69.82332271487107\n",
      "TimeSinceStart : 488.7128357887268\n",
      "Training Loss : 0.3543131351470947\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 101001\n",
      "mean reward (100 episodes) -67.907610\n",
      "best mean reward -67.907610\n",
      "running time 493.364613\n",
      "Train_EnvstepsSoFar : 101001\n",
      "Train_AverageReturn : -67.90761027783046\n",
      "Train_BestReturn : -67.90761027783046\n",
      "TimeSinceStart : 493.3646128177643\n",
      "Training Loss : 0.2713487148284912\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 102001\n",
      "mean reward (100 episodes) -67.283570\n",
      "best mean reward -67.283570\n",
      "running time 497.576193\n",
      "Train_EnvstepsSoFar : 102001\n",
      "Train_AverageReturn : -67.28357044260268\n",
      "Train_BestReturn : -67.28357044260268\n",
      "TimeSinceStart : 497.57619285583496\n",
      "Training Loss : 0.09353841841220856\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 103001\n",
      "mean reward (100 episodes) -65.144821\n",
      "best mean reward -65.144821\n",
      "running time 502.578564\n",
      "Train_EnvstepsSoFar : 103001\n",
      "Train_AverageReturn : -65.14482087868738\n",
      "Train_BestReturn : -65.14482087868738\n",
      "TimeSinceStart : 502.57856369018555\n",
      "Training Loss : 0.36905860900878906\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 104001\n",
      "mean reward (100 episodes) -62.061539\n",
      "best mean reward -62.061539\n",
      "running time 506.665686\n",
      "Train_EnvstepsSoFar : 104001\n",
      "Train_AverageReturn : -62.06153906898342\n",
      "Train_BestReturn : -62.06153906898342\n",
      "TimeSinceStart : 506.6656858921051\n",
      "Training Loss : 0.1743796169757843\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 105001\n",
      "mean reward (100 episodes) -59.508643\n",
      "best mean reward -59.508643\n",
      "running time 511.610413\n",
      "Train_EnvstepsSoFar : 105001\n",
      "Train_AverageReturn : -59.50864327817697\n",
      "Train_BestReturn : -59.50864327817697\n",
      "TimeSinceStart : 511.61041259765625\n",
      "Training Loss : 0.20275402069091797\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 106001\n",
      "mean reward (100 episodes) -53.711639\n",
      "best mean reward -53.711639\n",
      "running time 517.162763\n",
      "Train_EnvstepsSoFar : 106001\n",
      "Train_AverageReturn : -53.71163868228866\n",
      "Train_BestReturn : -53.71163868228866\n",
      "TimeSinceStart : 517.1627628803253\n",
      "Training Loss : 0.14710812270641327\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 107001\n",
      "mean reward (100 episodes) -52.696713\n",
      "best mean reward -52.696713\n",
      "running time 528.038487\n",
      "Train_EnvstepsSoFar : 107001\n",
      "Train_AverageReturn : -52.69671294489532\n",
      "Train_BestReturn : -52.69671294489532\n",
      "TimeSinceStart : 528.0384867191315\n",
      "Training Loss : 0.12203685194253922\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 108001\n",
      "mean reward (100 episodes) -48.283629\n",
      "best mean reward -48.283629\n",
      "running time 533.530062\n",
      "Train_EnvstepsSoFar : 108001\n",
      "Train_AverageReturn : -48.283629356665955\n",
      "Train_BestReturn : -48.283629356665955\n",
      "TimeSinceStart : 533.5300617218018\n",
      "Training Loss : 0.06895040720701218\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 109001\n",
      "mean reward (100 episodes) -47.613674\n",
      "best mean reward -47.613674\n",
      "running time 545.036637\n",
      "Train_EnvstepsSoFar : 109001\n",
      "Train_AverageReturn : -47.6136735697192\n",
      "Train_BestReturn : -47.6136735697192\n",
      "TimeSinceStart : 545.0366368293762\n",
      "Training Loss : 0.5823303461074829\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 110001\n",
      "mean reward (100 episodes) -41.795071\n",
      "best mean reward -41.795071\n",
      "running time 549.186397\n",
      "Train_EnvstepsSoFar : 110001\n",
      "Train_AverageReturn : -41.79507083991961\n",
      "Train_BestReturn : -41.79507083991961\n",
      "TimeSinceStart : 549.1863968372345\n",
      "Training Loss : 0.08911784738302231\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 111001\n",
      "mean reward (100 episodes) -39.001285\n",
      "best mean reward -39.001285\n",
      "running time 554.630691\n",
      "Train_EnvstepsSoFar : 111001\n",
      "Train_AverageReturn : -39.00128512689186\n",
      "Train_BestReturn : -39.00128512689186\n",
      "TimeSinceStart : 554.6306908130646\n",
      "Training Loss : 0.08940636366605759\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 112001\n",
      "mean reward (100 episodes) -41.653335\n",
      "best mean reward -39.001285\n",
      "running time 561.425829\n",
      "Train_EnvstepsSoFar : 112001\n",
      "Train_AverageReturn : -41.65333526605338\n",
      "Train_BestReturn : -39.00128512689186\n",
      "TimeSinceStart : 561.4258286952972\n",
      "Training Loss : 0.07221543788909912\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 113001\n",
      "mean reward (100 episodes) -38.940139\n",
      "best mean reward -38.940139\n",
      "running time 573.483876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 113001\n",
      "Train_AverageReturn : -38.94013941648632\n",
      "Train_BestReturn : -38.94013941648632\n",
      "TimeSinceStart : 573.4838757514954\n",
      "Training Loss : 0.19876936078071594\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 114001\n",
      "mean reward (100 episodes) -37.702094\n",
      "best mean reward -37.702094\n",
      "running time 579.240916\n",
      "Train_EnvstepsSoFar : 114001\n",
      "Train_AverageReturn : -37.70209443447619\n",
      "Train_BestReturn : -37.70209443447619\n",
      "TimeSinceStart : 579.2409157752991\n",
      "Training Loss : 0.1421586126089096\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 115001\n",
      "mean reward (100 episodes) -35.762724\n",
      "best mean reward -35.762724\n",
      "running time 584.613102\n",
      "Train_EnvstepsSoFar : 115001\n",
      "Train_AverageReturn : -35.76272432745173\n",
      "Train_BestReturn : -35.76272432745173\n",
      "TimeSinceStart : 584.6131017208099\n",
      "Training Loss : 0.16620247066020966\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 116001\n",
      "mean reward (100 episodes) -29.393110\n",
      "best mean reward -29.393110\n",
      "running time 589.142798\n",
      "Train_EnvstepsSoFar : 116001\n",
      "Train_AverageReturn : -29.393109654459373\n",
      "Train_BestReturn : -29.393109654459373\n",
      "TimeSinceStart : 589.1427977085114\n",
      "Training Loss : 0.09152545779943466\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 117001\n",
      "mean reward (100 episodes) -25.245907\n",
      "best mean reward -25.245907\n",
      "running time 594.526086\n",
      "Train_EnvstepsSoFar : 117001\n",
      "Train_AverageReturn : -25.24590728175874\n",
      "Train_BestReturn : -25.24590728175874\n",
      "TimeSinceStart : 594.5260858535767\n",
      "Training Loss : 0.20103411376476288\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 118001\n",
      "mean reward (100 episodes) -23.667441\n",
      "best mean reward -23.667441\n",
      "running time 600.669113\n",
      "Train_EnvstepsSoFar : 118001\n",
      "Train_AverageReturn : -23.667440679056035\n",
      "Train_BestReturn : -23.667440679056035\n",
      "TimeSinceStart : 600.6691126823425\n",
      "Training Loss : 0.1852979063987732\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 119001\n",
      "mean reward (100 episodes) -23.049878\n",
      "best mean reward -23.049878\n",
      "running time 606.463898\n",
      "Train_EnvstepsSoFar : 119001\n",
      "Train_AverageReturn : -23.04987759711948\n",
      "Train_BestReturn : -23.04987759711948\n",
      "TimeSinceStart : 606.4638977050781\n",
      "Training Loss : 0.15974728763103485\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 120001\n",
      "mean reward (100 episodes) -20.080149\n",
      "best mean reward -20.080149\n",
      "running time 611.234489\n",
      "Train_EnvstepsSoFar : 120001\n",
      "Train_AverageReturn : -20.080148517070327\n",
      "Train_BestReturn : -20.080148517070327\n",
      "TimeSinceStart : 611.2344887256622\n",
      "Training Loss : 0.12861613929271698\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 121001\n",
      "mean reward (100 episodes) -16.875685\n",
      "best mean reward -16.875685\n",
      "running time 616.096932\n",
      "Train_EnvstepsSoFar : 121001\n",
      "Train_AverageReturn : -16.87568472949264\n",
      "Train_BestReturn : -16.87568472949264\n",
      "TimeSinceStart : 616.0969316959381\n",
      "Training Loss : 0.37827685475349426\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 122001\n",
      "mean reward (100 episodes) -16.587680\n",
      "best mean reward -16.587680\n",
      "running time 621.405078\n",
      "Train_EnvstepsSoFar : 122001\n",
      "Train_AverageReturn : -16.587679561447015\n",
      "Train_BestReturn : -16.587679561447015\n",
      "TimeSinceStart : 621.4050779342651\n",
      "Training Loss : 0.08361982554197311\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 123001\n",
      "mean reward (100 episodes) -12.690323\n",
      "best mean reward -12.690323\n",
      "running time 625.894186\n",
      "Train_EnvstepsSoFar : 123001\n",
      "Train_AverageReturn : -12.6903230598862\n",
      "Train_BestReturn : -12.6903230598862\n",
      "TimeSinceStart : 625.8941857814789\n",
      "Training Loss : 0.5308345556259155\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 124001\n",
      "mean reward (100 episodes) -13.081139\n",
      "best mean reward -12.690323\n",
      "running time 630.659639\n",
      "Train_EnvstepsSoFar : 124001\n",
      "Train_AverageReturn : -13.081139407079075\n",
      "Train_BestReturn : -12.6903230598862\n",
      "TimeSinceStart : 630.6596388816833\n",
      "Training Loss : 0.10259139537811279\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 125001\n",
      "mean reward (100 episodes) -9.810068\n",
      "best mean reward -9.810068\n",
      "running time 634.883239\n",
      "Train_EnvstepsSoFar : 125001\n",
      "Train_AverageReturn : -9.810067895210507\n",
      "Train_BestReturn : -9.810067895210507\n",
      "TimeSinceStart : 634.8832387924194\n",
      "Training Loss : 0.15578718483448029\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 126001\n",
      "mean reward (100 episodes) -9.686410\n",
      "best mean reward -9.686410\n",
      "running time 639.333251\n",
      "Train_EnvstepsSoFar : 126001\n",
      "Train_AverageReturn : -9.686410390731638\n",
      "Train_BestReturn : -9.686410390731638\n",
      "TimeSinceStart : 639.3332507610321\n",
      "Training Loss : 0.0914759486913681\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 127001\n",
      "mean reward (100 episodes) -9.529007\n",
      "best mean reward -9.529007\n",
      "running time 645.321557\n",
      "Train_EnvstepsSoFar : 127001\n",
      "Train_AverageReturn : -9.52900680237347\n",
      "Train_BestReturn : -9.52900680237347\n",
      "TimeSinceStart : 645.3215568065643\n",
      "Training Loss : 0.08098962157964706\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 128001\n",
      "mean reward (100 episodes) -6.131256\n",
      "best mean reward -6.131256\n",
      "running time 649.977612\n",
      "Train_EnvstepsSoFar : 128001\n",
      "Train_AverageReturn : -6.131255508196375\n",
      "Train_BestReturn : -6.131255508196375\n",
      "TimeSinceStart : 649.9776120185852\n",
      "Training Loss : 0.551317572593689\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 129001\n",
      "mean reward (100 episodes) -4.830425\n",
      "best mean reward -4.830425\n",
      "running time 655.586837\n",
      "Train_EnvstepsSoFar : 129001\n",
      "Train_AverageReturn : -4.830424799592105\n",
      "Train_BestReturn : -4.830424799592105\n",
      "TimeSinceStart : 655.5868368148804\n",
      "Training Loss : 0.049112848937511444\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 130001\n",
      "mean reward (100 episodes) -4.286968\n",
      "best mean reward -4.286968\n",
      "running time 661.255660\n",
      "Train_EnvstepsSoFar : 130001\n",
      "Train_AverageReturn : -4.286968013405681\n",
      "Train_BestReturn : -4.286968013405681\n",
      "TimeSinceStart : 661.2556600570679\n",
      "Training Loss : 0.2227264791727066\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 131001\n",
      "mean reward (100 episodes) -1.423879\n",
      "best mean reward -1.423879\n",
      "running time 666.793470\n",
      "Train_EnvstepsSoFar : 131001\n",
      "Train_AverageReturn : -1.423878575820145\n",
      "Train_BestReturn : -1.423878575820145\n",
      "TimeSinceStart : 666.7934699058533\n",
      "Training Loss : 0.22223001718521118\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 132001\n",
      "mean reward (100 episodes) 2.962820\n",
      "best mean reward 2.962820\n",
      "running time 670.954891\n",
      "Train_EnvstepsSoFar : 132001\n",
      "Train_AverageReturn : 2.9628200265643456\n",
      "Train_BestReturn : 2.9628200265643456\n",
      "TimeSinceStart : 670.9548909664154\n",
      "Training Loss : 0.24472655355930328\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 133001\n",
      "mean reward (100 episodes) 5.033622\n",
      "best mean reward 5.033622\n",
      "running time 675.809199\n",
      "Train_EnvstepsSoFar : 133001\n",
      "Train_AverageReturn : 5.033621750138968\n",
      "Train_BestReturn : 5.033621750138968\n",
      "TimeSinceStart : 675.8091988563538\n",
      "Training Loss : 0.1538805067539215\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 134001\n",
      "mean reward (100 episodes) 6.295530\n",
      "best mean reward 6.295530\n",
      "running time 680.795081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 134001\n",
      "Train_AverageReturn : 6.295530160449896\n",
      "Train_BestReturn : 6.295530160449896\n",
      "TimeSinceStart : 680.7950809001923\n",
      "Training Loss : 0.1001020148396492\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 135001\n",
      "mean reward (100 episodes) 8.128076\n",
      "best mean reward 8.128076\n",
      "running time 685.277220\n",
      "Train_EnvstepsSoFar : 135001\n",
      "Train_AverageReturn : 8.128076282953847\n",
      "Train_BestReturn : 8.128076282953847\n",
      "TimeSinceStart : 685.2772197723389\n",
      "Training Loss : 0.35216158628463745\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 136001\n",
      "mean reward (100 episodes) 9.296009\n",
      "best mean reward 9.296009\n",
      "running time 691.534290\n",
      "Train_EnvstepsSoFar : 136001\n",
      "Train_AverageReturn : 9.29600921257809\n",
      "Train_BestReturn : 9.29600921257809\n",
      "TimeSinceStart : 691.5342898368835\n",
      "Training Loss : 0.13239380717277527\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 137001\n",
      "mean reward (100 episodes) 16.905231\n",
      "best mean reward 16.905231\n",
      "running time 695.932888\n",
      "Train_EnvstepsSoFar : 137001\n",
      "Train_AverageReturn : 16.905231045864483\n",
      "Train_BestReturn : 16.905231045864483\n",
      "TimeSinceStart : 695.9328877925873\n",
      "Training Loss : 0.07784350961446762\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 138001\n",
      "mean reward (100 episodes) 23.960173\n",
      "best mean reward 23.960173\n",
      "running time 700.491751\n",
      "Train_EnvstepsSoFar : 138001\n",
      "Train_AverageReturn : 23.960173407191487\n",
      "Train_BestReturn : 23.960173407191487\n",
      "TimeSinceStart : 700.4917507171631\n",
      "Training Loss : 0.17738835513591766\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 139001\n",
      "mean reward (100 episodes) 29.241282\n",
      "best mean reward 29.241282\n",
      "running time 704.921582\n",
      "Train_EnvstepsSoFar : 139001\n",
      "Train_AverageReturn : 29.24128216369833\n",
      "Train_BestReturn : 29.24128216369833\n",
      "TimeSinceStart : 704.9215817451477\n",
      "Training Loss : 0.24302037060260773\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 140001\n",
      "mean reward (100 episodes) 33.965031\n",
      "best mean reward 33.965031\n",
      "running time 709.482631\n",
      "Train_EnvstepsSoFar : 140001\n",
      "Train_AverageReturn : 33.96503140961154\n",
      "Train_BestReturn : 33.96503140961154\n",
      "TimeSinceStart : 709.4826307296753\n",
      "Training Loss : 0.4915419816970825\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 141001\n",
      "mean reward (100 episodes) 39.159229\n",
      "best mean reward 39.159229\n",
      "running time 713.599193\n",
      "Train_EnvstepsSoFar : 141001\n",
      "Train_AverageReturn : 39.159229437893345\n",
      "Train_BestReturn : 39.159229437893345\n",
      "TimeSinceStart : 713.5991928577423\n",
      "Training Loss : 0.1729171723127365\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 142001\n",
      "mean reward (100 episodes) 42.928944\n",
      "best mean reward 42.928944\n",
      "running time 718.109872\n",
      "Train_EnvstepsSoFar : 142001\n",
      "Train_AverageReturn : 42.92894356529441\n",
      "Train_BestReturn : 42.92894356529441\n",
      "TimeSinceStart : 718.1098718643188\n",
      "Training Loss : 0.4083881974220276\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 143001\n",
      "mean reward (100 episodes) 43.138208\n",
      "best mean reward 43.138208\n",
      "running time 723.605442\n",
      "Train_EnvstepsSoFar : 143001\n",
      "Train_AverageReturn : 43.13820759162688\n",
      "Train_BestReturn : 43.13820759162688\n",
      "TimeSinceStart : 723.6054418087006\n",
      "Training Loss : 0.18113717436790466\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 144001\n",
      "mean reward (100 episodes) 42.457659\n",
      "best mean reward 43.138208\n",
      "running time 728.686416\n",
      "Train_EnvstepsSoFar : 144001\n",
      "Train_AverageReturn : 42.45765856023516\n",
      "Train_BestReturn : 43.13820759162688\n",
      "TimeSinceStart : 728.6864156723022\n",
      "Training Loss : 0.10978826135396957\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 145001\n",
      "mean reward (100 episodes) 40.603351\n",
      "best mean reward 43.138208\n",
      "running time 733.558799\n",
      "Train_EnvstepsSoFar : 145001\n",
      "Train_AverageReturn : 40.60335096077752\n",
      "Train_BestReturn : 43.13820759162688\n",
      "TimeSinceStart : 733.558798789978\n",
      "Training Loss : 0.12473331391811371\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 146001\n",
      "mean reward (100 episodes) 38.437118\n",
      "best mean reward 43.138208\n",
      "running time 738.242020\n",
      "Train_EnvstepsSoFar : 146001\n",
      "Train_AverageReturn : 38.43711842860321\n",
      "Train_BestReturn : 43.13820759162688\n",
      "TimeSinceStart : 738.2420196533203\n",
      "Training Loss : 0.048286259174346924\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 147001\n",
      "mean reward (100 episodes) 38.512332\n",
      "best mean reward 43.138208\n",
      "running time 742.960552\n",
      "Train_EnvstepsSoFar : 147001\n",
      "Train_AverageReturn : 38.51233236351383\n",
      "Train_BestReturn : 43.13820759162688\n",
      "TimeSinceStart : 742.960551738739\n",
      "Training Loss : 0.4022546112537384\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 148001\n",
      "mean reward (100 episodes) 41.275391\n",
      "best mean reward 43.138208\n",
      "running time 747.976074\n",
      "Train_EnvstepsSoFar : 148001\n",
      "Train_AverageReturn : 41.27539087300518\n",
      "Train_BestReturn : 43.13820759162688\n",
      "TimeSinceStart : 747.9760739803314\n",
      "Training Loss : 0.3845283091068268\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 149001\n",
      "mean reward (100 episodes) 41.710287\n",
      "best mean reward 43.138208\n",
      "running time 753.641685\n",
      "Train_EnvstepsSoFar : 149001\n",
      "Train_AverageReturn : 41.710286962610965\n",
      "Train_BestReturn : 43.13820759162688\n",
      "TimeSinceStart : 753.6416850090027\n",
      "Training Loss : 0.8128912448883057\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 150001\n",
      "mean reward (100 episodes) 42.089581\n",
      "best mean reward 43.138208\n",
      "running time 759.243079\n",
      "Train_EnvstepsSoFar : 150001\n",
      "Train_AverageReturn : 42.089581096822236\n",
      "Train_BestReturn : 43.13820759162688\n",
      "TimeSinceStart : 759.2430787086487\n",
      "Training Loss : 0.167354017496109\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 151001\n",
      "mean reward (100 episodes) 47.762013\n",
      "best mean reward 47.762013\n",
      "running time 763.962927\n",
      "Train_EnvstepsSoFar : 151001\n",
      "Train_AverageReturn : 47.762013029973176\n",
      "Train_BestReturn : 47.762013029973176\n",
      "TimeSinceStart : 763.962926864624\n",
      "Training Loss : 0.3010486960411072\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 152001\n",
      "mean reward (100 episodes) 48.763010\n",
      "best mean reward 48.763010\n",
      "running time 768.586271\n",
      "Train_EnvstepsSoFar : 152001\n",
      "Train_AverageReturn : 48.763010368954696\n",
      "Train_BestReturn : 48.763010368954696\n",
      "TimeSinceStart : 768.5862708091736\n",
      "Training Loss : 0.2773742377758026\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 153001\n",
      "mean reward (100 episodes) 49.745692\n",
      "best mean reward 49.745692\n",
      "running time 774.392766\n",
      "Train_EnvstepsSoFar : 153001\n",
      "Train_AverageReturn : 49.74569216749692\n",
      "Train_BestReturn : 49.74569216749692\n",
      "TimeSinceStart : 774.3927657604218\n",
      "Training Loss : 0.0866212397813797\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 154001\n",
      "mean reward (100 episodes) 48.037790\n",
      "best mean reward 49.745692\n",
      "running time 780.056107\n",
      "Train_EnvstepsSoFar : 154001\n",
      "Train_AverageReturn : 48.03778981986073\n",
      "Train_BestReturn : 49.74569216749692\n",
      "TimeSinceStart : 780.0561068058014\n",
      "Training Loss : 0.2045077234506607\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 155001\n",
      "mean reward (100 episodes) 49.528426\n",
      "best mean reward 49.745692\n",
      "running time 784.937666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 155001\n",
      "Train_AverageReturn : 49.52842624477105\n",
      "Train_BestReturn : 49.74569216749692\n",
      "TimeSinceStart : 784.9376657009125\n",
      "Training Loss : 0.056239739060401917\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 156001\n",
      "mean reward (100 episodes) 50.155182\n",
      "best mean reward 50.155182\n",
      "running time 790.992136\n",
      "Train_EnvstepsSoFar : 156001\n",
      "Train_AverageReturn : 50.15518166562048\n",
      "Train_BestReturn : 50.15518166562048\n",
      "TimeSinceStart : 790.9921357631683\n",
      "Training Loss : 0.7790929079055786\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 157001\n",
      "mean reward (100 episodes) 48.583750\n",
      "best mean reward 50.155182\n",
      "running time 797.817924\n",
      "Train_EnvstepsSoFar : 157001\n",
      "Train_AverageReturn : 48.58374973339592\n",
      "Train_BestReturn : 50.15518166562048\n",
      "TimeSinceStart : 797.817923784256\n",
      "Training Loss : 0.691045343875885\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 158001\n",
      "mean reward (100 episodes) 54.510171\n",
      "best mean reward 54.510171\n",
      "running time 802.829352\n",
      "Train_EnvstepsSoFar : 158001\n",
      "Train_AverageReturn : 54.510170899794076\n",
      "Train_BestReturn : 54.510170899794076\n",
      "TimeSinceStart : 802.829351902008\n",
      "Training Loss : 0.04142114520072937\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 159001\n",
      "mean reward (100 episodes) 55.656180\n",
      "best mean reward 55.656180\n",
      "running time 809.202220\n",
      "Train_EnvstepsSoFar : 159001\n",
      "Train_AverageReturn : 55.656180063580166\n",
      "Train_BestReturn : 55.656180063580166\n",
      "TimeSinceStart : 809.2022199630737\n",
      "Training Loss : 0.32112112641334534\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 160001\n",
      "mean reward (100 episodes) 56.510152\n",
      "best mean reward 56.510152\n",
      "running time 814.037218\n",
      "Train_EnvstepsSoFar : 160001\n",
      "Train_AverageReturn : 56.51015229905279\n",
      "Train_BestReturn : 56.51015229905279\n",
      "TimeSinceStart : 814.0372176170349\n",
      "Training Loss : 0.7421975135803223\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 161001\n",
      "mean reward (100 episodes) 56.883274\n",
      "best mean reward 56.883274\n",
      "running time 820.380271\n",
      "Train_EnvstepsSoFar : 161001\n",
      "Train_AverageReturn : 56.88327409794385\n",
      "Train_BestReturn : 56.88327409794385\n",
      "TimeSinceStart : 820.3802707195282\n",
      "Training Loss : 0.2548255920410156\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 162001\n",
      "mean reward (100 episodes) 54.603389\n",
      "best mean reward 56.883274\n",
      "running time 825.499927\n",
      "Train_EnvstepsSoFar : 162001\n",
      "Train_AverageReturn : 54.6033890938431\n",
      "Train_BestReturn : 56.88327409794385\n",
      "TimeSinceStart : 825.4999268054962\n",
      "Training Loss : 0.08101968467235565\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 163001\n",
      "mean reward (100 episodes) 54.206875\n",
      "best mean reward 56.883274\n",
      "running time 830.917637\n",
      "Train_EnvstepsSoFar : 163001\n",
      "Train_AverageReturn : 54.20687544751977\n",
      "Train_BestReturn : 56.88327409794385\n",
      "TimeSinceStart : 830.9176368713379\n",
      "Training Loss : 0.09036711603403091\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 164001\n",
      "mean reward (100 episodes) 55.457206\n",
      "best mean reward 56.883274\n",
      "running time 835.617355\n",
      "Train_EnvstepsSoFar : 164001\n",
      "Train_AverageReturn : 55.45720643570235\n",
      "Train_BestReturn : 56.88327409794385\n",
      "TimeSinceStart : 835.6173548698425\n",
      "Training Loss : 0.105143703520298\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 165001\n",
      "mean reward (100 episodes) 55.127444\n",
      "best mean reward 56.883274\n",
      "running time 841.156419\n",
      "Train_EnvstepsSoFar : 165001\n",
      "Train_AverageReturn : 55.12744415081635\n",
      "Train_BestReturn : 56.88327409794385\n",
      "TimeSinceStart : 841.156418800354\n",
      "Training Loss : 0.09541475027799606\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 166001\n",
      "mean reward (100 episodes) 55.856335\n",
      "best mean reward 56.883274\n",
      "running time 845.406271\n",
      "Train_EnvstepsSoFar : 166001\n",
      "Train_AverageReturn : 55.856335253779314\n",
      "Train_BestReturn : 56.88327409794385\n",
      "TimeSinceStart : 845.406270980835\n",
      "Training Loss : 0.10437390208244324\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 167001\n",
      "mean reward (100 episodes) 55.088714\n",
      "best mean reward 56.883274\n",
      "running time 850.143765\n",
      "Train_EnvstepsSoFar : 167001\n",
      "Train_AverageReturn : 55.08871414859907\n",
      "Train_BestReturn : 56.88327409794385\n",
      "TimeSinceStart : 850.1437649726868\n",
      "Training Loss : 0.057034172117710114\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 168001\n",
      "mean reward (100 episodes) 58.380301\n",
      "best mean reward 58.380301\n",
      "running time 855.581868\n",
      "Train_EnvstepsSoFar : 168001\n",
      "Train_AverageReturn : 58.38030116510297\n",
      "Train_BestReturn : 58.38030116510297\n",
      "TimeSinceStart : 855.5818679332733\n",
      "Training Loss : 0.37362346053123474\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 169001\n",
      "mean reward (100 episodes) 57.398904\n",
      "best mean reward 58.380301\n",
      "running time 860.727035\n",
      "Train_EnvstepsSoFar : 169001\n",
      "Train_AverageReturn : 57.3989041769243\n",
      "Train_BestReturn : 58.38030116510297\n",
      "TimeSinceStart : 860.7270348072052\n",
      "Training Loss : 0.06382542848587036\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 170001\n",
      "mean reward (100 episodes) 56.115968\n",
      "best mean reward 58.380301\n",
      "running time 866.926452\n",
      "Train_EnvstepsSoFar : 170001\n",
      "Train_AverageReturn : 56.115968053776776\n",
      "Train_BestReturn : 58.38030116510297\n",
      "TimeSinceStart : 866.926451921463\n",
      "Training Loss : 0.25711309909820557\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 171001\n",
      "mean reward (100 episodes) 56.187201\n",
      "best mean reward 58.380301\n",
      "running time 872.199460\n",
      "Train_EnvstepsSoFar : 171001\n",
      "Train_AverageReturn : 56.187200634311566\n",
      "Train_BestReturn : 58.38030116510297\n",
      "TimeSinceStart : 872.1994597911835\n",
      "Training Loss : 0.13206854462623596\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 172001\n",
      "mean reward (100 episodes) 55.585188\n",
      "best mean reward 58.380301\n",
      "running time 877.573767\n",
      "Train_EnvstepsSoFar : 172001\n",
      "Train_AverageReturn : 55.58518773371982\n",
      "Train_BestReturn : 58.38030116510297\n",
      "TimeSinceStart : 877.573766708374\n",
      "Training Loss : 0.36327290534973145\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 173001\n",
      "mean reward (100 episodes) 54.941791\n",
      "best mean reward 58.380301\n",
      "running time 883.151811\n",
      "Train_EnvstepsSoFar : 173001\n",
      "Train_AverageReturn : 54.94179096963467\n",
      "Train_BestReturn : 58.38030116510297\n",
      "TimeSinceStart : 883.1518106460571\n",
      "Training Loss : 0.07206618040800095\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 174001\n",
      "mean reward (100 episodes) 53.784995\n",
      "best mean reward 58.380301\n",
      "running time 888.319704\n",
      "Train_EnvstepsSoFar : 174001\n",
      "Train_AverageReturn : 53.784995100835424\n",
      "Train_BestReturn : 58.38030116510297\n",
      "TimeSinceStart : 888.3197038173676\n",
      "Training Loss : 0.1439526379108429\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 175001\n",
      "mean reward (100 episodes) 54.077043\n",
      "best mean reward 58.380301\n",
      "running time 893.423130\n",
      "Train_EnvstepsSoFar : 175001\n",
      "Train_AverageReturn : 54.07704253782326\n",
      "Train_BestReturn : 58.38030116510297\n",
      "TimeSinceStart : 893.4231297969818\n",
      "Training Loss : 0.1910327672958374\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 176001\n",
      "mean reward (100 episodes) 53.110079\n",
      "best mean reward 58.380301\n",
      "running time 897.979174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 176001\n",
      "Train_AverageReturn : 53.11007901617103\n",
      "Train_BestReturn : 58.38030116510297\n",
      "TimeSinceStart : 897.9791738986969\n",
      "Training Loss : 0.1825830638408661\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 177001\n",
      "mean reward (100 episodes) 52.856909\n",
      "best mean reward 58.380301\n",
      "running time 903.195857\n",
      "Train_EnvstepsSoFar : 177001\n",
      "Train_AverageReturn : 52.85690922973777\n",
      "Train_BestReturn : 58.38030116510297\n",
      "TimeSinceStart : 903.1958568096161\n",
      "Training Loss : 0.1502504199743271\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 178001\n",
      "mean reward (100 episodes) 53.127931\n",
      "best mean reward 58.380301\n",
      "running time 908.406853\n",
      "Train_EnvstepsSoFar : 178001\n",
      "Train_AverageReturn : 53.12793065028716\n",
      "Train_BestReturn : 58.38030116510297\n",
      "TimeSinceStart : 908.406852722168\n",
      "Training Loss : 0.2219274491071701\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 179001\n",
      "mean reward (100 episodes) 50.373328\n",
      "best mean reward 58.380301\n",
      "running time 914.338817\n",
      "Train_EnvstepsSoFar : 179001\n",
      "Train_AverageReturn : 50.37332822493318\n",
      "Train_BestReturn : 58.38030116510297\n",
      "TimeSinceStart : 914.3388168811798\n",
      "Training Loss : 0.17201407253742218\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 180001\n",
      "mean reward (100 episodes) 51.692388\n",
      "best mean reward 58.380301\n",
      "running time 919.056293\n",
      "Train_EnvstepsSoFar : 180001\n",
      "Train_AverageReturn : 51.69238840788157\n",
      "Train_BestReturn : 58.38030116510297\n",
      "TimeSinceStart : 919.0562927722931\n",
      "Training Loss : 0.08184896409511566\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 181001\n",
      "mean reward (100 episodes) 54.849946\n",
      "best mean reward 58.380301\n",
      "running time 923.545209\n",
      "Train_EnvstepsSoFar : 181001\n",
      "Train_AverageReturn : 54.849946189878665\n",
      "Train_BestReturn : 58.38030116510297\n",
      "TimeSinceStart : 923.5452086925507\n",
      "Training Loss : 0.25983646512031555\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 182001\n",
      "mean reward (100 episodes) 53.125857\n",
      "best mean reward 58.380301\n",
      "running time 928.893751\n",
      "Train_EnvstepsSoFar : 182001\n",
      "Train_AverageReturn : 53.12585692875404\n",
      "Train_BestReturn : 58.38030116510297\n",
      "TimeSinceStart : 928.8937509059906\n",
      "Training Loss : 0.1089281514286995\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 183001\n",
      "mean reward (100 episodes) 58.008383\n",
      "best mean reward 58.380301\n",
      "running time 933.460110\n",
      "Train_EnvstepsSoFar : 183001\n",
      "Train_AverageReturn : 58.008383432714155\n",
      "Train_BestReturn : 58.38030116510297\n",
      "TimeSinceStart : 933.4601097106934\n",
      "Training Loss : 0.04856252670288086\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 184001\n",
      "mean reward (100 episodes) 56.757168\n",
      "best mean reward 58.380301\n",
      "running time 938.690960\n",
      "Train_EnvstepsSoFar : 184001\n",
      "Train_AverageReturn : 56.757168164364586\n",
      "Train_BestReturn : 58.38030116510297\n",
      "TimeSinceStart : 938.6909596920013\n",
      "Training Loss : 0.07075127959251404\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 185001\n",
      "mean reward (100 episodes) 55.762311\n",
      "best mean reward 58.380301\n",
      "running time 943.640141\n",
      "Train_EnvstepsSoFar : 185001\n",
      "Train_AverageReturn : 55.762311269556655\n",
      "Train_BestReturn : 58.38030116510297\n",
      "TimeSinceStart : 943.6401407718658\n",
      "Training Loss : 0.4632439911365509\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 186001\n",
      "mean reward (100 episodes) 60.192913\n",
      "best mean reward 60.192913\n",
      "running time 947.866579\n",
      "Train_EnvstepsSoFar : 186001\n",
      "Train_AverageReturn : 60.19291282642427\n",
      "Train_BestReturn : 60.19291282642427\n",
      "TimeSinceStart : 947.8665788173676\n",
      "Training Loss : 0.11437196284532547\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 187001\n",
      "mean reward (100 episodes) 61.753721\n",
      "best mean reward 61.753721\n",
      "running time 951.853487\n",
      "Train_EnvstepsSoFar : 187001\n",
      "Train_AverageReturn : 61.75372100715494\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 951.8534867763519\n",
      "Training Loss : 0.0925273448228836\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 188001\n",
      "mean reward (100 episodes) 59.856136\n",
      "best mean reward 61.753721\n",
      "running time 956.700327\n",
      "Train_EnvstepsSoFar : 188001\n",
      "Train_AverageReturn : 59.85613583399105\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 956.7003269195557\n",
      "Training Loss : 0.048683542758226395\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 189001\n",
      "mean reward (100 episodes) 59.100672\n",
      "best mean reward 61.753721\n",
      "running time 962.417122\n",
      "Train_EnvstepsSoFar : 189001\n",
      "Train_AverageReturn : 59.10067193986777\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 962.4171216487885\n",
      "Training Loss : 0.09660805016756058\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 190001\n",
      "mean reward (100 episodes) 58.624772\n",
      "best mean reward 61.753721\n",
      "running time 966.610796\n",
      "Train_EnvstepsSoFar : 190001\n",
      "Train_AverageReturn : 58.62477182651574\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 966.6107959747314\n",
      "Training Loss : 0.07628201693296432\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 191001\n",
      "mean reward (100 episodes) 60.409710\n",
      "best mean reward 61.753721\n",
      "running time 971.019009\n",
      "Train_EnvstepsSoFar : 191001\n",
      "Train_AverageReturn : 60.409710024207236\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 971.0190088748932\n",
      "Training Loss : 0.24377191066741943\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 192001\n",
      "mean reward (100 episodes) 59.371153\n",
      "best mean reward 61.753721\n",
      "running time 977.583794\n",
      "Train_EnvstepsSoFar : 192001\n",
      "Train_AverageReturn : 59.37115342511677\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 977.5837938785553\n",
      "Training Loss : 0.05650454759597778\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 193001\n",
      "mean reward (100 episodes) 61.361270\n",
      "best mean reward 61.753721\n",
      "running time 983.158617\n",
      "Train_EnvstepsSoFar : 193001\n",
      "Train_AverageReturn : 61.36126980928224\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 983.1586167812347\n",
      "Training Loss : 0.14562448859214783\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 194001\n",
      "mean reward (100 episodes) 59.014630\n",
      "best mean reward 61.753721\n",
      "running time 987.906689\n",
      "Train_EnvstepsSoFar : 194001\n",
      "Train_AverageReturn : 59.014629883478825\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 987.9066889286041\n",
      "Training Loss : 0.14448124170303345\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 195001\n",
      "mean reward (100 episodes) 56.650952\n",
      "best mean reward 61.753721\n",
      "running time 992.689744\n",
      "Train_EnvstepsSoFar : 195001\n",
      "Train_AverageReturn : 56.650951905996614\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 992.6897437572479\n",
      "Training Loss : 0.05497576668858528\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 196001\n",
      "mean reward (100 episodes) 57.415677\n",
      "best mean reward 61.753721\n",
      "running time 997.113540\n",
      "Train_EnvstepsSoFar : 196001\n",
      "Train_AverageReturn : 57.4156768564052\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 997.1135396957397\n",
      "Training Loss : 0.07857733219861984\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 197001\n",
      "mean reward (100 episodes) 54.958212\n",
      "best mean reward 61.753721\n",
      "running time 1002.225383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 197001\n",
      "Train_AverageReturn : 54.95821160839641\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1002.2253828048706\n",
      "Training Loss : 0.05537428706884384\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 198001\n",
      "mean reward (100 episodes) 53.980004\n",
      "best mean reward 61.753721\n",
      "running time 1007.067003\n",
      "Train_EnvstepsSoFar : 198001\n",
      "Train_AverageReturn : 53.9800041279502\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1007.0670027732849\n",
      "Training Loss : 0.10930000245571136\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 199001\n",
      "mean reward (100 episodes) 51.660589\n",
      "best mean reward 61.753721\n",
      "running time 1012.469322\n",
      "Train_EnvstepsSoFar : 199001\n",
      "Train_AverageReturn : 51.6605892481416\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1012.4693219661713\n",
      "Training Loss : 0.409710168838501\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 200001\n",
      "mean reward (100 episodes) 51.009598\n",
      "best mean reward 61.753721\n",
      "running time 1016.858979\n",
      "Train_EnvstepsSoFar : 200001\n",
      "Train_AverageReturn : 51.00959822979576\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1016.8589787483215\n",
      "Training Loss : 0.08357415348291397\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 201001\n",
      "mean reward (100 episodes) 50.606550\n",
      "best mean reward 61.753721\n",
      "running time 1021.831899\n",
      "Train_EnvstepsSoFar : 201001\n",
      "Train_AverageReturn : 50.606549801928175\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1021.83189868927\n",
      "Training Loss : 3.526780843734741\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 202001\n",
      "mean reward (100 episodes) 47.847402\n",
      "best mean reward 61.753721\n",
      "running time 1026.865775\n",
      "Train_EnvstepsSoFar : 202001\n",
      "Train_AverageReturn : 47.847401635574414\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1026.8657748699188\n",
      "Training Loss : 0.21268188953399658\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 203001\n",
      "mean reward (100 episodes) 50.602034\n",
      "best mean reward 61.753721\n",
      "running time 1030.930862\n",
      "Train_EnvstepsSoFar : 203001\n",
      "Train_AverageReturn : 50.60203401282636\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1030.930861711502\n",
      "Training Loss : 0.0598171204328537\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 204001\n",
      "mean reward (100 episodes) 49.590265\n",
      "best mean reward 61.753721\n",
      "running time 1035.570683\n",
      "Train_EnvstepsSoFar : 204001\n",
      "Train_AverageReturn : 49.59026454571036\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1035.5706827640533\n",
      "Training Loss : 0.35077762603759766\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 205001\n",
      "mean reward (100 episodes) 51.955037\n",
      "best mean reward 61.753721\n",
      "running time 1040.146436\n",
      "Train_EnvstepsSoFar : 205001\n",
      "Train_AverageReturn : 51.95503719656497\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1040.1464357376099\n",
      "Training Loss : 0.06559538841247559\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 206001\n",
      "mean reward (100 episodes) 54.728703\n",
      "best mean reward 61.753721\n",
      "running time 1045.380020\n",
      "Train_EnvstepsSoFar : 206001\n",
      "Train_AverageReturn : 54.728703332466196\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1045.380019903183\n",
      "Training Loss : 0.07985518127679825\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 207001\n",
      "mean reward (100 episodes) 53.672636\n",
      "best mean reward 61.753721\n",
      "running time 1050.401182\n",
      "Train_EnvstepsSoFar : 207001\n",
      "Train_AverageReturn : 53.67263565689093\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1050.401181936264\n",
      "Training Loss : 0.10221495479345322\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 208001\n",
      "mean reward (100 episodes) 53.788078\n",
      "best mean reward 61.753721\n",
      "running time 1055.704185\n",
      "Train_EnvstepsSoFar : 208001\n",
      "Train_AverageReturn : 53.78807805152453\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1055.704184770584\n",
      "Training Loss : 0.42883333563804626\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 209001\n",
      "mean reward (100 episodes) 51.164263\n",
      "best mean reward 61.753721\n",
      "running time 1061.623447\n",
      "Train_EnvstepsSoFar : 209001\n",
      "Train_AverageReturn : 51.16426288422858\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1061.6234467029572\n",
      "Training Loss : 0.03976788371801376\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 210001\n",
      "mean reward (100 episodes) 51.009082\n",
      "best mean reward 61.753721\n",
      "running time 1066.521501\n",
      "Train_EnvstepsSoFar : 210001\n",
      "Train_AverageReturn : 51.009082114282236\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1066.521500825882\n",
      "Training Loss : 0.1828724890947342\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 211001\n",
      "mean reward (100 episodes) 51.018753\n",
      "best mean reward 61.753721\n",
      "running time 1072.294250\n",
      "Train_EnvstepsSoFar : 211001\n",
      "Train_AverageReturn : 51.01875256152148\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1072.2942497730255\n",
      "Training Loss : 0.05956144258379936\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 212001\n",
      "mean reward (100 episodes) 51.835294\n",
      "best mean reward 61.753721\n",
      "running time 1076.301828\n",
      "Train_EnvstepsSoFar : 212001\n",
      "Train_AverageReturn : 51.835294228082184\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1076.3018276691437\n",
      "Training Loss : 0.45005765557289124\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 213001\n",
      "mean reward (100 episodes) 52.062302\n",
      "best mean reward 61.753721\n",
      "running time 1081.234235\n",
      "Train_EnvstepsSoFar : 213001\n",
      "Train_AverageReturn : 52.06230211514497\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1081.2342348098755\n",
      "Training Loss : 0.10644753277301788\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 214001\n",
      "mean reward (100 episodes) 49.944836\n",
      "best mean reward 61.753721\n",
      "running time 1085.898801\n",
      "Train_EnvstepsSoFar : 214001\n",
      "Train_AverageReturn : 49.944835941040054\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1085.8988008499146\n",
      "Training Loss : 0.8553208112716675\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 215001\n",
      "mean reward (100 episodes) 49.131214\n",
      "best mean reward 61.753721\n",
      "running time 1090.546699\n",
      "Train_EnvstepsSoFar : 215001\n",
      "Train_AverageReturn : 49.13121410555754\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1090.54669880867\n",
      "Training Loss : 0.07861883193254471\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 216001\n",
      "mean reward (100 episodes) 46.957150\n",
      "best mean reward 61.753721\n",
      "running time 1094.889750\n",
      "Train_EnvstepsSoFar : 216001\n",
      "Train_AverageReturn : 46.957149718403116\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1094.8897500038147\n",
      "Training Loss : 0.17151983082294464\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 217001\n",
      "mean reward (100 episodes) 43.047747\n",
      "best mean reward 61.753721\n",
      "running time 1100.359271\n",
      "Train_EnvstepsSoFar : 217001\n",
      "Train_AverageReturn : 43.04774670098306\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1100.359270811081\n",
      "Training Loss : 0.25302597880363464\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 218001\n",
      "mean reward (100 episodes) 43.430950\n",
      "best mean reward 61.753721\n",
      "running time 1104.959931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 218001\n",
      "Train_AverageReturn : 43.430950071762084\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1104.959930896759\n",
      "Training Loss : 0.07710558921098709\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 219001\n",
      "mean reward (100 episodes) 40.370862\n",
      "best mean reward 61.753721\n",
      "running time 1110.311688\n",
      "Train_EnvstepsSoFar : 219001\n",
      "Train_AverageReturn : 40.37086216611678\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1110.3116879463196\n",
      "Training Loss : 0.10148104280233383\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 220001\n",
      "mean reward (100 episodes) 39.816360\n",
      "best mean reward 61.753721\n",
      "running time 1115.372871\n",
      "Train_EnvstepsSoFar : 220001\n",
      "Train_AverageReturn : 39.81636017693489\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1115.3728709220886\n",
      "Training Loss : 0.03826781362295151\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 221001\n",
      "mean reward (100 episodes) 38.497854\n",
      "best mean reward 61.753721\n",
      "running time 1120.655161\n",
      "Train_EnvstepsSoFar : 221001\n",
      "Train_AverageReturn : 38.49785360170704\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1120.655160665512\n",
      "Training Loss : 0.06351164728403091\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 222001\n",
      "mean reward (100 episodes) 39.123396\n",
      "best mean reward 61.753721\n",
      "running time 1124.641970\n",
      "Train_EnvstepsSoFar : 222001\n",
      "Train_AverageReturn : 39.123396070517984\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1124.6419699192047\n",
      "Training Loss : 0.09229325503110886\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 223001\n",
      "mean reward (100 episodes) 40.039564\n",
      "best mean reward 61.753721\n",
      "running time 1128.523813\n",
      "Train_EnvstepsSoFar : 223001\n",
      "Train_AverageReturn : 40.03956374436709\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1128.5238127708435\n",
      "Training Loss : 0.11198270320892334\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 224001\n",
      "mean reward (100 episodes) 40.062592\n",
      "best mean reward 61.753721\n",
      "running time 1133.784800\n",
      "Train_EnvstepsSoFar : 224001\n",
      "Train_AverageReturn : 40.06259159827735\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1133.7847998142242\n",
      "Training Loss : 0.07391169667243958\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 225001\n",
      "mean reward (100 episodes) 40.600151\n",
      "best mean reward 61.753721\n",
      "running time 1138.893897\n",
      "Train_EnvstepsSoFar : 225001\n",
      "Train_AverageReturn : 40.600150501863105\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1138.893896818161\n",
      "Training Loss : 0.6270447373390198\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 226001\n",
      "mean reward (100 episodes) 39.650650\n",
      "best mean reward 61.753721\n",
      "running time 1143.659634\n",
      "Train_EnvstepsSoFar : 226001\n",
      "Train_AverageReturn : 39.650650045829664\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1143.6596338748932\n",
      "Training Loss : 0.1561383306980133\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 227001\n",
      "mean reward (100 episodes) 41.012842\n",
      "best mean reward 61.753721\n",
      "running time 1148.088726\n",
      "Train_EnvstepsSoFar : 227001\n",
      "Train_AverageReturn : 41.01284207864477\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1148.0887258052826\n",
      "Training Loss : 0.3927376866340637\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 228001\n",
      "mean reward (100 episodes) 48.945533\n",
      "best mean reward 61.753721\n",
      "running time 1152.173381\n",
      "Train_EnvstepsSoFar : 228001\n",
      "Train_AverageReturn : 48.945532528289625\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1152.1733808517456\n",
      "Training Loss : 0.19151556491851807\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 229001\n",
      "mean reward (100 episodes) 55.116036\n",
      "best mean reward 61.753721\n",
      "running time 1156.049623\n",
      "Train_EnvstepsSoFar : 229001\n",
      "Train_AverageReturn : 55.116036131123536\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1156.0496227741241\n",
      "Training Loss : 0.07732845842838287\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 230001\n",
      "mean reward (100 episodes) 52.090984\n",
      "best mean reward 61.753721\n",
      "running time 1161.621977\n",
      "Train_EnvstepsSoFar : 230001\n",
      "Train_AverageReturn : 52.090983633434725\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1161.621976852417\n",
      "Training Loss : 0.12079470604658127\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 231001\n",
      "mean reward (100 episodes) 53.918748\n",
      "best mean reward 61.753721\n",
      "running time 1167.893180\n",
      "Train_EnvstepsSoFar : 231001\n",
      "Train_AverageReturn : 53.91874809603576\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1167.8931798934937\n",
      "Training Loss : 0.03811273351311684\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 232001\n",
      "mean reward (100 episodes) 54.716140\n",
      "best mean reward 61.753721\n",
      "running time 1172.391171\n",
      "Train_EnvstepsSoFar : 232001\n",
      "Train_AverageReturn : 54.71613973491429\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1172.3911707401276\n",
      "Training Loss : 0.08698828518390656\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 233001\n",
      "mean reward (100 episodes) 60.438416\n",
      "best mean reward 61.753721\n",
      "running time 1176.998075\n",
      "Train_EnvstepsSoFar : 233001\n",
      "Train_AverageReturn : 60.438416197673774\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1176.9980747699738\n",
      "Training Loss : 0.6541037559509277\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 234001\n",
      "mean reward (100 episodes) 58.919454\n",
      "best mean reward 61.753721\n",
      "running time 1181.978863\n",
      "Train_EnvstepsSoFar : 234001\n",
      "Train_AverageReturn : 58.91945368385015\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1181.9788627624512\n",
      "Training Loss : 0.25305402278900146\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 235001\n",
      "mean reward (100 episodes) 59.402979\n",
      "best mean reward 61.753721\n",
      "running time 1189.139844\n",
      "Train_EnvstepsSoFar : 235001\n",
      "Train_AverageReturn : 59.40297947635996\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1189.1398437023163\n",
      "Training Loss : 1.8413468599319458\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 236001\n",
      "mean reward (100 episodes) 59.883864\n",
      "best mean reward 61.753721\n",
      "running time 1193.857096\n",
      "Train_EnvstepsSoFar : 236001\n",
      "Train_AverageReturn : 59.8838640935199\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1193.8570959568024\n",
      "Training Loss : 0.1814083307981491\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 237001\n",
      "mean reward (100 episodes) 61.603748\n",
      "best mean reward 61.753721\n",
      "running time 1197.886018\n",
      "Train_EnvstepsSoFar : 237001\n",
      "Train_AverageReturn : 61.60374812544802\n",
      "Train_BestReturn : 61.75372100715494\n",
      "TimeSinceStart : 1197.8860177993774\n",
      "Training Loss : 0.11369600892066956\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 238001\n",
      "mean reward (100 episodes) 64.031755\n",
      "best mean reward 64.031755\n",
      "running time 1202.245720\n",
      "Train_EnvstepsSoFar : 238001\n",
      "Train_AverageReturn : 64.03175465080814\n",
      "Train_BestReturn : 64.03175465080814\n",
      "TimeSinceStart : 1202.2457196712494\n",
      "Training Loss : 0.33932358026504517\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 239001\n",
      "mean reward (100 episodes) 59.394329\n",
      "best mean reward 64.031755\n",
      "running time 1207.161096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 239001\n",
      "Train_AverageReturn : 59.39432851196456\n",
      "Train_BestReturn : 64.03175465080814\n",
      "TimeSinceStart : 1207.1610958576202\n",
      "Training Loss : 0.385295033454895\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 240001\n",
      "mean reward (100 episodes) 59.844980\n",
      "best mean reward 64.031755\n",
      "running time 1211.080508\n",
      "Train_EnvstepsSoFar : 240001\n",
      "Train_AverageReturn : 59.84497960258777\n",
      "Train_BestReturn : 64.03175465080814\n",
      "TimeSinceStart : 1211.0805077552795\n",
      "Training Loss : 0.07036439329385757\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 241001\n",
      "mean reward (100 episodes) 63.080044\n",
      "best mean reward 64.031755\n",
      "running time 1214.940825\n",
      "Train_EnvstepsSoFar : 241001\n",
      "Train_AverageReturn : 63.08004375250173\n",
      "Train_BestReturn : 64.03175465080814\n",
      "TimeSinceStart : 1214.9408247470856\n",
      "Training Loss : 0.134998619556427\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 242001\n",
      "mean reward (100 episodes) 65.810851\n",
      "best mean reward 65.810851\n",
      "running time 1219.111631\n",
      "Train_EnvstepsSoFar : 242001\n",
      "Train_AverageReturn : 65.81085088195226\n",
      "Train_BestReturn : 65.81085088195226\n",
      "TimeSinceStart : 1219.1116306781769\n",
      "Training Loss : 0.12549199163913727\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 243001\n",
      "mean reward (100 episodes) 65.740330\n",
      "best mean reward 65.810851\n",
      "running time 1224.114235\n",
      "Train_EnvstepsSoFar : 243001\n",
      "Train_AverageReturn : 65.74032980605354\n",
      "Train_BestReturn : 65.81085088195226\n",
      "TimeSinceStart : 1224.1142346858978\n",
      "Training Loss : 0.08333852142095566\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 244001\n",
      "mean reward (100 episodes) 70.329725\n",
      "best mean reward 70.329725\n",
      "running time 1228.456481\n",
      "Train_EnvstepsSoFar : 244001\n",
      "Train_AverageReturn : 70.32972523422683\n",
      "Train_BestReturn : 70.32972523422683\n",
      "TimeSinceStart : 1228.4564809799194\n",
      "Training Loss : 0.18087871372699738\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 245001\n",
      "mean reward (100 episodes) 67.711728\n",
      "best mean reward 70.329725\n",
      "running time 1234.366237\n",
      "Train_EnvstepsSoFar : 245001\n",
      "Train_AverageReturn : 67.71172806303503\n",
      "Train_BestReturn : 70.32972523422683\n",
      "TimeSinceStart : 1234.3662369251251\n",
      "Training Loss : 1.043505311012268\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 246001\n",
      "mean reward (100 episodes) 67.894788\n",
      "best mean reward 70.329725\n",
      "running time 1239.718072\n",
      "Train_EnvstepsSoFar : 246001\n",
      "Train_AverageReturn : 67.89478843473965\n",
      "Train_BestReturn : 70.32972523422683\n",
      "TimeSinceStart : 1239.7180716991425\n",
      "Training Loss : 1.2536736726760864\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 247001\n",
      "mean reward (100 episodes) 65.953637\n",
      "best mean reward 70.329725\n",
      "running time 1244.150941\n",
      "Train_EnvstepsSoFar : 247001\n",
      "Train_AverageReturn : 65.95363672666824\n",
      "Train_BestReturn : 70.32972523422683\n",
      "TimeSinceStart : 1244.1509408950806\n",
      "Training Loss : 0.1314329206943512\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 248001\n",
      "mean reward (100 episodes) 73.898505\n",
      "best mean reward 73.898505\n",
      "running time 1249.143727\n",
      "Train_EnvstepsSoFar : 248001\n",
      "Train_AverageReturn : 73.89850483306425\n",
      "Train_BestReturn : 73.89850483306425\n",
      "TimeSinceStart : 1249.143726825714\n",
      "Training Loss : 0.2791384160518646\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 249001\n",
      "mean reward (100 episodes) 72.738560\n",
      "best mean reward 73.898505\n",
      "running time 1253.163726\n",
      "Train_EnvstepsSoFar : 249001\n",
      "Train_AverageReturn : 72.73856049405015\n",
      "Train_BestReturn : 73.89850483306425\n",
      "TimeSinceStart : 1253.1637256145477\n",
      "Training Loss : 0.14932435750961304\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 250001\n",
      "mean reward (100 episodes) 74.144335\n",
      "best mean reward 74.144335\n",
      "running time 1257.108633\n",
      "Train_EnvstepsSoFar : 250001\n",
      "Train_AverageReturn : 74.14433530924413\n",
      "Train_BestReturn : 74.14433530924413\n",
      "TimeSinceStart : 1257.1086328029633\n",
      "Training Loss : 0.14909400045871735\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 251001\n",
      "mean reward (100 episodes) 76.696917\n",
      "best mean reward 76.696917\n",
      "running time 1261.736184\n",
      "Train_EnvstepsSoFar : 251001\n",
      "Train_AverageReturn : 76.69691651529538\n",
      "Train_BestReturn : 76.69691651529538\n",
      "TimeSinceStart : 1261.736183643341\n",
      "Training Loss : 0.1586080938577652\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 252001\n",
      "mean reward (100 episodes) 78.175106\n",
      "best mean reward 78.175106\n",
      "running time 1267.446244\n",
      "Train_EnvstepsSoFar : 252001\n",
      "Train_AverageReturn : 78.17510645170363\n",
      "Train_BestReturn : 78.17510645170363\n",
      "TimeSinceStart : 1267.44624376297\n",
      "Training Loss : 0.06461653113365173\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 253001\n",
      "mean reward (100 episodes) 80.639541\n",
      "best mean reward 80.639541\n",
      "running time 1271.823790\n",
      "Train_EnvstepsSoFar : 253001\n",
      "Train_AverageReturn : 80.63954088430603\n",
      "Train_BestReturn : 80.63954088430603\n",
      "TimeSinceStart : 1271.8237898349762\n",
      "Training Loss : 0.11586683988571167\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 254001\n",
      "mean reward (100 episodes) 77.660712\n",
      "best mean reward 80.639541\n",
      "running time 1276.890455\n",
      "Train_EnvstepsSoFar : 254001\n",
      "Train_AverageReturn : 77.66071219855425\n",
      "Train_BestReturn : 80.63954088430603\n",
      "TimeSinceStart : 1276.8904547691345\n",
      "Training Loss : 0.10128718614578247\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 255001\n",
      "mean reward (100 episodes) 85.081831\n",
      "best mean reward 85.081831\n",
      "running time 1281.356710\n",
      "Train_EnvstepsSoFar : 255001\n",
      "Train_AverageReturn : 85.08183092718383\n",
      "Train_BestReturn : 85.08183092718383\n",
      "TimeSinceStart : 1281.3567097187042\n",
      "Training Loss : 0.42954960465431213\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 256001\n",
      "mean reward (100 episodes) 82.851924\n",
      "best mean reward 85.081831\n",
      "running time 1285.955114\n",
      "Train_EnvstepsSoFar : 256001\n",
      "Train_AverageReturn : 82.85192373418018\n",
      "Train_BestReturn : 85.08183092718383\n",
      "TimeSinceStart : 1285.9551138877869\n",
      "Training Loss : 0.17292650043964386\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 257001\n",
      "mean reward (100 episodes) 90.355911\n",
      "best mean reward 90.355911\n",
      "running time 1290.633326\n",
      "Train_EnvstepsSoFar : 257001\n",
      "Train_AverageReturn : 90.35591058457548\n",
      "Train_BestReturn : 90.35591058457548\n",
      "TimeSinceStart : 1290.6333258152008\n",
      "Training Loss : 0.28252723813056946\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 258001\n",
      "mean reward (100 episodes) 95.463883\n",
      "best mean reward 95.463883\n",
      "running time 1295.190136\n",
      "Train_EnvstepsSoFar : 258001\n",
      "Train_AverageReturn : 95.46388294302993\n",
      "Train_BestReturn : 95.46388294302993\n",
      "TimeSinceStart : 1295.190135717392\n",
      "Training Loss : 2.2467427253723145\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 259001\n",
      "mean reward (100 episodes) 98.721612\n",
      "best mean reward 98.721612\n",
      "running time 1299.226260\n",
      "Train_EnvstepsSoFar : 259001\n",
      "Train_AverageReturn : 98.72161211550447\n",
      "Train_BestReturn : 98.72161211550447\n",
      "TimeSinceStart : 1299.2262599468231\n",
      "Training Loss : 0.10438909381628036\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 260001\n",
      "mean reward (100 episodes) 106.254542\n",
      "best mean reward 106.254542\n",
      "running time 1303.052614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 260001\n",
      "Train_AverageReturn : 106.25454243877869\n",
      "Train_BestReturn : 106.25454243877869\n",
      "TimeSinceStart : 1303.052613735199\n",
      "Training Loss : 0.09705817699432373\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 261001\n",
      "mean reward (100 episodes) 108.439437\n",
      "best mean reward 108.439437\n",
      "running time 1307.840602\n",
      "Train_EnvstepsSoFar : 261001\n",
      "Train_AverageReturn : 108.43943674516628\n",
      "Train_BestReturn : 108.43943674516628\n",
      "TimeSinceStart : 1307.840601682663\n",
      "Training Loss : 0.2091386616230011\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 262001\n",
      "mean reward (100 episodes) 111.434224\n",
      "best mean reward 111.434224\n",
      "running time 1311.541255\n",
      "Train_EnvstepsSoFar : 262001\n",
      "Train_AverageReturn : 111.43422388194847\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1311.5412547588348\n",
      "Training Loss : 0.07927919179201126\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 263001\n",
      "mean reward (100 episodes) 109.460664\n",
      "best mean reward 111.434224\n",
      "running time 1315.768353\n",
      "Train_EnvstepsSoFar : 263001\n",
      "Train_AverageReturn : 109.46066432218561\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1315.768352985382\n",
      "Training Loss : 0.1946522295475006\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 264001\n",
      "mean reward (100 episodes) 106.853105\n",
      "best mean reward 111.434224\n",
      "running time 1319.748169\n",
      "Train_EnvstepsSoFar : 264001\n",
      "Train_AverageReturn : 106.85310471191802\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1319.7481689453125\n",
      "Training Loss : 0.09030268341302872\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 265001\n",
      "mean reward (100 episodes) 108.428227\n",
      "best mean reward 111.434224\n",
      "running time 1323.884135\n",
      "Train_EnvstepsSoFar : 265001\n",
      "Train_AverageReturn : 108.42822724024289\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1323.8841347694397\n",
      "Training Loss : 0.11026414483785629\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 266001\n",
      "mean reward (100 episodes) 109.030503\n",
      "best mean reward 111.434224\n",
      "running time 1328.740280\n",
      "Train_EnvstepsSoFar : 266001\n",
      "Train_AverageReturn : 109.03050300084747\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1328.74027967453\n",
      "Training Loss : 0.15350371599197388\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 267001\n",
      "mean reward (100 episodes) 110.170654\n",
      "best mean reward 111.434224\n",
      "running time 1332.679093\n",
      "Train_EnvstepsSoFar : 267001\n",
      "Train_AverageReturn : 110.17065366706787\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1332.6790928840637\n",
      "Training Loss : 0.09468360245227814\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 268001\n",
      "mean reward (100 episodes) 110.758361\n",
      "best mean reward 111.434224\n",
      "running time 1336.604639\n",
      "Train_EnvstepsSoFar : 268001\n",
      "Train_AverageReturn : 110.75836052113203\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1336.6046388149261\n",
      "Training Loss : 0.6734524965286255\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 269001\n",
      "mean reward (100 episodes) 104.749313\n",
      "best mean reward 111.434224\n",
      "running time 1340.733184\n",
      "Train_EnvstepsSoFar : 269001\n",
      "Train_AverageReturn : 104.74931346018607\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1340.7331838607788\n",
      "Training Loss : 0.15496474504470825\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 270001\n",
      "mean reward (100 episodes) 104.501445\n",
      "best mean reward 111.434224\n",
      "running time 1344.567127\n",
      "Train_EnvstepsSoFar : 270001\n",
      "Train_AverageReturn : 104.50144508430186\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1344.567126750946\n",
      "Training Loss : 0.402137815952301\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 271001\n",
      "mean reward (100 episodes) 105.744819\n",
      "best mean reward 111.434224\n",
      "running time 1348.793544\n",
      "Train_EnvstepsSoFar : 271001\n",
      "Train_AverageReturn : 105.74481912251198\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1348.7935438156128\n",
      "Training Loss : 0.0610421746969223\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 272001\n",
      "mean reward (100 episodes) 105.922979\n",
      "best mean reward 111.434224\n",
      "running time 1352.807319\n",
      "Train_EnvstepsSoFar : 272001\n",
      "Train_AverageReturn : 105.92297889535595\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1352.807318687439\n",
      "Training Loss : 0.14984455704689026\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 273001\n",
      "mean reward (100 episodes) 104.685654\n",
      "best mean reward 111.434224\n",
      "running time 1356.921124\n",
      "Train_EnvstepsSoFar : 273001\n",
      "Train_AverageReturn : 104.6856538371766\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1356.9211237430573\n",
      "Training Loss : 0.8620103597640991\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 274001\n",
      "mean reward (100 episodes) 109.853888\n",
      "best mean reward 111.434224\n",
      "running time 1360.957680\n",
      "Train_EnvstepsSoFar : 274001\n",
      "Train_AverageReturn : 109.85388790523422\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1360.9576797485352\n",
      "Training Loss : 0.18159976601600647\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 275001\n",
      "mean reward (100 episodes) 103.688948\n",
      "best mean reward 111.434224\n",
      "running time 1365.280267\n",
      "Train_EnvstepsSoFar : 275001\n",
      "Train_AverageReturn : 103.68894812751127\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1365.2802670001984\n",
      "Training Loss : 0.2306632399559021\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 276001\n",
      "mean reward (100 episodes) 103.942808\n",
      "best mean reward 111.434224\n",
      "running time 1370.501402\n",
      "Train_EnvstepsSoFar : 276001\n",
      "Train_AverageReturn : 103.9428079452195\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1370.5014016628265\n",
      "Training Loss : 0.20140066742897034\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 277001\n",
      "mean reward (100 episodes) 101.000898\n",
      "best mean reward 111.434224\n",
      "running time 1376.411812\n",
      "Train_EnvstepsSoFar : 277001\n",
      "Train_AverageReturn : 101.00089777738873\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1376.4118118286133\n",
      "Training Loss : 0.30576783418655396\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 278001\n",
      "mean reward (100 episodes) 100.461552\n",
      "best mean reward 111.434224\n",
      "running time 1380.963698\n",
      "Train_EnvstepsSoFar : 278001\n",
      "Train_AverageReturn : 100.4615524865044\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1380.9636979103088\n",
      "Training Loss : 0.21394973993301392\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 279001\n",
      "mean reward (100 episodes) 99.650901\n",
      "best mean reward 111.434224\n",
      "running time 1386.374239\n",
      "Train_EnvstepsSoFar : 279001\n",
      "Train_AverageReturn : 99.65090133388772\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1386.3742389678955\n",
      "Training Loss : 0.5425695776939392\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 280001\n",
      "mean reward (100 episodes) 100.458521\n",
      "best mean reward 111.434224\n",
      "running time 1390.523384\n",
      "Train_EnvstepsSoFar : 280001\n",
      "Train_AverageReturn : 100.45852116198459\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1390.5233836174011\n",
      "Training Loss : 0.09890054166316986\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 281001\n",
      "mean reward (100 episodes) 98.213046\n",
      "best mean reward 111.434224\n",
      "running time 1394.253769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 281001\n",
      "Train_AverageReturn : 98.21304623164588\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1394.2537689208984\n",
      "Training Loss : 0.4301981031894684\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 282001\n",
      "mean reward (100 episodes) 98.115374\n",
      "best mean reward 111.434224\n",
      "running time 1398.831038\n",
      "Train_EnvstepsSoFar : 282001\n",
      "Train_AverageReturn : 98.11537440352237\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1398.8310377597809\n",
      "Training Loss : 0.04977509751915932\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 283001\n",
      "mean reward (100 episodes) 95.838155\n",
      "best mean reward 111.434224\n",
      "running time 1403.351485\n",
      "Train_EnvstepsSoFar : 283001\n",
      "Train_AverageReturn : 95.83815534826698\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1403.3514847755432\n",
      "Training Loss : 2.5616588592529297\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 284001\n",
      "mean reward (100 episodes) 93.107762\n",
      "best mean reward 111.434224\n",
      "running time 1407.155368\n",
      "Train_EnvstepsSoFar : 284001\n",
      "Train_AverageReturn : 93.10776156118749\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1407.1553678512573\n",
      "Training Loss : 7.194842338562012\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 285001\n",
      "mean reward (100 episodes) 89.503707\n",
      "best mean reward 111.434224\n",
      "running time 1410.848752\n",
      "Train_EnvstepsSoFar : 285001\n",
      "Train_AverageReturn : 89.50370652426989\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1410.848751783371\n",
      "Training Loss : 1.0207316875457764\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 286001\n",
      "mean reward (100 episodes) 82.172210\n",
      "best mean reward 111.434224\n",
      "running time 1414.663196\n",
      "Train_EnvstepsSoFar : 286001\n",
      "Train_AverageReturn : 82.17220982755572\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1414.663195848465\n",
      "Training Loss : 0.4980429708957672\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 287001\n",
      "mean reward (100 episodes) 82.870875\n",
      "best mean reward 111.434224\n",
      "running time 1418.507198\n",
      "Train_EnvstepsSoFar : 287001\n",
      "Train_AverageReturn : 82.87087450841611\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1418.507197856903\n",
      "Training Loss : 0.19474510848522186\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 288001\n",
      "mean reward (100 episodes) 76.720383\n",
      "best mean reward 111.434224\n",
      "running time 1422.684819\n",
      "Train_EnvstepsSoFar : 288001\n",
      "Train_AverageReturn : 76.7203827981239\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1422.6848187446594\n",
      "Training Loss : 0.11025051772594452\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 289001\n",
      "mean reward (100 episodes) 84.587305\n",
      "best mean reward 111.434224\n",
      "running time 1427.057403\n",
      "Train_EnvstepsSoFar : 289001\n",
      "Train_AverageReturn : 84.58730528301393\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1427.0574028491974\n",
      "Training Loss : 0.19099761545658112\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 290001\n",
      "mean reward (100 episodes) 86.634098\n",
      "best mean reward 111.434224\n",
      "running time 1430.936609\n",
      "Train_EnvstepsSoFar : 290001\n",
      "Train_AverageReturn : 86.63409849478968\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1430.93660902977\n",
      "Training Loss : 1.3852406740188599\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 291001\n",
      "mean reward (100 episodes) 78.764220\n",
      "best mean reward 111.434224\n",
      "running time 1434.667775\n",
      "Train_EnvstepsSoFar : 291001\n",
      "Train_AverageReturn : 78.76422022691482\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1434.6677749156952\n",
      "Training Loss : 0.6219061613082886\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 292001\n",
      "mean reward (100 episodes) 79.631813\n",
      "best mean reward 111.434224\n",
      "running time 1438.696889\n",
      "Train_EnvstepsSoFar : 292001\n",
      "Train_AverageReturn : 79.63181327824887\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1438.6968886852264\n",
      "Training Loss : 1.161041498184204\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 293001\n",
      "mean reward (100 episodes) 77.359627\n",
      "best mean reward 111.434224\n",
      "running time 1442.509742\n",
      "Train_EnvstepsSoFar : 293001\n",
      "Train_AverageReturn : 77.35962661409383\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1442.509741783142\n",
      "Training Loss : 0.14113615453243256\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 294001\n",
      "mean reward (100 episodes) 83.712843\n",
      "best mean reward 111.434224\n",
      "running time 1446.261789\n",
      "Train_EnvstepsSoFar : 294001\n",
      "Train_AverageReturn : 83.71284312357194\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1446.2617888450623\n",
      "Training Loss : 0.27219635248184204\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 295001\n",
      "mean reward (100 episodes) 89.580502\n",
      "best mean reward 111.434224\n",
      "running time 1449.945801\n",
      "Train_EnvstepsSoFar : 295001\n",
      "Train_AverageReturn : 89.58050184175039\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1449.94580078125\n",
      "Training Loss : 0.13058844208717346\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 296001\n",
      "mean reward (100 episodes) 83.496456\n",
      "best mean reward 111.434224\n",
      "running time 1453.891550\n",
      "Train_EnvstepsSoFar : 296001\n",
      "Train_AverageReturn : 83.49645646163323\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1453.8915498256683\n",
      "Training Loss : 1.4614198207855225\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 297001\n",
      "mean reward (100 episodes) 81.047978\n",
      "best mean reward 111.434224\n",
      "running time 1457.667503\n",
      "Train_EnvstepsSoFar : 297001\n",
      "Train_AverageReturn : 81.04797845640682\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1457.6675026416779\n",
      "Training Loss : 1.919327974319458\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 298001\n",
      "mean reward (100 episodes) 81.370096\n",
      "best mean reward 111.434224\n",
      "running time 1461.443799\n",
      "Train_EnvstepsSoFar : 298001\n",
      "Train_AverageReturn : 81.37009645752796\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1461.4437987804413\n",
      "Training Loss : 2.3581817150115967\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 299001\n",
      "mean reward (100 episodes) 76.093886\n",
      "best mean reward 111.434224\n",
      "running time 1465.696924\n",
      "Train_EnvstepsSoFar : 299001\n",
      "Train_AverageReturn : 76.09388558568325\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1465.6969237327576\n",
      "Training Loss : 0.167689248919487\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 300001\n",
      "mean reward (100 episodes) 69.233179\n",
      "best mean reward 111.434224\n",
      "running time 1469.604124\n",
      "Train_EnvstepsSoFar : 300001\n",
      "Train_AverageReturn : 69.23317886813375\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1469.6041238307953\n",
      "Training Loss : 0.3318122923374176\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 301001\n",
      "mean reward (100 episodes) 67.972305\n",
      "best mean reward 111.434224\n",
      "running time 1473.872146\n",
      "Train_EnvstepsSoFar : 301001\n",
      "Train_AverageReturn : 67.97230474249028\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1473.8721458911896\n",
      "Training Loss : 4.587485313415527\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 302001\n",
      "mean reward (100 episodes) 66.257883\n",
      "best mean reward 111.434224\n",
      "running time 1477.668658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 302001\n",
      "Train_AverageReturn : 66.25788343540682\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1477.6686577796936\n",
      "Training Loss : 0.1646890640258789\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 303001\n",
      "mean reward (100 episodes) 72.096381\n",
      "best mean reward 111.434224\n",
      "running time 1481.427359\n",
      "Train_EnvstepsSoFar : 303001\n",
      "Train_AverageReturn : 72.09638082603392\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1481.427358865738\n",
      "Training Loss : 0.14765644073486328\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 304001\n",
      "mean reward (100 episodes) 78.520576\n",
      "best mean reward 111.434224\n",
      "running time 1485.230649\n",
      "Train_EnvstepsSoFar : 304001\n",
      "Train_AverageReturn : 78.52057607296659\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1485.2306487560272\n",
      "Training Loss : 0.6232301592826843\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 305001\n",
      "mean reward (100 episodes) 80.773832\n",
      "best mean reward 111.434224\n",
      "running time 1488.981350\n",
      "Train_EnvstepsSoFar : 305001\n",
      "Train_AverageReturn : 80.77383203164362\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1488.9813497066498\n",
      "Training Loss : 5.231637001037598\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 306001\n",
      "mean reward (100 episodes) 80.365062\n",
      "best mean reward 111.434224\n",
      "running time 1493.534503\n",
      "Train_EnvstepsSoFar : 306001\n",
      "Train_AverageReturn : 80.36506208334703\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1493.5345027446747\n",
      "Training Loss : 1.9156043529510498\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 307001\n",
      "mean reward (100 episodes) 77.392637\n",
      "best mean reward 111.434224\n",
      "running time 1497.224897\n",
      "Train_EnvstepsSoFar : 307001\n",
      "Train_AverageReturn : 77.39263747091896\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1497.2248969078064\n",
      "Training Loss : 0.4059673845767975\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 308001\n",
      "mean reward (100 episodes) 73.515793\n",
      "best mean reward 111.434224\n",
      "running time 1501.701466\n",
      "Train_EnvstepsSoFar : 308001\n",
      "Train_AverageReturn : 73.51579330786711\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1501.701465845108\n",
      "Training Loss : 0.30967625975608826\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 309001\n",
      "mean reward (100 episodes) 73.636902\n",
      "best mean reward 111.434224\n",
      "running time 1506.180864\n",
      "Train_EnvstepsSoFar : 309001\n",
      "Train_AverageReturn : 73.63690206640491\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1506.1808638572693\n",
      "Training Loss : 4.994583606719971\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 310001\n",
      "mean reward (100 episodes) 70.885196\n",
      "best mean reward 111.434224\n",
      "running time 1512.257880\n",
      "Train_EnvstepsSoFar : 310001\n",
      "Train_AverageReturn : 70.88519646112789\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1512.2578797340393\n",
      "Training Loss : 0.3098652958869934\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 311001\n",
      "mean reward (100 episodes) 75.781300\n",
      "best mean reward 111.434224\n",
      "running time 1516.306397\n",
      "Train_EnvstepsSoFar : 311001\n",
      "Train_AverageReturn : 75.78129957421059\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1516.3063967227936\n",
      "Training Loss : 0.17157286405563354\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 312001\n",
      "mean reward (100 episodes) 77.221995\n",
      "best mean reward 111.434224\n",
      "running time 1520.538985\n",
      "Train_EnvstepsSoFar : 312001\n",
      "Train_AverageReturn : 77.22199539925482\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1520.5389847755432\n",
      "Training Loss : 0.3496200740337372\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 313001\n",
      "mean reward (100 episodes) 78.356546\n",
      "best mean reward 111.434224\n",
      "running time 1524.415468\n",
      "Train_EnvstepsSoFar : 313001\n",
      "Train_AverageReturn : 78.35654577693187\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1524.4154679775238\n",
      "Training Loss : 1.150814414024353\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 314001\n",
      "mean reward (100 episodes) 74.023561\n",
      "best mean reward 111.434224\n",
      "running time 1529.410912\n",
      "Train_EnvstepsSoFar : 314001\n",
      "Train_AverageReturn : 74.02356069365865\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1529.4109117984772\n",
      "Training Loss : 0.13731886446475983\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 315001\n",
      "mean reward (100 episodes) 72.308805\n",
      "best mean reward 111.434224\n",
      "running time 1534.117410\n",
      "Train_EnvstepsSoFar : 315001\n",
      "Train_AverageReturn : 72.3088052890401\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1534.1174097061157\n",
      "Training Loss : 0.32423949241638184\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 316001\n",
      "mean reward (100 episodes) 73.956297\n",
      "best mean reward 111.434224\n",
      "running time 1539.432799\n",
      "Train_EnvstepsSoFar : 316001\n",
      "Train_AverageReturn : 73.95629710530412\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1539.4327988624573\n",
      "Training Loss : 0.1610168218612671\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 317001\n",
      "mean reward (100 episodes) 64.949139\n",
      "best mean reward 111.434224\n",
      "running time 1544.089249\n",
      "Train_EnvstepsSoFar : 317001\n",
      "Train_AverageReturn : 64.94913936617576\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1544.0892488956451\n",
      "Training Loss : 0.08163788169622421\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 318001\n",
      "mean reward (100 episodes) 69.451381\n",
      "best mean reward 111.434224\n",
      "running time 1548.997128\n",
      "Train_EnvstepsSoFar : 318001\n",
      "Train_AverageReturn : 69.45138137833548\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1548.9971277713776\n",
      "Training Loss : 0.11392495781183243\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 319001\n",
      "mean reward (100 episodes) 77.503473\n",
      "best mean reward 111.434224\n",
      "running time 1554.271299\n",
      "Train_EnvstepsSoFar : 319001\n",
      "Train_AverageReturn : 77.50347329003016\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1554.2712986469269\n",
      "Training Loss : 0.12630586326122284\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 320001\n",
      "mean reward (100 episodes) 79.340629\n",
      "best mean reward 111.434224\n",
      "running time 1559.632536\n",
      "Train_EnvstepsSoFar : 320001\n",
      "Train_AverageReturn : 79.34062890395435\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1559.6325356960297\n",
      "Training Loss : 0.488533079624176\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 321001\n",
      "mean reward (100 episodes) 79.437125\n",
      "best mean reward 111.434224\n",
      "running time 1566.770152\n",
      "Train_EnvstepsSoFar : 321001\n",
      "Train_AverageReturn : 79.43712477302333\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1566.7701518535614\n",
      "Training Loss : 0.6198825836181641\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 322001\n",
      "mean reward (100 episodes) 78.802505\n",
      "best mean reward 111.434224\n",
      "running time 1571.648110\n",
      "Train_EnvstepsSoFar : 322001\n",
      "Train_AverageReturn : 78.80250526183501\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1571.6481096744537\n",
      "Training Loss : 0.16065271198749542\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 323001\n",
      "mean reward (100 episodes) 81.361636\n",
      "best mean reward 111.434224\n",
      "running time 1577.495739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 323001\n",
      "Train_AverageReturn : 81.36163598072828\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1577.4957387447357\n",
      "Training Loss : 0.5112910866737366\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 324001\n",
      "mean reward (100 episodes) 85.289433\n",
      "best mean reward 111.434224\n",
      "running time 1584.111949\n",
      "Train_EnvstepsSoFar : 324001\n",
      "Train_AverageReturn : 85.28943327155476\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1584.1119487285614\n",
      "Training Loss : 2.316347360610962\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 325001\n",
      "mean reward (100 episodes) 83.123821\n",
      "best mean reward 111.434224\n",
      "running time 1595.092228\n",
      "Train_EnvstepsSoFar : 325001\n",
      "Train_AverageReturn : 83.1238212953312\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1595.0922276973724\n",
      "Training Loss : 0.21583746373653412\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 326001\n",
      "mean reward (100 episodes) 81.786815\n",
      "best mean reward 111.434224\n",
      "running time 1601.959269\n",
      "Train_EnvstepsSoFar : 326001\n",
      "Train_AverageReturn : 81.78681510991859\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1601.9592688083649\n",
      "Training Loss : 0.08865796029567719\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 327001\n",
      "mean reward (100 episodes) 81.516135\n",
      "best mean reward 111.434224\n",
      "running time 1606.595917\n",
      "Train_EnvstepsSoFar : 327001\n",
      "Train_AverageReturn : 81.51613471189285\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1606.5959167480469\n",
      "Training Loss : 0.4587329626083374\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 328001\n",
      "mean reward (100 episodes) 82.649410\n",
      "best mean reward 111.434224\n",
      "running time 1611.368302\n",
      "Train_EnvstepsSoFar : 328001\n",
      "Train_AverageReturn : 82.64940973750964\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1611.3683016300201\n",
      "Training Loss : 4.261863708496094\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 329001\n",
      "mean reward (100 episodes) 79.370949\n",
      "best mean reward 111.434224\n",
      "running time 1616.479789\n",
      "Train_EnvstepsSoFar : 329001\n",
      "Train_AverageReturn : 79.37094898814148\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1616.4797887802124\n",
      "Training Loss : 0.08281596004962921\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 330001\n",
      "mean reward (100 episodes) 78.703837\n",
      "best mean reward 111.434224\n",
      "running time 1623.188687\n",
      "Train_EnvstepsSoFar : 330001\n",
      "Train_AverageReturn : 78.70383721960684\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1623.1886868476868\n",
      "Training Loss : 0.21575908362865448\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 331001\n",
      "mean reward (100 episodes) 78.719269\n",
      "best mean reward 111.434224\n",
      "running time 1634.872891\n",
      "Train_EnvstepsSoFar : 331001\n",
      "Train_AverageReturn : 78.71926899292639\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1634.8728907108307\n",
      "Training Loss : 0.12502898275852203\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 332001\n",
      "mean reward (100 episodes) 76.754020\n",
      "best mean reward 111.434224\n",
      "running time 1641.584676\n",
      "Train_EnvstepsSoFar : 332001\n",
      "Train_AverageReturn : 76.75401992989131\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1641.5846757888794\n",
      "Training Loss : 0.5035539269447327\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 333001\n",
      "mean reward (100 episodes) 76.880372\n",
      "best mean reward 111.434224\n",
      "running time 1647.928774\n",
      "Train_EnvstepsSoFar : 333001\n",
      "Train_AverageReturn : 76.8803722018639\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1647.9287738800049\n",
      "Training Loss : 0.07531139999628067\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 334001\n",
      "mean reward (100 episodes) 77.541592\n",
      "best mean reward 111.434224\n",
      "running time 1652.859349\n",
      "Train_EnvstepsSoFar : 334001\n",
      "Train_AverageReturn : 77.54159187330721\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1652.8593490123749\n",
      "Training Loss : 0.6613742113113403\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 335001\n",
      "mean reward (100 episodes) 79.873134\n",
      "best mean reward 111.434224\n",
      "running time 1658.135716\n",
      "Train_EnvstepsSoFar : 335001\n",
      "Train_AverageReturn : 79.87313350990674\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1658.1357157230377\n",
      "Training Loss : 0.1851685792207718\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 336001\n",
      "mean reward (100 episodes) 80.121332\n",
      "best mean reward 111.434224\n",
      "running time 1663.844237\n",
      "Train_EnvstepsSoFar : 336001\n",
      "Train_AverageReturn : 80.12133199092914\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1663.84423661232\n",
      "Training Loss : 0.1869736611843109\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 337001\n",
      "mean reward (100 episodes) 77.389764\n",
      "best mean reward 111.434224\n",
      "running time 1670.156328\n",
      "Train_EnvstepsSoFar : 337001\n",
      "Train_AverageReturn : 77.389764287959\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1670.1563277244568\n",
      "Training Loss : 0.08146591484546661\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 338001\n",
      "mean reward (100 episodes) 76.604180\n",
      "best mean reward 111.434224\n",
      "running time 1675.062373\n",
      "Train_EnvstepsSoFar : 338001\n",
      "Train_AverageReturn : 76.60418034287227\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1675.0623726844788\n",
      "Training Loss : 0.14517073333263397\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 339001\n",
      "mean reward (100 episodes) 75.056955\n",
      "best mean reward 111.434224\n",
      "running time 1680.813336\n",
      "Train_EnvstepsSoFar : 339001\n",
      "Train_AverageReturn : 75.05695458387217\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1680.8133356571198\n",
      "Training Loss : 0.10723476111888885\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 340001\n",
      "mean reward (100 episodes) 75.193690\n",
      "best mean reward 111.434224\n",
      "running time 1686.054117\n",
      "Train_EnvstepsSoFar : 340001\n",
      "Train_AverageReturn : 75.19369040182654\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1686.0541167259216\n",
      "Training Loss : 0.9327356219291687\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 341001\n",
      "mean reward (100 episodes) 79.819889\n",
      "best mean reward 111.434224\n",
      "running time 1691.263696\n",
      "Train_EnvstepsSoFar : 341001\n",
      "Train_AverageReturn : 79.81988928127522\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1691.263695716858\n",
      "Training Loss : 3.8883185386657715\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 342001\n",
      "mean reward (100 episodes) 78.491409\n",
      "best mean reward 111.434224\n",
      "running time 1696.641012\n",
      "Train_EnvstepsSoFar : 342001\n",
      "Train_AverageReturn : 78.49140883910106\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1696.6410119533539\n",
      "Training Loss : 0.08288856595754623\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 343001\n",
      "mean reward (100 episodes) 80.796093\n",
      "best mean reward 111.434224\n",
      "running time 1702.292373\n",
      "Train_EnvstepsSoFar : 343001\n",
      "Train_AverageReturn : 80.79609262243874\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1702.2923729419708\n",
      "Training Loss : 0.2953492999076843\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 344001\n",
      "mean reward (100 episodes) 80.039178\n",
      "best mean reward 111.434224\n",
      "running time 1706.958199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 344001\n",
      "Train_AverageReturn : 80.03917779669517\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1706.9581990242004\n",
      "Training Loss : 0.44380971789360046\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 345001\n",
      "mean reward (100 episodes) 77.978689\n",
      "best mean reward 111.434224\n",
      "running time 1712.299788\n",
      "Train_EnvstepsSoFar : 345001\n",
      "Train_AverageReturn : 77.97868928657584\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1712.2997879981995\n",
      "Training Loss : 0.13271623849868774\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 346001\n",
      "mean reward (100 episodes) 78.895388\n",
      "best mean reward 111.434224\n",
      "running time 1717.729288\n",
      "Train_EnvstepsSoFar : 346001\n",
      "Train_AverageReturn : 78.89538834733906\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1717.7292878627777\n",
      "Training Loss : 0.49396106600761414\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 347001\n",
      "mean reward (100 episodes) 82.282625\n",
      "best mean reward 111.434224\n",
      "running time 1722.978518\n",
      "Train_EnvstepsSoFar : 347001\n",
      "Train_AverageReturn : 82.2826249765761\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1722.9785180091858\n",
      "Training Loss : 0.11974141746759415\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 348001\n",
      "mean reward (100 episodes) 77.291432\n",
      "best mean reward 111.434224\n",
      "running time 1727.376269\n",
      "Train_EnvstepsSoFar : 348001\n",
      "Train_AverageReturn : 77.29143186958163\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1727.3762686252594\n",
      "Training Loss : 0.7980032563209534\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 349001\n",
      "mean reward (100 episodes) 72.916080\n",
      "best mean reward 111.434224\n",
      "running time 1733.839520\n",
      "Train_EnvstepsSoFar : 349001\n",
      "Train_AverageReturn : 72.91608044619444\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1733.8395199775696\n",
      "Training Loss : 0.2896292805671692\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 350001\n",
      "mean reward (100 episodes) 80.157003\n",
      "best mean reward 111.434224\n",
      "running time 1738.559096\n",
      "Train_EnvstepsSoFar : 350001\n",
      "Train_AverageReturn : 80.15700301081512\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1738.559095621109\n",
      "Training Loss : 0.2277035415172577\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 351001\n",
      "mean reward (100 episodes) 80.278858\n",
      "best mean reward 111.434224\n",
      "running time 1745.133960\n",
      "Train_EnvstepsSoFar : 351001\n",
      "Train_AverageReturn : 80.27885808291043\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1745.1339597702026\n",
      "Training Loss : 0.1919555813074112\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 352001\n",
      "mean reward (100 episodes) 80.854962\n",
      "best mean reward 111.434224\n",
      "running time 1751.148831\n",
      "Train_EnvstepsSoFar : 352001\n",
      "Train_AverageReturn : 80.85496157620221\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1751.1488308906555\n",
      "Training Loss : 0.1448555886745453\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 353001\n",
      "mean reward (100 episodes) 80.233209\n",
      "best mean reward 111.434224\n",
      "running time 1757.207330\n",
      "Train_EnvstepsSoFar : 353001\n",
      "Train_AverageReturn : 80.23320892797953\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1757.207329750061\n",
      "Training Loss : 0.09818891435861588\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 354001\n",
      "mean reward (100 episodes) 77.830061\n",
      "best mean reward 111.434224\n",
      "running time 1762.112405\n",
      "Train_EnvstepsSoFar : 354001\n",
      "Train_AverageReturn : 77.83006120620274\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1762.1124048233032\n",
      "Training Loss : 0.1659252941608429\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 355001\n",
      "mean reward (100 episodes) 76.532812\n",
      "best mean reward 111.434224\n",
      "running time 1767.235660\n",
      "Train_EnvstepsSoFar : 355001\n",
      "Train_AverageReturn : 76.53281208726568\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1767.2356598377228\n",
      "Training Loss : 0.14040344953536987\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 356001\n",
      "mean reward (100 episodes) 78.630943\n",
      "best mean reward 111.434224\n",
      "running time 1771.748664\n",
      "Train_EnvstepsSoFar : 356001\n",
      "Train_AverageReturn : 78.63094279321386\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1771.7486636638641\n",
      "Training Loss : 0.16088742017745972\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 357001\n",
      "mean reward (100 episodes) 74.435066\n",
      "best mean reward 111.434224\n",
      "running time 1776.351218\n",
      "Train_EnvstepsSoFar : 357001\n",
      "Train_AverageReturn : 74.43506569031487\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1776.3512177467346\n",
      "Training Loss : 0.08148791640996933\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 358001\n",
      "mean reward (100 episodes) 74.533772\n",
      "best mean reward 111.434224\n",
      "running time 1781.738156\n",
      "Train_EnvstepsSoFar : 358001\n",
      "Train_AverageReturn : 74.53377218352665\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1781.7381558418274\n",
      "Training Loss : 0.2173188477754593\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 359001\n",
      "mean reward (100 episodes) 74.794762\n",
      "best mean reward 111.434224\n",
      "running time 1787.384354\n",
      "Train_EnvstepsSoFar : 359001\n",
      "Train_AverageReturn : 74.7947623517717\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1787.3843536376953\n",
      "Training Loss : 0.1396905779838562\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 360001\n",
      "mean reward (100 episodes) 75.215597\n",
      "best mean reward 111.434224\n",
      "running time 1792.143415\n",
      "Train_EnvstepsSoFar : 360001\n",
      "Train_AverageReturn : 75.21559740041194\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1792.1434149742126\n",
      "Training Loss : 0.11812086403369904\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 361001\n",
      "mean reward (100 episodes) 76.308714\n",
      "best mean reward 111.434224\n",
      "running time 1797.236180\n",
      "Train_EnvstepsSoFar : 361001\n",
      "Train_AverageReturn : 76.30871413690026\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1797.2361798286438\n",
      "Training Loss : 0.11362099647521973\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 362001\n",
      "mean reward (100 episodes) 75.866620\n",
      "best mean reward 111.434224\n",
      "running time 1801.345696\n",
      "Train_EnvstepsSoFar : 362001\n",
      "Train_AverageReturn : 75.86662028705324\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1801.3456959724426\n",
      "Training Loss : 0.1467566043138504\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 363001\n",
      "mean reward (100 episodes) 79.088430\n",
      "best mean reward 111.434224\n",
      "running time 1805.844112\n",
      "Train_EnvstepsSoFar : 363001\n",
      "Train_AverageReturn : 79.088430373546\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1805.8441116809845\n",
      "Training Loss : 0.06160704419016838\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 364001\n",
      "mean reward (100 episodes) 85.059679\n",
      "best mean reward 111.434224\n",
      "running time 1810.072669\n",
      "Train_EnvstepsSoFar : 364001\n",
      "Train_AverageReturn : 85.05967853595054\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1810.0726687908173\n",
      "Training Loss : 0.6367166042327881\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 365001\n",
      "mean reward (100 episodes) 79.094999\n",
      "best mean reward 111.434224\n",
      "running time 1813.685349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 365001\n",
      "Train_AverageReturn : 79.09499866636916\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1813.6853487491608\n",
      "Training Loss : 0.26603585481643677\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 366001\n",
      "mean reward (100 episodes) 78.809102\n",
      "best mean reward 111.434224\n",
      "running time 1820.726495\n",
      "Train_EnvstepsSoFar : 366001\n",
      "Train_AverageReturn : 78.80910180035778\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1820.7264947891235\n",
      "Training Loss : 0.10514353960752487\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 367001\n",
      "mean reward (100 episodes) 82.094490\n",
      "best mean reward 111.434224\n",
      "running time 1824.582403\n",
      "Train_EnvstepsSoFar : 367001\n",
      "Train_AverageReturn : 82.09449039687364\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1824.5824027061462\n",
      "Training Loss : 0.21501457691192627\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 368001\n",
      "mean reward (100 episodes) 85.202741\n",
      "best mean reward 111.434224\n",
      "running time 1828.451837\n",
      "Train_EnvstepsSoFar : 368001\n",
      "Train_AverageReturn : 85.2027411331946\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1828.4518368244171\n",
      "Training Loss : 0.08895549178123474\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 369001\n",
      "mean reward (100 episodes) 82.464832\n",
      "best mean reward 111.434224\n",
      "running time 1832.784003\n",
      "Train_EnvstepsSoFar : 369001\n",
      "Train_AverageReturn : 82.46483244413501\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1832.7840027809143\n",
      "Training Loss : 0.48557913303375244\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 370001\n",
      "mean reward (100 episodes) 80.974442\n",
      "best mean reward 111.434224\n",
      "running time 1837.231418\n",
      "Train_EnvstepsSoFar : 370001\n",
      "Train_AverageReturn : 80.97444230571814\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1837.2314176559448\n",
      "Training Loss : 0.09428712725639343\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 371001\n",
      "mean reward (100 episodes) 78.400591\n",
      "best mean reward 111.434224\n",
      "running time 1843.289985\n",
      "Train_EnvstepsSoFar : 371001\n",
      "Train_AverageReturn : 78.40059147997704\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1843.2899849414825\n",
      "Training Loss : 0.07758291065692902\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 372001\n",
      "mean reward (100 episodes) 78.375237\n",
      "best mean reward 111.434224\n",
      "running time 1849.047091\n",
      "Train_EnvstepsSoFar : 372001\n",
      "Train_AverageReturn : 78.37523714673063\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1849.047090768814\n",
      "Training Loss : 0.16179907321929932\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 373001\n",
      "mean reward (100 episodes) 80.208151\n",
      "best mean reward 111.434224\n",
      "running time 1853.463640\n",
      "Train_EnvstepsSoFar : 373001\n",
      "Train_AverageReturn : 80.20815095877558\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1853.4636397361755\n",
      "Training Loss : 0.12695899605751038\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 374001\n",
      "mean reward (100 episodes) 83.661846\n",
      "best mean reward 111.434224\n",
      "running time 1858.342388\n",
      "Train_EnvstepsSoFar : 374001\n",
      "Train_AverageReturn : 83.66184579616201\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1858.3423879146576\n",
      "Training Loss : 0.11900153756141663\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 375001\n",
      "mean reward (100 episodes) 83.811044\n",
      "best mean reward 111.434224\n",
      "running time 1863.968301\n",
      "Train_EnvstepsSoFar : 375001\n",
      "Train_AverageReturn : 83.81104384093462\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1863.968300819397\n",
      "Training Loss : 0.09174052625894547\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 376001\n",
      "mean reward (100 episodes) 81.685867\n",
      "best mean reward 111.434224\n",
      "running time 1869.217692\n",
      "Train_EnvstepsSoFar : 376001\n",
      "Train_AverageReturn : 81.68586725356136\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1869.217691898346\n",
      "Training Loss : 0.17467881739139557\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 377001\n",
      "mean reward (100 episodes) 81.185664\n",
      "best mean reward 111.434224\n",
      "running time 1872.951888\n",
      "Train_EnvstepsSoFar : 377001\n",
      "Train_AverageReturn : 81.1856635474594\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1872.951887845993\n",
      "Training Loss : 0.08287815004587173\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 378001\n",
      "mean reward (100 episodes) 78.938435\n",
      "best mean reward 111.434224\n",
      "running time 1878.454563\n",
      "Train_EnvstepsSoFar : 378001\n",
      "Train_AverageReturn : 78.9384345338548\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1878.454562664032\n",
      "Training Loss : 0.871081531047821\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 379001\n",
      "mean reward (100 episodes) 73.237412\n",
      "best mean reward 111.434224\n",
      "running time 1882.094883\n",
      "Train_EnvstepsSoFar : 379001\n",
      "Train_AverageReturn : 73.2374116274546\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1882.0948827266693\n",
      "Training Loss : 2.225858211517334\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 380001\n",
      "mean reward (100 episodes) 70.091958\n",
      "best mean reward 111.434224\n",
      "running time 1887.840202\n",
      "Train_EnvstepsSoFar : 380001\n",
      "Train_AverageReturn : 70.09195829542777\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1887.8402018547058\n",
      "Training Loss : 0.10302714258432388\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 381001\n",
      "mean reward (100 episodes) 69.922297\n",
      "best mean reward 111.434224\n",
      "running time 1892.555709\n",
      "Train_EnvstepsSoFar : 381001\n",
      "Train_AverageReturn : 69.92229740575627\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1892.5557088851929\n",
      "Training Loss : 0.5211249589920044\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 382001\n",
      "mean reward (100 episodes) 70.527708\n",
      "best mean reward 111.434224\n",
      "running time 1897.394032\n",
      "Train_EnvstepsSoFar : 382001\n",
      "Train_AverageReturn : 70.52770833859165\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1897.3940317630768\n",
      "Training Loss : 0.13610249757766724\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 383001\n",
      "mean reward (100 episodes) 70.810844\n",
      "best mean reward 111.434224\n",
      "running time 1904.153879\n",
      "Train_EnvstepsSoFar : 383001\n",
      "Train_AverageReturn : 70.81084354784713\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1904.1538786888123\n",
      "Training Loss : 0.8965893983840942\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 384001\n",
      "mean reward (100 episodes) 70.180798\n",
      "best mean reward 111.434224\n",
      "running time 1908.665304\n",
      "Train_EnvstepsSoFar : 384001\n",
      "Train_AverageReturn : 70.1807982044598\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1908.6653037071228\n",
      "Training Loss : 0.06607396900653839\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 385001\n",
      "mean reward (100 episodes) 72.479903\n",
      "best mean reward 111.434224\n",
      "running time 1912.339694\n",
      "Train_EnvstepsSoFar : 385001\n",
      "Train_AverageReturn : 72.47990328891616\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1912.3396937847137\n",
      "Training Loss : 0.2182300090789795\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 386001\n",
      "mean reward (100 episodes) 72.821264\n",
      "best mean reward 111.434224\n",
      "running time 1916.573808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 386001\n",
      "Train_AverageReturn : 72.8212636268294\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1916.5738077163696\n",
      "Training Loss : 0.15756593644618988\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 387001\n",
      "mean reward (100 episodes) 73.700423\n",
      "best mean reward 111.434224\n",
      "running time 1920.509021\n",
      "Train_EnvstepsSoFar : 387001\n",
      "Train_AverageReturn : 73.70042320303729\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1920.509020805359\n",
      "Training Loss : 0.13601134717464447\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 388001\n",
      "mean reward (100 episodes) 72.818299\n",
      "best mean reward 111.434224\n",
      "running time 1924.430449\n",
      "Train_EnvstepsSoFar : 388001\n",
      "Train_AverageReturn : 72.81829933177663\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1924.430448770523\n",
      "Training Loss : 0.06638436019420624\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 389001\n",
      "mean reward (100 episodes) 73.435458\n",
      "best mean reward 111.434224\n",
      "running time 1928.389759\n",
      "Train_EnvstepsSoFar : 389001\n",
      "Train_AverageReturn : 73.43545759638096\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1928.3897588253021\n",
      "Training Loss : 0.6082730293273926\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 390001\n",
      "mean reward (100 episodes) 71.252821\n",
      "best mean reward 111.434224\n",
      "running time 1933.586785\n",
      "Train_EnvstepsSoFar : 390001\n",
      "Train_AverageReturn : 71.25282097415797\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1933.5867848396301\n",
      "Training Loss : 0.1698186993598938\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 391001\n",
      "mean reward (100 episodes) 69.821196\n",
      "best mean reward 111.434224\n",
      "running time 1939.053546\n",
      "Train_EnvstepsSoFar : 391001\n",
      "Train_AverageReturn : 69.82119645058403\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1939.0535459518433\n",
      "Training Loss : 0.283625990152359\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 392001\n",
      "mean reward (100 episodes) 70.993626\n",
      "best mean reward 111.434224\n",
      "running time 1943.014252\n",
      "Train_EnvstepsSoFar : 392001\n",
      "Train_AverageReturn : 70.99362633354957\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1943.014251947403\n",
      "Training Loss : 0.08136475086212158\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 393001\n",
      "mean reward (100 episodes) 65.149090\n",
      "best mean reward 111.434224\n",
      "running time 1947.599055\n",
      "Train_EnvstepsSoFar : 393001\n",
      "Train_AverageReturn : 65.14908952300105\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1947.599054813385\n",
      "Training Loss : 0.6799585819244385\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 394001\n",
      "mean reward (100 episodes) 68.949219\n",
      "best mean reward 111.434224\n",
      "running time 1951.553531\n",
      "Train_EnvstepsSoFar : 394001\n",
      "Train_AverageReturn : 68.94921884336043\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1951.5535306930542\n",
      "Training Loss : 0.07975944876670837\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 395001\n",
      "mean reward (100 episodes) 69.687710\n",
      "best mean reward 111.434224\n",
      "running time 1956.234499\n",
      "Train_EnvstepsSoFar : 395001\n",
      "Train_AverageReturn : 69.68771019282859\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1956.2344987392426\n",
      "Training Loss : 0.21191012859344482\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 396001\n",
      "mean reward (100 episodes) 65.616746\n",
      "best mean reward 111.434224\n",
      "running time 1960.210219\n",
      "Train_EnvstepsSoFar : 396001\n",
      "Train_AverageReturn : 65.61674607944589\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1960.2102189064026\n",
      "Training Loss : 0.1376323252916336\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 397001\n",
      "mean reward (100 episodes) 58.510237\n",
      "best mean reward 111.434224\n",
      "running time 1965.073563\n",
      "Train_EnvstepsSoFar : 397001\n",
      "Train_AverageReturn : 58.510237078074105\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1965.073562860489\n",
      "Training Loss : 1.9517035484313965\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 398001\n",
      "mean reward (100 episodes) 59.455684\n",
      "best mean reward 111.434224\n",
      "running time 1968.829161\n",
      "Train_EnvstepsSoFar : 398001\n",
      "Train_AverageReturn : 59.45568428423525\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1968.8291609287262\n",
      "Training Loss : 0.07277975976467133\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 399001\n",
      "mean reward (100 episodes) 59.541005\n",
      "best mean reward 111.434224\n",
      "running time 1972.637032\n",
      "Train_EnvstepsSoFar : 399001\n",
      "Train_AverageReturn : 59.5410046880514\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1972.6370317935944\n",
      "Training Loss : 0.12867867946624756\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 400001\n",
      "mean reward (100 episodes) 60.568199\n",
      "best mean reward 111.434224\n",
      "running time 1978.519420\n",
      "Train_EnvstepsSoFar : 400001\n",
      "Train_AverageReturn : 60.56819925106288\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1978.5194199085236\n",
      "Training Loss : 0.8002333641052246\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 401001\n",
      "mean reward (100 episodes) 64.973070\n",
      "best mean reward 111.434224\n",
      "running time 1982.615835\n",
      "Train_EnvstepsSoFar : 401001\n",
      "Train_AverageReturn : 64.9730701468982\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1982.6158347129822\n",
      "Training Loss : 0.1692562997341156\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 402001\n",
      "mean reward (100 episodes) 62.262620\n",
      "best mean reward 111.434224\n",
      "running time 1987.045249\n",
      "Train_EnvstepsSoFar : 402001\n",
      "Train_AverageReturn : 62.26261964692999\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1987.0452489852905\n",
      "Training Loss : 0.1258082240819931\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 403001\n",
      "mean reward (100 episodes) 62.373809\n",
      "best mean reward 111.434224\n",
      "running time 1992.147877\n",
      "Train_EnvstepsSoFar : 403001\n",
      "Train_AverageReturn : 62.373809070869775\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1992.147876739502\n",
      "Training Loss : 0.1921270191669464\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 404001\n",
      "mean reward (100 episodes) 62.736430\n",
      "best mean reward 111.434224\n",
      "running time 1997.237637\n",
      "Train_EnvstepsSoFar : 404001\n",
      "Train_AverageReturn : 62.73642989404161\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 1997.2376368045807\n",
      "Training Loss : 2.2345240116119385\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 405001\n",
      "mean reward (100 episodes) 65.410352\n",
      "best mean reward 111.434224\n",
      "running time 2002.359831\n",
      "Train_EnvstepsSoFar : 405001\n",
      "Train_AverageReturn : 65.41035218204826\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 2002.3598306179047\n",
      "Training Loss : 0.8802325129508972\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 406001\n",
      "mean reward (100 episodes) 66.155825\n",
      "best mean reward 111.434224\n",
      "running time 2006.061541\n",
      "Train_EnvstepsSoFar : 406001\n",
      "Train_AverageReturn : 66.15582515610218\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 2006.0615408420563\n",
      "Training Loss : 4.310591220855713\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 407001\n",
      "mean reward (100 episodes) 69.168713\n",
      "best mean reward 111.434224\n",
      "running time 2011.827491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 407001\n",
      "Train_AverageReturn : 69.1687133411388\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 2011.8274908065796\n",
      "Training Loss : 0.10528846085071564\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 408001\n",
      "mean reward (100 episodes) 69.187521\n",
      "best mean reward 111.434224\n",
      "running time 2016.606773\n",
      "Train_EnvstepsSoFar : 408001\n",
      "Train_AverageReturn : 69.18752065640848\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 2016.6067728996277\n",
      "Training Loss : 0.26196619868278503\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 409001\n",
      "mean reward (100 episodes) 70.039205\n",
      "best mean reward 111.434224\n",
      "running time 2020.658957\n",
      "Train_EnvstepsSoFar : 409001\n",
      "Train_AverageReturn : 70.03920526344868\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 2020.6589567661285\n",
      "Training Loss : 0.5282951593399048\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 410001\n",
      "mean reward (100 episodes) 70.242707\n",
      "best mean reward 111.434224\n",
      "running time 2025.754958\n",
      "Train_EnvstepsSoFar : 410001\n",
      "Train_AverageReturn : 70.24270653747254\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 2025.7549576759338\n",
      "Training Loss : 0.1931079924106598\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 411001\n",
      "mean reward (100 episodes) 68.123425\n",
      "best mean reward 111.434224\n",
      "running time 2030.564157\n",
      "Train_EnvstepsSoFar : 411001\n",
      "Train_AverageReturn : 68.12342456952763\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 2030.5641567707062\n",
      "Training Loss : 0.12441545724868774\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 412001\n",
      "mean reward (100 episodes) 70.497325\n",
      "best mean reward 111.434224\n",
      "running time 2036.596898\n",
      "Train_EnvstepsSoFar : 412001\n",
      "Train_AverageReturn : 70.49732479538626\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 2036.5968976020813\n",
      "Training Loss : 0.09457401186227798\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 413001\n",
      "mean reward (100 episodes) 72.250228\n",
      "best mean reward 111.434224\n",
      "running time 2040.726187\n",
      "Train_EnvstepsSoFar : 413001\n",
      "Train_AverageReturn : 72.25022782685967\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 2040.726186990738\n",
      "Training Loss : 1.0464144945144653\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 414001\n",
      "mean reward (100 episodes) 72.604856\n",
      "best mean reward 111.434224\n",
      "running time 2045.746270\n",
      "Train_EnvstepsSoFar : 414001\n",
      "Train_AverageReturn : 72.60485550893029\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 2045.74626994133\n",
      "Training Loss : 1.6381443738937378\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 415001\n",
      "mean reward (100 episodes) 71.470048\n",
      "best mean reward 111.434224\n",
      "running time 2050.168226\n",
      "Train_EnvstepsSoFar : 415001\n",
      "Train_AverageReturn : 71.47004769005191\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 2050.1682257652283\n",
      "Training Loss : 0.24111302196979523\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 416001\n",
      "mean reward (100 episodes) 66.845372\n",
      "best mean reward 111.434224\n",
      "running time 2055.682056\n",
      "Train_EnvstepsSoFar : 416001\n",
      "Train_AverageReturn : 66.84537185335043\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 2055.682055950165\n",
      "Training Loss : 0.11960937827825546\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 417001\n",
      "mean reward (100 episodes) 66.607488\n",
      "best mean reward 111.434224\n",
      "running time 2059.410456\n",
      "Train_EnvstepsSoFar : 417001\n",
      "Train_AverageReturn : 66.60748770074447\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 2059.4104557037354\n",
      "Training Loss : 0.16292619705200195\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 418001\n",
      "mean reward (100 episodes) 75.009184\n",
      "best mean reward 111.434224\n",
      "running time 2063.144075\n",
      "Train_EnvstepsSoFar : 418001\n",
      "Train_AverageReturn : 75.00918378031031\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 2063.1440749168396\n",
      "Training Loss : 0.10783297568559647\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 419001\n",
      "mean reward (100 episodes) 74.418969\n",
      "best mean reward 111.434224\n",
      "running time 2066.749426\n",
      "Train_EnvstepsSoFar : 419001\n",
      "Train_AverageReturn : 74.41896911984111\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 2066.7494258880615\n",
      "Training Loss : 0.15926946699619293\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 420001\n",
      "mean reward (100 episodes) 75.049522\n",
      "best mean reward 111.434224\n",
      "running time 2070.421590\n",
      "Train_EnvstepsSoFar : 420001\n",
      "Train_AverageReturn : 75.04952188430826\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 2070.4215898513794\n",
      "Training Loss : 0.10312142968177795\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 421001\n",
      "mean reward (100 episodes) 78.892179\n",
      "best mean reward 111.434224\n",
      "running time 2074.284200\n",
      "Train_EnvstepsSoFar : 421001\n",
      "Train_AverageReturn : 78.89217865054533\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 2074.284199953079\n",
      "Training Loss : 0.18189959228038788\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 422001\n",
      "mean reward (100 episodes) 79.804529\n",
      "best mean reward 111.434224\n",
      "running time 2079.727988\n",
      "Train_EnvstepsSoFar : 422001\n",
      "Train_AverageReturn : 79.80452927608737\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 2079.727987766266\n",
      "Training Loss : 6.317317008972168\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 423001\n",
      "mean reward (100 episodes) 80.036674\n",
      "best mean reward 111.434224\n",
      "running time 2085.036736\n",
      "Train_EnvstepsSoFar : 423001\n",
      "Train_AverageReturn : 80.03667422708541\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 2085.036736011505\n",
      "Training Loss : 0.17740526795387268\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 424001\n",
      "mean reward (100 episodes) 85.682975\n",
      "best mean reward 111.434224\n",
      "running time 2089.270815\n",
      "Train_EnvstepsSoFar : 424001\n",
      "Train_AverageReturn : 85.68297475551219\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 2089.27081489563\n",
      "Training Loss : 0.9158936738967896\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 425001\n",
      "mean reward (100 episodes) 83.828435\n",
      "best mean reward 111.434224\n",
      "running time 2095.550473\n",
      "Train_EnvstepsSoFar : 425001\n",
      "Train_AverageReturn : 83.82843483418377\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 2095.550472974777\n",
      "Training Loss : 0.12522044777870178\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 426001\n",
      "mean reward (100 episodes) 87.796599\n",
      "best mean reward 111.434224\n",
      "running time 2100.350553\n",
      "Train_EnvstepsSoFar : 426001\n",
      "Train_AverageReturn : 87.79659887724138\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 2100.3505527973175\n",
      "Training Loss : 0.14455494284629822\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 427001\n",
      "mean reward (100 episodes) 90.087540\n",
      "best mean reward 111.434224\n",
      "running time 2104.732516\n",
      "Train_EnvstepsSoFar : 427001\n",
      "Train_AverageReturn : 90.08753977729177\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 2104.73251581192\n",
      "Training Loss : 0.07860775291919708\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 428001\n",
      "mean reward (100 episodes) 87.693423\n",
      "best mean reward 111.434224\n",
      "running time 2109.064952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 428001\n",
      "Train_AverageReturn : 87.69342333525545\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 2109.064951658249\n",
      "Training Loss : 0.14004671573638916\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 429001\n",
      "mean reward (100 episodes) 84.776784\n",
      "best mean reward 111.434224\n",
      "running time 2114.478986\n",
      "Train_EnvstepsSoFar : 429001\n",
      "Train_AverageReturn : 84.77678350368772\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 2114.478985786438\n",
      "Training Loss : 0.0845358818769455\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 430001\n",
      "mean reward (100 episodes) 87.272257\n",
      "best mean reward 111.434224\n",
      "running time 2118.739276\n",
      "Train_EnvstepsSoFar : 430001\n",
      "Train_AverageReturn : 87.27225709287\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 2118.739275932312\n",
      "Training Loss : 0.184231698513031\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 431001\n",
      "mean reward (100 episodes) 88.820874\n",
      "best mean reward 111.434224\n",
      "running time 2123.132994\n",
      "Train_EnvstepsSoFar : 431001\n",
      "Train_AverageReturn : 88.82087384636587\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 2123.13299369812\n",
      "Training Loss : 0.08924970775842667\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 432001\n",
      "mean reward (100 episodes) 95.960364\n",
      "best mean reward 111.434224\n",
      "running time 2126.839428\n",
      "Train_EnvstepsSoFar : 432001\n",
      "Train_AverageReturn : 95.96036436906643\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 2126.8394277095795\n",
      "Training Loss : 0.22897124290466309\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 433001\n",
      "mean reward (100 episodes) 98.787773\n",
      "best mean reward 111.434224\n",
      "running time 2131.673520\n",
      "Train_EnvstepsSoFar : 433001\n",
      "Train_AverageReturn : 98.78777282722851\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 2131.673519849777\n",
      "Training Loss : 0.15265822410583496\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 434001\n",
      "mean reward (100 episodes) 103.200839\n",
      "best mean reward 111.434224\n",
      "running time 2136.103693\n",
      "Train_EnvstepsSoFar : 434001\n",
      "Train_AverageReturn : 103.20083922263485\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 2136.1036927700043\n",
      "Training Loss : 0.14092683792114258\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 435001\n",
      "mean reward (100 episodes) 107.419787\n",
      "best mean reward 111.434224\n",
      "running time 2140.467877\n",
      "Train_EnvstepsSoFar : 435001\n",
      "Train_AverageReturn : 107.41978677501487\n",
      "Train_BestReturn : 111.43422388194847\n",
      "TimeSinceStart : 2140.4678766727448\n",
      "Training Loss : 0.21410107612609863\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 436000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 436001\n",
      "mean reward (100 episodes) 117.430899\n",
      "best mean reward 117.430899\n",
      "running time 2145.725836\n",
      "Train_EnvstepsSoFar : 436001\n",
      "Train_AverageReturn : 117.430899109256\n",
      "Train_BestReturn : 117.430899109256\n",
      "TimeSinceStart : 2145.7258360385895\n",
      "Training Loss : 0.22853362560272217\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 437001\n",
      "mean reward (100 episodes) 123.657791\n",
      "best mean reward 123.657791\n",
      "running time 2150.207242\n",
      "Train_EnvstepsSoFar : 437001\n",
      "Train_AverageReturn : 123.65779107862201\n",
      "Train_BestReturn : 123.65779107862201\n",
      "TimeSinceStart : 2150.2072417736053\n",
      "Training Loss : 0.11993830651044846\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 438001\n",
      "mean reward (100 episodes) 124.504082\n",
      "best mean reward 124.504082\n",
      "running time 2154.283116\n",
      "Train_EnvstepsSoFar : 438001\n",
      "Train_AverageReturn : 124.50408204327407\n",
      "Train_BestReturn : 124.50408204327407\n",
      "TimeSinceStart : 2154.2831158638\n",
      "Training Loss : 0.581930935382843\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 439001\n",
      "mean reward (100 episodes) 127.210293\n",
      "best mean reward 127.210293\n",
      "running time 2158.856882\n",
      "Train_EnvstepsSoFar : 439001\n",
      "Train_AverageReturn : 127.21029261263\n",
      "Train_BestReturn : 127.21029261263\n",
      "TimeSinceStart : 2158.8568818569183\n",
      "Training Loss : 0.20628131926059723\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 440001\n",
      "mean reward (100 episodes) 129.683416\n",
      "best mean reward 129.683416\n",
      "running time 2166.638363\n",
      "Train_EnvstepsSoFar : 440001\n",
      "Train_AverageReturn : 129.6834156870761\n",
      "Train_BestReturn : 129.6834156870761\n",
      "TimeSinceStart : 2166.6383628845215\n",
      "Training Loss : 0.16031058132648468\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 441001\n",
      "mean reward (100 episodes) 132.116227\n",
      "best mean reward 132.116227\n",
      "running time 2172.852033\n",
      "Train_EnvstepsSoFar : 441001\n",
      "Train_AverageReturn : 132.11622654066048\n",
      "Train_BestReturn : 132.11622654066048\n",
      "TimeSinceStart : 2172.8520328998566\n",
      "Training Loss : 0.15902847051620483\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 442001\n",
      "mean reward (100 episodes) 131.123038\n",
      "best mean reward 132.116227\n",
      "running time 2178.218894\n",
      "Train_EnvstepsSoFar : 442001\n",
      "Train_AverageReturn : 131.12303808838888\n",
      "Train_BestReturn : 132.11622654066048\n",
      "TimeSinceStart : 2178.218893766403\n",
      "Training Loss : 0.5944157838821411\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 443001\n",
      "mean reward (100 episodes) 129.197555\n",
      "best mean reward 132.116227\n",
      "running time 2183.280364\n",
      "Train_EnvstepsSoFar : 443001\n",
      "Train_AverageReturn : 129.1975548124209\n",
      "Train_BestReturn : 132.11622654066048\n",
      "TimeSinceStart : 2183.28036403656\n",
      "Training Loss : 0.09612574428319931\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 444001\n",
      "mean reward (100 episodes) 126.384956\n",
      "best mean reward 132.116227\n",
      "running time 2187.666179\n",
      "Train_EnvstepsSoFar : 444001\n",
      "Train_AverageReturn : 126.38495646017073\n",
      "Train_BestReturn : 132.11622654066048\n",
      "TimeSinceStart : 2187.666178703308\n",
      "Training Loss : 0.18569421768188477\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 445001\n",
      "mean reward (100 episodes) 127.509569\n",
      "best mean reward 132.116227\n",
      "running time 2197.592846\n",
      "Train_EnvstepsSoFar : 445001\n",
      "Train_AverageReturn : 127.5095685156924\n",
      "Train_BestReturn : 132.11622654066048\n",
      "TimeSinceStart : 2197.5928456783295\n",
      "Training Loss : 0.2150743305683136\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 446001\n",
      "mean reward (100 episodes) 132.999930\n",
      "best mean reward 132.999930\n",
      "running time 2201.679583\n",
      "Train_EnvstepsSoFar : 446001\n",
      "Train_AverageReturn : 132.99992990580648\n",
      "Train_BestReturn : 132.99992990580648\n",
      "TimeSinceStart : 2201.679582834244\n",
      "Training Loss : 0.1989210546016693\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 447001\n",
      "mean reward (100 episodes) 133.359835\n",
      "best mean reward 133.359835\n",
      "running time 2207.091078\n",
      "Train_EnvstepsSoFar : 447001\n",
      "Train_AverageReturn : 133.3598348710585\n",
      "Train_BestReturn : 133.3598348710585\n",
      "TimeSinceStart : 2207.0910778045654\n",
      "Training Loss : 0.08883622288703918\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 448001\n",
      "mean reward (100 episodes) 135.799016\n",
      "best mean reward 135.799016\n",
      "running time 2211.430612\n",
      "Train_EnvstepsSoFar : 448001\n",
      "Train_AverageReturn : 135.79901615427525\n",
      "Train_BestReturn : 135.79901615427525\n",
      "TimeSinceStart : 2211.430611848831\n",
      "Training Loss : 0.7615321278572083\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 449001\n",
      "mean reward (100 episodes) 134.175777\n",
      "best mean reward 135.799016\n",
      "running time 2217.778964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 449001\n",
      "Train_AverageReturn : 134.17577720157288\n",
      "Train_BestReturn : 135.79901615427525\n",
      "TimeSinceStart : 2217.778963804245\n",
      "Training Loss : 0.11487796157598495\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 450001\n",
      "mean reward (100 episodes) 134.609376\n",
      "best mean reward 135.799016\n",
      "running time 2223.213791\n",
      "Train_EnvstepsSoFar : 450001\n",
      "Train_AverageReturn : 134.60937637193254\n",
      "Train_BestReturn : 135.79901615427525\n",
      "TimeSinceStart : 2223.2137908935547\n",
      "Training Loss : 0.09704088419675827\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 451001\n",
      "mean reward (100 episodes) 136.345846\n",
      "best mean reward 136.345846\n",
      "running time 2227.517277\n",
      "Train_EnvstepsSoFar : 451001\n",
      "Train_AverageReturn : 136.34584640560243\n",
      "Train_BestReturn : 136.34584640560243\n",
      "TimeSinceStart : 2227.517276763916\n",
      "Training Loss : 1.4264951944351196\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 452001\n",
      "mean reward (100 episodes) 135.825031\n",
      "best mean reward 136.345846\n",
      "running time 2231.947442\n",
      "Train_EnvstepsSoFar : 452001\n",
      "Train_AverageReturn : 135.82503084593517\n",
      "Train_BestReturn : 136.34584640560243\n",
      "TimeSinceStart : 2231.94744181633\n",
      "Training Loss : 0.2907421290874481\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 453001\n",
      "mean reward (100 episodes) 133.378393\n",
      "best mean reward 136.345846\n",
      "running time 2236.955359\n",
      "Train_EnvstepsSoFar : 453001\n",
      "Train_AverageReturn : 133.37839251188413\n",
      "Train_BestReturn : 136.34584640560243\n",
      "TimeSinceStart : 2236.955358982086\n",
      "Training Loss : 0.08860626071691513\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 454001\n",
      "mean reward (100 episodes) 136.984920\n",
      "best mean reward 136.984920\n",
      "running time 2240.972767\n",
      "Train_EnvstepsSoFar : 454001\n",
      "Train_AverageReturn : 136.98491952641476\n",
      "Train_BestReturn : 136.98491952641476\n",
      "TimeSinceStart : 2240.9727668762207\n",
      "Training Loss : 0.06241411343216896\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 455001\n",
      "mean reward (100 episodes) 142.464857\n",
      "best mean reward 142.464857\n",
      "running time 2244.739637\n",
      "Train_EnvstepsSoFar : 455001\n",
      "Train_AverageReturn : 142.46485671813025\n",
      "Train_BestReturn : 142.46485671813025\n",
      "TimeSinceStart : 2244.7396368980408\n",
      "Training Loss : 0.11977792531251907\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 456001\n",
      "mean reward (100 episodes) 144.793397\n",
      "best mean reward 144.793397\n",
      "running time 2249.175423\n",
      "Train_EnvstepsSoFar : 456001\n",
      "Train_AverageReturn : 144.79339743302805\n",
      "Train_BestReturn : 144.79339743302805\n",
      "TimeSinceStart : 2249.1754229068756\n",
      "Training Loss : 0.2110043466091156\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 457001\n",
      "mean reward (100 episodes) 146.054081\n",
      "best mean reward 146.054081\n",
      "running time 2254.399671\n",
      "Train_EnvstepsSoFar : 457001\n",
      "Train_AverageReturn : 146.0540807344716\n",
      "Train_BestReturn : 146.0540807344716\n",
      "TimeSinceStart : 2254.3996708393097\n",
      "Training Loss : 0.6228461861610413\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 458001\n",
      "mean reward (100 episodes) 147.697537\n",
      "best mean reward 147.697537\n",
      "running time 2259.670123\n",
      "Train_EnvstepsSoFar : 458001\n",
      "Train_AverageReturn : 147.69753748257918\n",
      "Train_BestReturn : 147.69753748257918\n",
      "TimeSinceStart : 2259.670122861862\n",
      "Training Loss : 0.08625876903533936\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 459001\n",
      "mean reward (100 episodes) 149.869879\n",
      "best mean reward 149.869879\n",
      "running time 2264.171214\n",
      "Train_EnvstepsSoFar : 459001\n",
      "Train_AverageReturn : 149.8698785499396\n",
      "Train_BestReturn : 149.8698785499396\n",
      "TimeSinceStart : 2264.1712136268616\n",
      "Training Loss : 0.15018810331821442\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 460001\n",
      "mean reward (100 episodes) 150.007787\n",
      "best mean reward 150.007787\n",
      "running time 2269.932251\n",
      "Train_EnvstepsSoFar : 460001\n",
      "Train_AverageReturn : 150.0077868795462\n",
      "Train_BestReturn : 150.0077868795462\n",
      "TimeSinceStart : 2269.932250738144\n",
      "Training Loss : 0.09948991984128952\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 461001\n",
      "mean reward (100 episodes) 147.984639\n",
      "best mean reward 150.007787\n",
      "running time 2275.906764\n",
      "Train_EnvstepsSoFar : 461001\n",
      "Train_AverageReturn : 147.98463928452819\n",
      "Train_BestReturn : 150.0077868795462\n",
      "TimeSinceStart : 2275.9067640304565\n",
      "Training Loss : 0.09448357671499252\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 462001\n",
      "mean reward (100 episodes) 145.840664\n",
      "best mean reward 150.007787\n",
      "running time 2280.293612\n",
      "Train_EnvstepsSoFar : 462001\n",
      "Train_AverageReturn : 145.84066357810298\n",
      "Train_BestReturn : 150.0077868795462\n",
      "TimeSinceStart : 2280.2936120033264\n",
      "Training Loss : 0.12182518094778061\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 463001\n",
      "mean reward (100 episodes) 150.378865\n",
      "best mean reward 150.378865\n",
      "running time 2284.198825\n",
      "Train_EnvstepsSoFar : 463001\n",
      "Train_AverageReturn : 150.3788654123014\n",
      "Train_BestReturn : 150.3788654123014\n",
      "TimeSinceStart : 2284.1988248825073\n",
      "Training Loss : 0.12846355140209198\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 464001\n",
      "mean reward (100 episodes) 148.424901\n",
      "best mean reward 150.378865\n",
      "running time 2289.611395\n",
      "Train_EnvstepsSoFar : 464001\n",
      "Train_AverageReturn : 148.42490066711431\n",
      "Train_BestReturn : 150.3788654123014\n",
      "TimeSinceStart : 2289.6113946437836\n",
      "Training Loss : 0.7091388702392578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 465001\n",
      "mean reward (100 episodes) 146.164731\n",
      "best mean reward 150.378865\n",
      "running time 2296.423821\n",
      "Train_EnvstepsSoFar : 465001\n",
      "Train_AverageReturn : 146.1647309995064\n",
      "Train_BestReturn : 150.3788654123014\n",
      "TimeSinceStart : 2296.423820734024\n",
      "Training Loss : 0.16293174028396606\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 466001\n",
      "mean reward (100 episodes) 143.774784\n",
      "best mean reward 150.378865\n",
      "running time 2302.834192\n",
      "Train_EnvstepsSoFar : 466001\n",
      "Train_AverageReturn : 143.77478368517384\n",
      "Train_BestReturn : 150.3788654123014\n",
      "TimeSinceStart : 2302.834191799164\n",
      "Training Loss : 0.17195844650268555\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 467001\n",
      "mean reward (100 episodes) 143.733585\n",
      "best mean reward 150.378865\n",
      "running time 2307.798138\n",
      "Train_EnvstepsSoFar : 467001\n",
      "Train_AverageReturn : 143.7335851760054\n",
      "Train_BestReturn : 150.3788654123014\n",
      "TimeSinceStart : 2307.798137664795\n",
      "Training Loss : 0.17068105936050415\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 468001\n",
      "mean reward (100 episodes) 141.903630\n",
      "best mean reward 150.378865\n",
      "running time 2313.073861\n",
      "Train_EnvstepsSoFar : 468001\n",
      "Train_AverageReturn : 141.9036299473884\n",
      "Train_BestReturn : 150.3788654123014\n",
      "TimeSinceStart : 2313.0738608837128\n",
      "Training Loss : 2.0879921913146973\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 469001\n",
      "mean reward (100 episodes) 141.971064\n",
      "best mean reward 150.378865\n",
      "running time 2317.979815\n",
      "Train_EnvstepsSoFar : 469001\n",
      "Train_AverageReturn : 141.97106433003125\n",
      "Train_BestReturn : 150.3788654123014\n",
      "TimeSinceStart : 2317.9798147678375\n",
      "Training Loss : 4.573170185089111\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 470001\n",
      "mean reward (100 episodes) 140.989462\n",
      "best mean reward 150.378865\n",
      "running time 2323.905809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 470001\n",
      "Train_AverageReturn : 140.98946209107322\n",
      "Train_BestReturn : 150.3788654123014\n",
      "TimeSinceStart : 2323.90580868721\n",
      "Training Loss : 0.10000870376825333\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 471001\n",
      "mean reward (100 episodes) 141.617317\n",
      "best mean reward 150.378865\n",
      "running time 2328.025901\n",
      "Train_EnvstepsSoFar : 471001\n",
      "Train_AverageReturn : 141.6173165470646\n",
      "Train_BestReturn : 150.3788654123014\n",
      "TimeSinceStart : 2328.0259006023407\n",
      "Training Loss : 0.36122339963912964\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 472001\n",
      "mean reward (100 episodes) 142.111187\n",
      "best mean reward 150.378865\n",
      "running time 2331.944759\n",
      "Train_EnvstepsSoFar : 472001\n",
      "Train_AverageReturn : 142.11118718311081\n",
      "Train_BestReturn : 150.3788654123014\n",
      "TimeSinceStart : 2331.9447588920593\n",
      "Training Loss : 1.2600679397583008\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 473001\n",
      "mean reward (100 episodes) 142.363266\n",
      "best mean reward 150.378865\n",
      "running time 2336.693212\n",
      "Train_EnvstepsSoFar : 473001\n",
      "Train_AverageReturn : 142.3632656828041\n",
      "Train_BestReturn : 150.3788654123014\n",
      "TimeSinceStart : 2336.6932117938995\n",
      "Training Loss : 0.1778745949268341\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 474001\n",
      "mean reward (100 episodes) 140.030198\n",
      "best mean reward 150.378865\n",
      "running time 2341.889120\n",
      "Train_EnvstepsSoFar : 474001\n",
      "Train_AverageReturn : 140.03019848253206\n",
      "Train_BestReturn : 150.3788654123014\n",
      "TimeSinceStart : 2341.88911986351\n",
      "Training Loss : 0.12024828046560287\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 475001\n",
      "mean reward (100 episodes) 140.193049\n",
      "best mean reward 150.378865\n",
      "running time 2346.225227\n",
      "Train_EnvstepsSoFar : 475001\n",
      "Train_AverageReturn : 140.19304869042296\n",
      "Train_BestReturn : 150.3788654123014\n",
      "TimeSinceStart : 2346.22522687912\n",
      "Training Loss : 0.0990152359008789\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 476001\n",
      "mean reward (100 episodes) 142.240229\n",
      "best mean reward 150.378865\n",
      "running time 2350.441819\n",
      "Train_EnvstepsSoFar : 476001\n",
      "Train_AverageReturn : 142.24022889113144\n",
      "Train_BestReturn : 150.3788654123014\n",
      "TimeSinceStart : 2350.441818714142\n",
      "Training Loss : 0.12539736926555634\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 477001\n",
      "mean reward (100 episodes) 145.315452\n",
      "best mean reward 150.378865\n",
      "running time 2355.193949\n",
      "Train_EnvstepsSoFar : 477001\n",
      "Train_AverageReturn : 145.31545151950837\n",
      "Train_BestReturn : 150.3788654123014\n",
      "TimeSinceStart : 2355.1939487457275\n",
      "Training Loss : 0.09298785030841827\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 478001\n",
      "mean reward (100 episodes) 140.429672\n",
      "best mean reward 150.378865\n",
      "running time 2360.513734\n",
      "Train_EnvstepsSoFar : 478001\n",
      "Train_AverageReturn : 140.42967204899108\n",
      "Train_BestReturn : 150.3788654123014\n",
      "TimeSinceStart : 2360.5137338638306\n",
      "Training Loss : 0.11095152795314789\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 479001\n",
      "mean reward (100 episodes) 143.263529\n",
      "best mean reward 150.378865\n",
      "running time 2364.639376\n",
      "Train_EnvstepsSoFar : 479001\n",
      "Train_AverageReturn : 143.26352853686154\n",
      "Train_BestReturn : 150.3788654123014\n",
      "TimeSinceStart : 2364.6393756866455\n",
      "Training Loss : 3.500723361968994\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 480001\n",
      "mean reward (100 episodes) 142.792415\n",
      "best mean reward 150.378865\n",
      "running time 2370.025690\n",
      "Train_EnvstepsSoFar : 480001\n",
      "Train_AverageReturn : 142.7924148036807\n",
      "Train_BestReturn : 150.3788654123014\n",
      "TimeSinceStart : 2370.0256898403168\n",
      "Training Loss : 0.7538049221038818\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 481001\n",
      "mean reward (100 episodes) 144.996613\n",
      "best mean reward 150.378865\n",
      "running time 2373.818655\n",
      "Train_EnvstepsSoFar : 481001\n",
      "Train_AverageReturn : 144.99661334135303\n",
      "Train_BestReturn : 150.3788654123014\n",
      "TimeSinceStart : 2373.8186547756195\n",
      "Training Loss : 0.09912484884262085\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 482001\n",
      "mean reward (100 episodes) 144.321031\n",
      "best mean reward 150.378865\n",
      "running time 2378.419589\n",
      "Train_EnvstepsSoFar : 482001\n",
      "Train_AverageReturn : 144.32103128641324\n",
      "Train_BestReturn : 150.3788654123014\n",
      "TimeSinceStart : 2378.419588804245\n",
      "Training Loss : 0.2710679769515991\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 483001\n",
      "mean reward (100 episodes) 143.006308\n",
      "best mean reward 150.378865\n",
      "running time 2383.008858\n",
      "Train_EnvstepsSoFar : 483001\n",
      "Train_AverageReturn : 143.00630800913555\n",
      "Train_BestReturn : 150.3788654123014\n",
      "TimeSinceStart : 2383.008857727051\n",
      "Training Loss : 0.12119103223085403\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 484001\n",
      "mean reward (100 episodes) 132.843081\n",
      "best mean reward 150.378865\n",
      "running time 2388.296500\n",
      "Train_EnvstepsSoFar : 484001\n",
      "Train_AverageReturn : 132.84308084631883\n",
      "Train_BestReturn : 150.3788654123014\n",
      "TimeSinceStart : 2388.296499967575\n",
      "Training Loss : 3.103759288787842\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 485001\n",
      "mean reward (100 episodes) 130.659966\n",
      "best mean reward 150.378865\n",
      "running time 2394.286855\n",
      "Train_EnvstepsSoFar : 485001\n",
      "Train_AverageReturn : 130.65996579524273\n",
      "Train_BestReturn : 150.3788654123014\n",
      "TimeSinceStart : 2394.2868547439575\n",
      "Training Loss : 0.13401152193546295\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 486001\n",
      "mean reward (100 episodes) 132.936998\n",
      "best mean reward 150.378865\n",
      "running time 2398.301589\n",
      "Train_EnvstepsSoFar : 486001\n",
      "Train_AverageReturn : 132.93699795506143\n",
      "Train_BestReturn : 150.3788654123014\n",
      "TimeSinceStart : 2398.301589012146\n",
      "Training Loss : 0.16897818446159363\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 487001\n",
      "mean reward (100 episodes) 132.146908\n",
      "best mean reward 150.378865\n",
      "running time 2403.719115\n",
      "Train_EnvstepsSoFar : 487001\n",
      "Train_AverageReturn : 132.14690820681193\n",
      "Train_BestReturn : 150.3788654123014\n",
      "TimeSinceStart : 2403.7191150188446\n",
      "Training Loss : 0.75795978307724\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 488001\n",
      "mean reward (100 episodes) 130.397571\n",
      "best mean reward 150.378865\n",
      "running time 2408.326103\n",
      "Train_EnvstepsSoFar : 488001\n",
      "Train_AverageReturn : 130.3975707340062\n",
      "Train_BestReturn : 150.3788654123014\n",
      "TimeSinceStart : 2408.326102733612\n",
      "Training Loss : 2.720918655395508\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 489001\n",
      "mean reward (100 episodes) 132.874822\n",
      "best mean reward 150.378865\n",
      "running time 2414.293672\n",
      "Train_EnvstepsSoFar : 489001\n",
      "Train_AverageReturn : 132.8748224376749\n",
      "Train_BestReturn : 150.3788654123014\n",
      "TimeSinceStart : 2414.2936718463898\n",
      "Training Loss : 0.5830963850021362\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 490001\n",
      "mean reward (100 episodes) 133.510024\n",
      "best mean reward 150.378865\n",
      "running time 2418.196489\n",
      "Train_EnvstepsSoFar : 490001\n",
      "Train_AverageReturn : 133.510024063467\n",
      "Train_BestReturn : 150.3788654123014\n",
      "TimeSinceStart : 2418.1964888572693\n",
      "Training Loss : 0.10802079737186432\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 491001\n",
      "mean reward (100 episodes) 137.505761\n",
      "best mean reward 150.378865\n",
      "running time 2421.930800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 491001\n",
      "Train_AverageReturn : 137.50576119274183\n",
      "Train_BestReturn : 150.3788654123014\n",
      "TimeSinceStart : 2421.9307997226715\n",
      "Training Loss : 0.1173701137304306\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 492001\n",
      "mean reward (100 episodes) 134.681733\n",
      "best mean reward 150.378865\n",
      "running time 2425.681937\n",
      "Train_EnvstepsSoFar : 492001\n",
      "Train_AverageReturn : 134.68173333976395\n",
      "Train_BestReturn : 150.3788654123014\n",
      "TimeSinceStart : 2425.6819367408752\n",
      "Training Loss : 0.0847950279712677\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 493001\n",
      "mean reward (100 episodes) 142.151452\n",
      "best mean reward 150.378865\n",
      "running time 2430.201422\n",
      "Train_EnvstepsSoFar : 493001\n",
      "Train_AverageReturn : 142.15145228803226\n",
      "Train_BestReturn : 150.3788654123014\n",
      "TimeSinceStart : 2430.201421737671\n",
      "Training Loss : 0.14168283343315125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 494001\n",
      "mean reward (100 episodes) 140.361727\n",
      "best mean reward 150.378865\n",
      "running time 2434.155084\n",
      "Train_EnvstepsSoFar : 494001\n",
      "Train_AverageReturn : 140.36172669204302\n",
      "Train_BestReturn : 150.3788654123014\n",
      "TimeSinceStart : 2434.1550838947296\n",
      "Training Loss : 0.21241925656795502\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 495001\n",
      "mean reward (100 episodes) 139.428259\n",
      "best mean reward 150.378865\n",
      "running time 2437.998258\n",
      "Train_EnvstepsSoFar : 495001\n",
      "Train_AverageReturn : 139.4282593301706\n",
      "Train_BestReturn : 150.3788654123014\n",
      "TimeSinceStart : 2437.9982578754425\n",
      "Training Loss : 0.1635701209306717\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 496001\n",
      "mean reward (100 episodes) 140.573551\n",
      "best mean reward 150.378865\n",
      "running time 2442.052569\n",
      "Train_EnvstepsSoFar : 496001\n",
      "Train_AverageReturn : 140.57355133312632\n",
      "Train_BestReturn : 150.3788654123014\n",
      "TimeSinceStart : 2442.0525686740875\n",
      "Training Loss : 0.11666825413703918\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 497001\n",
      "mean reward (100 episodes) 138.152905\n",
      "best mean reward 150.378865\n",
      "running time 2446.041219\n",
      "Train_EnvstepsSoFar : 497001\n",
      "Train_AverageReturn : 138.15290485656496\n",
      "Train_BestReturn : 150.3788654123014\n",
      "TimeSinceStart : 2446.0412187576294\n",
      "Training Loss : 0.1763942688703537\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 498001\n",
      "mean reward (100 episodes) 135.650454\n",
      "best mean reward 150.378865\n",
      "running time 2450.134680\n",
      "Train_EnvstepsSoFar : 498001\n",
      "Train_AverageReturn : 135.6504537324791\n",
      "Train_BestReturn : 150.3788654123014\n",
      "TimeSinceStart : 2450.1346797943115\n",
      "Training Loss : 1.607884407043457\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 499000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 499001\n",
      "mean reward (100 episodes) 136.926143\n",
      "best mean reward 150.378865\n",
      "running time 2453.745330\n",
      "Train_EnvstepsSoFar : 499001\n",
      "Train_AverageReturn : 136.9261429057002\n",
      "Train_BestReturn : 150.3788654123014\n",
      "TimeSinceStart : 2453.7453298568726\n",
      "Training Loss : 0.32013916969299316\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Running DQN experiment with seed 2\n",
      "########################\n",
      "logging outputs to  logs/dqn/LunarLander/double_dqn/seed2\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "LunarLander-v3\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1\n",
      "mean reward (100 episodes) nan\n",
      "best mean reward -inf\n",
      "running time 0.000756\n",
      "Train_EnvstepsSoFar : 1\n",
      "TimeSinceStart : 0.0007560253143310547\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1001\n",
      "mean reward (100 episodes) -302.396906\n",
      "best mean reward -inf\n",
      "running time 0.582821\n",
      "Train_EnvstepsSoFar : 1001\n",
      "Train_AverageReturn : -302.3969055199246\n",
      "TimeSinceStart : 0.5828208923339844\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2001\n",
      "mean reward (100 episodes) -305.134453\n",
      "best mean reward -inf\n",
      "running time 4.346673\n",
      "Train_EnvstepsSoFar : 2001\n",
      "Train_AverageReturn : -305.13445259175523\n",
      "TimeSinceStart : 4.346673011779785\n",
      "Training Loss : 0.522304892539978\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3001\n",
      "mean reward (100 episodes) -284.363309\n",
      "best mean reward -inf\n",
      "running time 8.414296\n",
      "Train_EnvstepsSoFar : 3001\n",
      "Train_AverageReturn : -284.36330940937097\n",
      "TimeSinceStart : 8.41429591178894\n",
      "Training Loss : 0.32587260007858276\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4001\n",
      "mean reward (100 episodes) -275.612412\n",
      "best mean reward -inf\n",
      "running time 12.075486\n",
      "Train_EnvstepsSoFar : 4001\n",
      "Train_AverageReturn : -275.6124120764997\n",
      "TimeSinceStart : 12.075485944747925\n",
      "Training Loss : 0.2805907130241394\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5001\n",
      "mean reward (100 episodes) -269.137094\n",
      "best mean reward -inf\n",
      "running time 15.595907\n",
      "Train_EnvstepsSoFar : 5001\n",
      "Train_AverageReturn : -269.1370941621596\n",
      "TimeSinceStart : 15.595906734466553\n",
      "Training Loss : 0.8690057396888733\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6001\n",
      "mean reward (100 episodes) -265.447240\n",
      "best mean reward -inf\n",
      "running time 19.170044\n",
      "Train_EnvstepsSoFar : 6001\n",
      "Train_AverageReturn : -265.4472396243595\n",
      "TimeSinceStart : 19.1700439453125\n",
      "Training Loss : 0.29164737462997437\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7001\n",
      "mean reward (100 episodes) -256.046515\n",
      "best mean reward -inf\n",
      "running time 22.729373\n",
      "Train_EnvstepsSoFar : 7001\n",
      "Train_AverageReturn : -256.0465145852913\n",
      "TimeSinceStart : 22.72937297821045\n",
      "Training Loss : 0.6849083304405212\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8001\n",
      "mean reward (100 episodes) -239.109098\n",
      "best mean reward -inf\n",
      "running time 26.246224\n",
      "Train_EnvstepsSoFar : 8001\n",
      "Train_AverageReturn : -239.10909802923962\n",
      "TimeSinceStart : 26.24622392654419\n",
      "Training Loss : 3.538076877593994\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9001\n",
      "mean reward (100 episodes) -231.638781\n",
      "best mean reward -inf\n",
      "running time 29.796703\n",
      "Train_EnvstepsSoFar : 9001\n",
      "Train_AverageReturn : -231.6387814560549\n",
      "TimeSinceStart : 29.796703100204468\n",
      "Training Loss : 0.4641547203063965\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10001\n",
      "mean reward (100 episodes) -226.419804\n",
      "best mean reward -inf\n",
      "running time 33.353148\n",
      "Train_EnvstepsSoFar : 10001\n",
      "Train_AverageReturn : -226.41980352577863\n",
      "TimeSinceStart : 33.353147983551025\n",
      "Training Loss : 0.6497313976287842\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 11000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 11001\n",
      "mean reward (100 episodes) -222.879445\n",
      "best mean reward -inf\n",
      "running time 36.941873\n",
      "Train_EnvstepsSoFar : 11001\n",
      "Train_AverageReturn : -222.87944516830558\n",
      "TimeSinceStart : 36.9418728351593\n",
      "Training Loss : 2.0363576412200928\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 12000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 12001\n",
      "mean reward (100 episodes) -219.909023\n",
      "best mean reward -inf\n",
      "running time 42.858052\n",
      "Train_EnvstepsSoFar : 12001\n",
      "Train_AverageReturn : -219.90902337156044\n",
      "TimeSinceStart : 42.858051776885986\n",
      "Training Loss : 3.661614418029785\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 13000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 13001\n",
      "mean reward (100 episodes) -220.396556\n",
      "best mean reward -inf\n",
      "running time 46.539740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 13001\n",
      "Train_AverageReturn : -220.39655579400207\n",
      "TimeSinceStart : 46.53973984718323\n",
      "Training Loss : 1.612704873085022\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 14000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 14001\n",
      "mean reward (100 episodes) -218.416954\n",
      "best mean reward -inf\n",
      "running time 52.195799\n",
      "Train_EnvstepsSoFar : 14001\n",
      "Train_AverageReturn : -218.41695441379497\n",
      "TimeSinceStart : 52.195799112319946\n",
      "Training Loss : 6.472461700439453\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 15000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 15001\n",
      "mean reward (100 episodes) -220.200848\n",
      "best mean reward -inf\n",
      "running time 56.910662\n",
      "Train_EnvstepsSoFar : 15001\n",
      "Train_AverageReturn : -220.20084823536314\n",
      "TimeSinceStart : 56.910661935806274\n",
      "Training Loss : 1.5414334535598755\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 16000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 16001\n",
      "mean reward (100 episodes) -226.907324\n",
      "best mean reward -inf\n",
      "running time 60.945255\n",
      "Train_EnvstepsSoFar : 16001\n",
      "Train_AverageReturn : -226.90732432748806\n",
      "TimeSinceStart : 60.94525504112244\n",
      "Training Loss : 0.844512403011322\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 17000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 17001\n",
      "mean reward (100 episodes) -216.060476\n",
      "best mean reward -inf\n",
      "running time 64.855054\n",
      "Train_EnvstepsSoFar : 17001\n",
      "Train_AverageReturn : -216.06047613488644\n",
      "TimeSinceStart : 64.85505390167236\n",
      "Training Loss : 0.5940214991569519\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 18000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 18001\n",
      "mean reward (100 episodes) -215.342159\n",
      "best mean reward -inf\n",
      "running time 68.861067\n",
      "Train_EnvstepsSoFar : 18001\n",
      "Train_AverageReturn : -215.34215888884228\n",
      "TimeSinceStart : 68.86106705665588\n",
      "Training Loss : 0.4987693428993225\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 19000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 19001\n",
      "mean reward (100 episodes) -214.608606\n",
      "best mean reward -inf\n",
      "running time 73.209865\n",
      "Train_EnvstepsSoFar : 19001\n",
      "Train_AverageReturn : -214.6086058770846\n",
      "TimeSinceStart : 73.2098650932312\n",
      "Training Loss : 0.4299941956996918\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 20001\n",
      "mean reward (100 episodes) -214.820393\n",
      "best mean reward -inf\n",
      "running time 77.899348\n",
      "Train_EnvstepsSoFar : 20001\n",
      "Train_AverageReturn : -214.8203927331813\n",
      "TimeSinceStart : 77.89934802055359\n",
      "Training Loss : 1.560739278793335\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 21000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 21001\n",
      "mean reward (100 episodes) -212.709779\n",
      "best mean reward -inf\n",
      "running time 82.197001\n",
      "Train_EnvstepsSoFar : 21001\n",
      "Train_AverageReturn : -212.70977864649296\n",
      "TimeSinceStart : 82.1970009803772\n",
      "Training Loss : 1.418416976928711\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 22000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 22001\n",
      "mean reward (100 episodes) -207.602972\n",
      "best mean reward -207.602972\n",
      "running time 86.750764\n",
      "Train_EnvstepsSoFar : 22001\n",
      "Train_AverageReturn : -207.60297211883997\n",
      "Train_BestReturn : -207.60297211883997\n",
      "TimeSinceStart : 86.75076413154602\n",
      "Training Loss : 0.622446596622467\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 23000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 23001\n",
      "mean reward (100 episodes) -203.588676\n",
      "best mean reward -203.588676\n",
      "running time 91.468858\n",
      "Train_EnvstepsSoFar : 23001\n",
      "Train_AverageReturn : -203.5886760828498\n",
      "Train_BestReturn : -203.5886760828498\n",
      "TimeSinceStart : 91.46885800361633\n",
      "Training Loss : 0.8242132067680359\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 24000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 24001\n",
      "mean reward (100 episodes) -204.094641\n",
      "best mean reward -203.588676\n",
      "running time 96.917179\n",
      "Train_EnvstepsSoFar : 24001\n",
      "Train_AverageReturn : -204.09464129651846\n",
      "Train_BestReturn : -203.5886760828498\n",
      "TimeSinceStart : 96.91717886924744\n",
      "Training Loss : 1.5026276111602783\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 25000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 25001\n",
      "mean reward (100 episodes) -197.470887\n",
      "best mean reward -197.470887\n",
      "running time 102.018839\n",
      "Train_EnvstepsSoFar : 25001\n",
      "Train_AverageReturn : -197.47088745648256\n",
      "Train_BestReturn : -197.47088745648256\n",
      "TimeSinceStart : 102.01883912086487\n",
      "Training Loss : 0.8699529767036438\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 26000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 26001\n",
      "mean reward (100 episodes) -194.438497\n",
      "best mean reward -194.438497\n",
      "running time 106.874304\n",
      "Train_EnvstepsSoFar : 26001\n",
      "Train_AverageReturn : -194.43849749177608\n",
      "Train_BestReturn : -194.43849749177608\n",
      "TimeSinceStart : 106.8743040561676\n",
      "Training Loss : 0.9489879012107849\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 27000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 27001\n",
      "mean reward (100 episodes) -193.614157\n",
      "best mean reward -193.614157\n",
      "running time 111.820357\n",
      "Train_EnvstepsSoFar : 27001\n",
      "Train_AverageReturn : -193.61415673923605\n",
      "Train_BestReturn : -193.61415673923605\n",
      "TimeSinceStart : 111.82035684585571\n",
      "Training Loss : 0.9087864756584167\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 28000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 28001\n",
      "mean reward (100 episodes) -191.351410\n",
      "best mean reward -191.351410\n",
      "running time 116.258728\n",
      "Train_EnvstepsSoFar : 28001\n",
      "Train_AverageReturn : -191.35141043353528\n",
      "Train_BestReturn : -191.35141043353528\n",
      "TimeSinceStart : 116.25872802734375\n",
      "Training Loss : 0.4883745610713959\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 29000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 29001\n",
      "mean reward (100 episodes) -193.259082\n",
      "best mean reward -191.351410\n",
      "running time 120.802772\n",
      "Train_EnvstepsSoFar : 29001\n",
      "Train_AverageReturn : -193.2590822038459\n",
      "Train_BestReturn : -191.35141043353528\n",
      "TimeSinceStart : 120.8027720451355\n",
      "Training Loss : 0.6231558918952942\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 30001\n",
      "mean reward (100 episodes) -191.166881\n",
      "best mean reward -191.166881\n",
      "running time 125.467434\n",
      "Train_EnvstepsSoFar : 30001\n",
      "Train_AverageReturn : -191.1668809906764\n",
      "Train_BestReturn : -191.1668809906764\n",
      "TimeSinceStart : 125.46743392944336\n",
      "Training Loss : 1.002181887626648\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 31000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 31001\n",
      "mean reward (100 episodes) -189.899996\n",
      "best mean reward -189.899996\n",
      "running time 131.681239\n",
      "Train_EnvstepsSoFar : 31001\n",
      "Train_AverageReturn : -189.89999618145265\n",
      "Train_BestReturn : -189.89999618145265\n",
      "TimeSinceStart : 131.6812391281128\n",
      "Training Loss : 0.7305141091346741\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 32000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 32001\n",
      "mean reward (100 episodes) -186.754965\n",
      "best mean reward -186.754965\n",
      "running time 138.021582\n",
      "Train_EnvstepsSoFar : 32001\n",
      "Train_AverageReturn : -186.754964536601\n",
      "Train_BestReturn : -186.754964536601\n",
      "TimeSinceStart : 138.02158188819885\n",
      "Training Loss : 2.323061943054199\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 33000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 33001\n",
      "mean reward (100 episodes) -183.633413\n",
      "best mean reward -183.633413\n",
      "running time 142.717643\n",
      "Train_EnvstepsSoFar : 33001\n",
      "Train_AverageReturn : -183.63341259563782\n",
      "Train_BestReturn : -183.63341259563782\n",
      "TimeSinceStart : 142.71764302253723\n",
      "Training Loss : 0.29940512776374817\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 34000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 34001\n",
      "mean reward (100 episodes) -182.475999\n",
      "best mean reward -182.475999\n",
      "running time 147.352399\n",
      "Train_EnvstepsSoFar : 34001\n",
      "Train_AverageReturn : -182.47599890938648\n",
      "Train_BestReturn : -182.47599890938648\n",
      "TimeSinceStart : 147.3523988723755\n",
      "Training Loss : 0.7957837581634521\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 35000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 35001\n",
      "mean reward (100 episodes) -180.266790\n",
      "best mean reward -180.266790\n",
      "running time 152.640464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 35001\n",
      "Train_AverageReturn : -180.26678982527534\n",
      "Train_BestReturn : -180.26678982527534\n",
      "TimeSinceStart : 152.64046382904053\n",
      "Training Loss : 0.8518844246864319\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 36000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 36001\n",
      "mean reward (100 episodes) -179.460022\n",
      "best mean reward -179.460022\n",
      "running time 157.701024\n",
      "Train_EnvstepsSoFar : 36001\n",
      "Train_AverageReturn : -179.46002160208693\n",
      "Train_BestReturn : -179.46002160208693\n",
      "TimeSinceStart : 157.70102405548096\n",
      "Training Loss : 1.1785967350006104\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 37000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 37001\n",
      "mean reward (100 episodes) -179.316802\n",
      "best mean reward -179.316802\n",
      "running time 162.562716\n",
      "Train_EnvstepsSoFar : 37001\n",
      "Train_AverageReturn : -179.31680229048817\n",
      "Train_BestReturn : -179.31680229048817\n",
      "TimeSinceStart : 162.56271600723267\n",
      "Training Loss : 0.33487430214881897\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 38000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 38001\n",
      "mean reward (100 episodes) -176.942126\n",
      "best mean reward -176.942126\n",
      "running time 167.655748\n",
      "Train_EnvstepsSoFar : 38001\n",
      "Train_AverageReturn : -176.94212637627555\n",
      "Train_BestReturn : -176.94212637627555\n",
      "TimeSinceStart : 167.6557478904724\n",
      "Training Loss : 0.4164361357688904\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 39000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 39001\n",
      "mean reward (100 episodes) -177.411761\n",
      "best mean reward -176.942126\n",
      "running time 171.818952\n",
      "Train_EnvstepsSoFar : 39001\n",
      "Train_AverageReturn : -177.41176105918836\n",
      "Train_BestReturn : -176.94212637627555\n",
      "TimeSinceStart : 171.81895208358765\n",
      "Training Loss : 0.4775996804237366\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 40001\n",
      "mean reward (100 episodes) -175.015446\n",
      "best mean reward -175.015446\n",
      "running time 176.163993\n",
      "Train_EnvstepsSoFar : 40001\n",
      "Train_AverageReturn : -175.01544632217522\n",
      "Train_BestReturn : -175.01544632217522\n",
      "TimeSinceStart : 176.1639928817749\n",
      "Training Loss : 0.4385858178138733\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 41000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 41001\n",
      "mean reward (100 episodes) -170.646081\n",
      "best mean reward -170.646081\n",
      "running time 180.955863\n",
      "Train_EnvstepsSoFar : 41001\n",
      "Train_AverageReturn : -170.6460814540862\n",
      "Train_BestReturn : -170.6460814540862\n",
      "TimeSinceStart : 180.9558629989624\n",
      "Training Loss : 0.3764432668685913\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 42000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 42001\n",
      "mean reward (100 episodes) -170.906001\n",
      "best mean reward -170.646081\n",
      "running time 186.186612\n",
      "Train_EnvstepsSoFar : 42001\n",
      "Train_AverageReturn : -170.90600149771996\n",
      "Train_BestReturn : -170.6460814540862\n",
      "TimeSinceStart : 186.18661189079285\n",
      "Training Loss : 7.3416337966918945\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 43000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 43001\n",
      "mean reward (100 episodes) -171.524858\n",
      "best mean reward -170.646081\n",
      "running time 190.600256\n",
      "Train_EnvstepsSoFar : 43001\n",
      "Train_AverageReturn : -171.52485823309303\n",
      "Train_BestReturn : -170.6460814540862\n",
      "TimeSinceStart : 190.60025596618652\n",
      "Training Loss : 0.3081246614456177\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 44000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 44001\n",
      "mean reward (100 episodes) -171.613000\n",
      "best mean reward -170.646081\n",
      "running time 195.990172\n",
      "Train_EnvstepsSoFar : 44001\n",
      "Train_AverageReturn : -171.61299998208062\n",
      "Train_BestReturn : -170.6460814540862\n",
      "TimeSinceStart : 195.99017190933228\n",
      "Training Loss : 0.5387918949127197\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 45000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 45001\n",
      "mean reward (100 episodes) -168.841549\n",
      "best mean reward -168.841549\n",
      "running time 201.228078\n",
      "Train_EnvstepsSoFar : 45001\n",
      "Train_AverageReturn : -168.84154906733073\n",
      "Train_BestReturn : -168.84154906733073\n",
      "TimeSinceStart : 201.22807788848877\n",
      "Training Loss : 0.6187052726745605\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 46000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 46001\n",
      "mean reward (100 episodes) -169.080402\n",
      "best mean reward -168.841549\n",
      "running time 206.431695\n",
      "Train_EnvstepsSoFar : 46001\n",
      "Train_AverageReturn : -169.08040163878596\n",
      "Train_BestReturn : -168.84154906733073\n",
      "TimeSinceStart : 206.43169498443604\n",
      "Training Loss : 0.3275702893733978\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 47000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 47001\n",
      "mean reward (100 episodes) -167.885372\n",
      "best mean reward -167.885372\n",
      "running time 210.730328\n",
      "Train_EnvstepsSoFar : 47001\n",
      "Train_AverageReturn : -167.88537203405497\n",
      "Train_BestReturn : -167.88537203405497\n",
      "TimeSinceStart : 210.73032784461975\n",
      "Training Loss : 0.6103585362434387\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 48000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 48001\n",
      "mean reward (100 episodes) -165.577859\n",
      "best mean reward -165.577859\n",
      "running time 215.135844\n",
      "Train_EnvstepsSoFar : 48001\n",
      "Train_AverageReturn : -165.57785851261542\n",
      "Train_BestReturn : -165.57785851261542\n",
      "TimeSinceStart : 215.1358437538147\n",
      "Training Loss : 1.0598533153533936\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 49000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 49001\n",
      "mean reward (100 episodes) -165.170605\n",
      "best mean reward -165.170605\n",
      "running time 219.689047\n",
      "Train_EnvstepsSoFar : 49001\n",
      "Train_AverageReturn : -165.17060538699064\n",
      "Train_BestReturn : -165.17060538699064\n",
      "TimeSinceStart : 219.6890470981598\n",
      "Training Loss : 0.43215253949165344\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 50001\n",
      "mean reward (100 episodes) -162.977077\n",
      "best mean reward -162.977077\n",
      "running time 224.245631\n",
      "Train_EnvstepsSoFar : 50001\n",
      "Train_AverageReturn : -162.97707722645947\n",
      "Train_BestReturn : -162.97707722645947\n",
      "TimeSinceStart : 224.24563097953796\n",
      "Training Loss : 0.17644751071929932\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 51000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 51001\n",
      "mean reward (100 episodes) -164.887765\n",
      "best mean reward -162.977077\n",
      "running time 229.004718\n",
      "Train_EnvstepsSoFar : 51001\n",
      "Train_AverageReturn : -164.8877654664085\n",
      "Train_BestReturn : -162.97707722645947\n",
      "TimeSinceStart : 229.00471782684326\n",
      "Training Loss : 2.8533174991607666\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 52000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 52001\n",
      "mean reward (100 episodes) -164.276710\n",
      "best mean reward -162.977077\n",
      "running time 233.540970\n",
      "Train_EnvstepsSoFar : 52001\n",
      "Train_AverageReturn : -164.27671035476413\n",
      "Train_BestReturn : -162.97707722645947\n",
      "TimeSinceStart : 233.5409700870514\n",
      "Training Loss : 0.646748423576355\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 53000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 53001\n",
      "mean reward (100 episodes) -161.781975\n",
      "best mean reward -161.781975\n",
      "running time 238.257550\n",
      "Train_EnvstepsSoFar : 53001\n",
      "Train_AverageReturn : -161.781974849209\n",
      "Train_BestReturn : -161.781974849209\n",
      "TimeSinceStart : 238.25754976272583\n",
      "Training Loss : 0.35277044773101807\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 54000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 54001\n",
      "mean reward (100 episodes) -161.160418\n",
      "best mean reward -161.160418\n",
      "running time 243.168182\n",
      "Train_EnvstepsSoFar : 54001\n",
      "Train_AverageReturn : -161.1604183683213\n",
      "Train_BestReturn : -161.1604183683213\n",
      "TimeSinceStart : 243.1681821346283\n",
      "Training Loss : 0.5258105993270874\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 55000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 55001\n",
      "mean reward (100 episodes) -159.925438\n",
      "best mean reward -159.925438\n",
      "running time 247.625573\n",
      "Train_EnvstepsSoFar : 55001\n",
      "Train_AverageReturn : -159.92543771461717\n",
      "Train_BestReturn : -159.92543771461717\n",
      "TimeSinceStart : 247.62557291984558\n",
      "Training Loss : 0.7702925801277161\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 56000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 56001\n",
      "mean reward (100 episodes) -158.101264\n",
      "best mean reward -158.101264\n",
      "running time 252.408711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 56001\n",
      "Train_AverageReturn : -158.10126358991678\n",
      "Train_BestReturn : -158.10126358991678\n",
      "TimeSinceStart : 252.4087109565735\n",
      "Training Loss : 0.5487604737281799\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 57000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 57001\n",
      "mean reward (100 episodes) -154.090605\n",
      "best mean reward -154.090605\n",
      "running time 257.358698\n",
      "Train_EnvstepsSoFar : 57001\n",
      "Train_AverageReturn : -154.09060468124144\n",
      "Train_BestReturn : -154.09060468124144\n",
      "TimeSinceStart : 257.35869812965393\n",
      "Training Loss : 0.2686486840248108\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 58000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 58001\n",
      "mean reward (100 episodes) -149.077859\n",
      "best mean reward -149.077859\n",
      "running time 262.520087\n",
      "Train_EnvstepsSoFar : 58001\n",
      "Train_AverageReturn : -149.07785902419675\n",
      "Train_BestReturn : -149.07785902419675\n",
      "TimeSinceStart : 262.5200870037079\n",
      "Training Loss : 0.3289509117603302\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 59000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 59001\n",
      "mean reward (100 episodes) -151.233430\n",
      "best mean reward -149.077859\n",
      "running time 268.451509\n",
      "Train_EnvstepsSoFar : 59001\n",
      "Train_AverageReturn : -151.23343008457286\n",
      "Train_BestReturn : -149.07785902419675\n",
      "TimeSinceStart : 268.45150899887085\n",
      "Training Loss : 0.560648500919342\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 60001\n",
      "mean reward (100 episodes) -150.625836\n",
      "best mean reward -149.077859\n",
      "running time 273.216990\n",
      "Train_EnvstepsSoFar : 60001\n",
      "Train_AverageReturn : -150.62583628536632\n",
      "Train_BestReturn : -149.07785902419675\n",
      "TimeSinceStart : 273.2169899940491\n",
      "Training Loss : 0.4655669331550598\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 61000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 61001\n",
      "mean reward (100 episodes) -150.102767\n",
      "best mean reward -149.077859\n",
      "running time 277.947181\n",
      "Train_EnvstepsSoFar : 61001\n",
      "Train_AverageReturn : -150.1027669654757\n",
      "Train_BestReturn : -149.07785902419675\n",
      "TimeSinceStart : 277.9471809864044\n",
      "Training Loss : 0.37950342893600464\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 62000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 62001\n",
      "mean reward (100 episodes) -148.953518\n",
      "best mean reward -148.953518\n",
      "running time 283.449418\n",
      "Train_EnvstepsSoFar : 62001\n",
      "Train_AverageReturn : -148.95351751858618\n",
      "Train_BestReturn : -148.95351751858618\n",
      "TimeSinceStart : 283.44941806793213\n",
      "Training Loss : 0.271028071641922\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 63000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 63001\n",
      "mean reward (100 episodes) -147.372871\n",
      "best mean reward -147.372871\n",
      "running time 289.304188\n",
      "Train_EnvstepsSoFar : 63001\n",
      "Train_AverageReturn : -147.3728710192506\n",
      "Train_BestReturn : -147.3728710192506\n",
      "TimeSinceStart : 289.3041880130768\n",
      "Training Loss : 0.4006207585334778\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 64000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 64001\n",
      "mean reward (100 episodes) -145.668973\n",
      "best mean reward -145.668973\n",
      "running time 294.152877\n",
      "Train_EnvstepsSoFar : 64001\n",
      "Train_AverageReturn : -145.6689729974573\n",
      "Train_BestReturn : -145.6689729974573\n",
      "TimeSinceStart : 294.15287709236145\n",
      "Training Loss : 0.6871321201324463\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 65000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 65001\n",
      "mean reward (100 episodes) -146.000360\n",
      "best mean reward -145.668973\n",
      "running time 298.902256\n",
      "Train_EnvstepsSoFar : 65001\n",
      "Train_AverageReturn : -146.00035997716878\n",
      "Train_BestReturn : -145.6689729974573\n",
      "TimeSinceStart : 298.9022560119629\n",
      "Training Loss : 0.2420613318681717\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 66000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 66001\n",
      "mean reward (100 episodes) -146.313793\n",
      "best mean reward -145.668973\n",
      "running time 303.395219\n",
      "Train_EnvstepsSoFar : 66001\n",
      "Train_AverageReturn : -146.31379250303792\n",
      "Train_BestReturn : -145.6689729974573\n",
      "TimeSinceStart : 303.3952190876007\n",
      "Training Loss : 0.2580554783344269\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 67000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 67001\n",
      "mean reward (100 episodes) -145.428564\n",
      "best mean reward -145.428564\n",
      "running time 308.577888\n",
      "Train_EnvstepsSoFar : 67001\n",
      "Train_AverageReturn : -145.42856416397203\n",
      "Train_BestReturn : -145.42856416397203\n",
      "TimeSinceStart : 308.5778880119324\n",
      "Training Loss : 0.3603588044643402\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 68000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 68001\n",
      "mean reward (100 episodes) -147.475389\n",
      "best mean reward -145.428564\n",
      "running time 313.216902\n",
      "Train_EnvstepsSoFar : 68001\n",
      "Train_AverageReturn : -147.47538894713614\n",
      "Train_BestReturn : -145.42856416397203\n",
      "TimeSinceStart : 313.2169020175934\n",
      "Training Loss : 0.3534981906414032\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 69000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 69001\n",
      "mean reward (100 episodes) -150.825949\n",
      "best mean reward -145.428564\n",
      "running time 317.837619\n",
      "Train_EnvstepsSoFar : 69001\n",
      "Train_AverageReturn : -150.8259486176466\n",
      "Train_BestReturn : -145.42856416397203\n",
      "TimeSinceStart : 317.8376190662384\n",
      "Training Loss : 0.2944006621837616\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 70001\n",
      "mean reward (100 episodes) -147.547948\n",
      "best mean reward -145.428564\n",
      "running time 323.099709\n",
      "Train_EnvstepsSoFar : 70001\n",
      "Train_AverageReturn : -147.54794769126534\n",
      "Train_BestReturn : -145.42856416397203\n",
      "TimeSinceStart : 323.09970903396606\n",
      "Training Loss : 0.21012964844703674\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 71000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 71001\n",
      "mean reward (100 episodes) -147.134764\n",
      "best mean reward -145.428564\n",
      "running time 328.086925\n",
      "Train_EnvstepsSoFar : 71001\n",
      "Train_AverageReturn : -147.13476413553906\n",
      "Train_BestReturn : -145.42856416397203\n",
      "TimeSinceStart : 328.08692479133606\n",
      "Training Loss : 0.190373033285141\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 72000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 72001\n",
      "mean reward (100 episodes) -146.526726\n",
      "best mean reward -145.428564\n",
      "running time 332.033419\n",
      "Train_EnvstepsSoFar : 72001\n",
      "Train_AverageReturn : -146.5267260716098\n",
      "Train_BestReturn : -145.42856416397203\n",
      "TimeSinceStart : 332.03341913223267\n",
      "Training Loss : 0.28853482007980347\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 73000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 73001\n",
      "mean reward (100 episodes) -146.942135\n",
      "best mean reward -145.428564\n",
      "running time 337.009946\n",
      "Train_EnvstepsSoFar : 73001\n",
      "Train_AverageReturn : -146.94213465067457\n",
      "Train_BestReturn : -145.42856416397203\n",
      "TimeSinceStart : 337.0099458694458\n",
      "Training Loss : 0.19089065492153168\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 74000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 74001\n",
      "mean reward (100 episodes) -144.621938\n",
      "best mean reward -144.621938\n",
      "running time 342.636351\n",
      "Train_EnvstepsSoFar : 74001\n",
      "Train_AverageReturn : -144.62193848065385\n",
      "Train_BestReturn : -144.62193848065385\n",
      "TimeSinceStart : 342.636351108551\n",
      "Training Loss : 0.3674231767654419\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 75000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 75001\n",
      "mean reward (100 episodes) -141.219891\n",
      "best mean reward -141.219891\n",
      "running time 347.246439\n",
      "Train_EnvstepsSoFar : 75001\n",
      "Train_AverageReturn : -141.21989135843808\n",
      "Train_BestReturn : -141.21989135843808\n",
      "TimeSinceStart : 347.24643898010254\n",
      "Training Loss : 0.2603573203086853\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 76000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 76001\n",
      "mean reward (100 episodes) -140.391358\n",
      "best mean reward -140.391358\n",
      "running time 352.200107\n",
      "Train_EnvstepsSoFar : 76001\n",
      "Train_AverageReturn : -140.3913579256478\n",
      "Train_BestReturn : -140.3913579256478\n",
      "TimeSinceStart : 352.20010709762573\n",
      "Training Loss : 0.1595749706029892\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 77000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 77001\n",
      "mean reward (100 episodes) -137.995546\n",
      "best mean reward -137.995546\n",
      "running time 356.946836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 77001\n",
      "Train_AverageReturn : -137.99554585694568\n",
      "Train_BestReturn : -137.99554585694568\n",
      "TimeSinceStart : 356.94683599472046\n",
      "Training Loss : 0.5645427703857422\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 78000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 78001\n",
      "mean reward (100 episodes) -135.948716\n",
      "best mean reward -135.948716\n",
      "running time 361.933619\n",
      "Train_EnvstepsSoFar : 78001\n",
      "Train_AverageReturn : -135.94871619547345\n",
      "Train_BestReturn : -135.94871619547345\n",
      "TimeSinceStart : 361.9336190223694\n",
      "Training Loss : 0.2336585521697998\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 79000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 79001\n",
      "mean reward (100 episodes) -135.872690\n",
      "best mean reward -135.872690\n",
      "running time 368.712294\n",
      "Train_EnvstepsSoFar : 79001\n",
      "Train_AverageReturn : -135.87268981213845\n",
      "Train_BestReturn : -135.87268981213845\n",
      "TimeSinceStart : 368.7122938632965\n",
      "Training Loss : 0.22659209370613098\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 80001\n",
      "mean reward (100 episodes) -131.016735\n",
      "best mean reward -131.016735\n",
      "running time 375.657445\n",
      "Train_EnvstepsSoFar : 80001\n",
      "Train_AverageReturn : -131.01673528379928\n",
      "Train_BestReturn : -131.01673528379928\n",
      "TimeSinceStart : 375.65744495391846\n",
      "Training Loss : 0.287394255399704\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 81000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 81001\n",
      "mean reward (100 episodes) -129.393361\n",
      "best mean reward -129.393361\n",
      "running time 381.735929\n",
      "Train_EnvstepsSoFar : 81001\n",
      "Train_AverageReturn : -129.3933606801928\n",
      "Train_BestReturn : -129.3933606801928\n",
      "TimeSinceStart : 381.73592877388\n",
      "Training Loss : 0.18879184126853943\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 82000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 82001\n",
      "mean reward (100 episodes) -126.841317\n",
      "best mean reward -126.841317\n",
      "running time 387.925048\n",
      "Train_EnvstepsSoFar : 82001\n",
      "Train_AverageReturn : -126.84131739533359\n",
      "Train_BestReturn : -126.84131739533359\n",
      "TimeSinceStart : 387.9250478744507\n",
      "Training Loss : 0.21156257390975952\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 83000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 83001\n",
      "mean reward (100 episodes) -125.094743\n",
      "best mean reward -125.094743\n",
      "running time 392.905570\n",
      "Train_EnvstepsSoFar : 83001\n",
      "Train_AverageReturn : -125.09474334815327\n",
      "Train_BestReturn : -125.09474334815327\n",
      "TimeSinceStart : 392.9055700302124\n",
      "Training Loss : 0.17338508367538452\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 84000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 84001\n",
      "mean reward (100 episodes) -120.777866\n",
      "best mean reward -120.777866\n",
      "running time 398.253527\n",
      "Train_EnvstepsSoFar : 84001\n",
      "Train_AverageReturn : -120.77786588045679\n",
      "Train_BestReturn : -120.77786588045679\n",
      "TimeSinceStart : 398.25352716445923\n",
      "Training Loss : 0.1464139074087143\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 85000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 85001\n",
      "mean reward (100 episodes) -117.118374\n",
      "best mean reward -117.118374\n",
      "running time 403.835138\n",
      "Train_EnvstepsSoFar : 85001\n",
      "Train_AverageReturn : -117.1183743915444\n",
      "Train_BestReturn : -117.1183743915444\n",
      "TimeSinceStart : 403.8351380825043\n",
      "Training Loss : 1.1243070363998413\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 86000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 86001\n",
      "mean reward (100 episodes) -109.468661\n",
      "best mean reward -109.468661\n",
      "running time 408.604361\n",
      "Train_EnvstepsSoFar : 86001\n",
      "Train_AverageReturn : -109.4686611871879\n",
      "Train_BestReturn : -109.4686611871879\n",
      "TimeSinceStart : 408.6043610572815\n",
      "Training Loss : 0.0988326221704483\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 87000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 87001\n",
      "mean reward (100 episodes) -102.299312\n",
      "best mean reward -102.299312\n",
      "running time 413.092983\n",
      "Train_EnvstepsSoFar : 87001\n",
      "Train_AverageReturn : -102.29931238590014\n",
      "Train_BestReturn : -102.29931238590014\n",
      "TimeSinceStart : 413.09298300743103\n",
      "Training Loss : 0.15383023023605347\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 88000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 88001\n",
      "mean reward (100 episodes) -102.803802\n",
      "best mean reward -102.299312\n",
      "running time 419.836572\n",
      "Train_EnvstepsSoFar : 88001\n",
      "Train_AverageReturn : -102.80380181074261\n",
      "Train_BestReturn : -102.29931238590014\n",
      "TimeSinceStart : 419.836571931839\n",
      "Training Loss : 0.47204458713531494\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 89000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 89001\n",
      "mean reward (100 episodes) -99.286984\n",
      "best mean reward -99.286984\n",
      "running time 424.976035\n",
      "Train_EnvstepsSoFar : 89001\n",
      "Train_AverageReturn : -99.28698436875621\n",
      "Train_BestReturn : -99.28698436875621\n",
      "TimeSinceStart : 424.97603487968445\n",
      "Training Loss : 1.1536483764648438\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 90001\n",
      "mean reward (100 episodes) -96.184060\n",
      "best mean reward -96.184060\n",
      "running time 430.038105\n",
      "Train_EnvstepsSoFar : 90001\n",
      "Train_AverageReturn : -96.18406008003095\n",
      "Train_BestReturn : -96.18406008003095\n",
      "TimeSinceStart : 430.03810477256775\n",
      "Training Loss : 0.15780757367610931\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 91000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 91001\n",
      "mean reward (100 episodes) -92.919414\n",
      "best mean reward -92.919414\n",
      "running time 434.958323\n",
      "Train_EnvstepsSoFar : 91001\n",
      "Train_AverageReturn : -92.91941444423686\n",
      "Train_BestReturn : -92.91941444423686\n",
      "TimeSinceStart : 434.9583230018616\n",
      "Training Loss : 0.12923070788383484\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 92000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 92001\n",
      "mean reward (100 episodes) -88.313353\n",
      "best mean reward -88.313353\n",
      "running time 439.445649\n",
      "Train_EnvstepsSoFar : 92001\n",
      "Train_AverageReturn : -88.31335263870531\n",
      "Train_BestReturn : -88.31335263870531\n",
      "TimeSinceStart : 439.4456489086151\n",
      "Training Loss : 0.24458737671375275\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 93000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 93001\n",
      "mean reward (100 episodes) -87.513521\n",
      "best mean reward -87.513521\n",
      "running time 443.785862\n",
      "Train_EnvstepsSoFar : 93001\n",
      "Train_AverageReturn : -87.51352078851241\n",
      "Train_BestReturn : -87.51352078851241\n",
      "TimeSinceStart : 443.78586196899414\n",
      "Training Loss : 0.11714951694011688\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 94000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 94001\n",
      "mean reward (100 episodes) -86.396241\n",
      "best mean reward -86.396241\n",
      "running time 447.994587\n",
      "Train_EnvstepsSoFar : 94001\n",
      "Train_AverageReturn : -86.39624057958893\n",
      "Train_BestReturn : -86.39624057958893\n",
      "TimeSinceStart : 447.9945869445801\n",
      "Training Loss : 0.12624993920326233\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 95000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 95001\n",
      "mean reward (100 episodes) -85.216686\n",
      "best mean reward -85.216686\n",
      "running time 452.450432\n",
      "Train_EnvstepsSoFar : 95001\n",
      "Train_AverageReturn : -85.21668634403093\n",
      "Train_BestReturn : -85.21668634403093\n",
      "TimeSinceStart : 452.45043206214905\n",
      "Training Loss : 0.42616918683052063\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 96000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 96001\n",
      "mean reward (100 episodes) -80.065576\n",
      "best mean reward -80.065576\n",
      "running time 456.654475\n",
      "Train_EnvstepsSoFar : 96001\n",
      "Train_AverageReturn : -80.06557619158674\n",
      "Train_BestReturn : -80.06557619158674\n",
      "TimeSinceStart : 456.6544749736786\n",
      "Training Loss : 0.2093656212091446\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 97000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 97001\n",
      "mean reward (100 episodes) -79.870397\n",
      "best mean reward -79.870397\n",
      "running time 461.495217\n",
      "Train_EnvstepsSoFar : 97001\n",
      "Train_AverageReturn : -79.87039716436429\n",
      "Train_BestReturn : -79.87039716436429\n",
      "TimeSinceStart : 461.49521684646606\n",
      "Training Loss : 0.25937390327453613\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 98000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 98001\n",
      "mean reward (100 episodes) -74.046447\n",
      "best mean reward -74.046447\n",
      "running time 465.682235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 98001\n",
      "Train_AverageReturn : -74.04644722581939\n",
      "Train_BestReturn : -74.04644722581939\n",
      "TimeSinceStart : 465.6822350025177\n",
      "Training Loss : 1.1019253730773926\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 99000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 99001\n",
      "mean reward (100 episodes) -72.458857\n",
      "best mean reward -72.458857\n",
      "running time 470.549030\n",
      "Train_EnvstepsSoFar : 99001\n",
      "Train_AverageReturn : -72.45885658505051\n",
      "Train_BestReturn : -72.45885658505051\n",
      "TimeSinceStart : 470.5490298271179\n",
      "Training Loss : 1.1360039710998535\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 100001\n",
      "mean reward (100 episodes) -70.307850\n",
      "best mean reward -70.307850\n",
      "running time 475.009503\n",
      "Train_EnvstepsSoFar : 100001\n",
      "Train_AverageReturn : -70.30784973902453\n",
      "Train_BestReturn : -70.30784973902453\n",
      "TimeSinceStart : 475.0095031261444\n",
      "Training Loss : 0.1778167337179184\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 101001\n",
      "mean reward (100 episodes) -67.920749\n",
      "best mean reward -67.920749\n",
      "running time 479.863029\n",
      "Train_EnvstepsSoFar : 101001\n",
      "Train_AverageReturn : -67.9207492405377\n",
      "Train_BestReturn : -67.9207492405377\n",
      "TimeSinceStart : 479.8630290031433\n",
      "Training Loss : 0.1866457462310791\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 102001\n",
      "mean reward (100 episodes) -67.596795\n",
      "best mean reward -67.596795\n",
      "running time 484.532094\n",
      "Train_EnvstepsSoFar : 102001\n",
      "Train_AverageReturn : -67.5967954299077\n",
      "Train_BestReturn : -67.5967954299077\n",
      "TimeSinceStart : 484.53209400177\n",
      "Training Loss : 0.11765626817941666\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 103001\n",
      "mean reward (100 episodes) -63.758129\n",
      "best mean reward -63.758129\n",
      "running time 488.518965\n",
      "Train_EnvstepsSoFar : 103001\n",
      "Train_AverageReturn : -63.75812872850517\n",
      "Train_BestReturn : -63.75812872850517\n",
      "TimeSinceStart : 488.51896500587463\n",
      "Training Loss : 0.30818769335746765\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 104001\n",
      "mean reward (100 episodes) -63.403405\n",
      "best mean reward -63.403405\n",
      "running time 493.235621\n",
      "Train_EnvstepsSoFar : 104001\n",
      "Train_AverageReturn : -63.40340549411832\n",
      "Train_BestReturn : -63.40340549411832\n",
      "TimeSinceStart : 493.2356209754944\n",
      "Training Loss : 0.2645208239555359\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 105001\n",
      "mean reward (100 episodes) -63.706059\n",
      "best mean reward -63.403405\n",
      "running time 498.007240\n",
      "Train_EnvstepsSoFar : 105001\n",
      "Train_AverageReturn : -63.706058837945314\n",
      "Train_BestReturn : -63.40340549411832\n",
      "TimeSinceStart : 498.0072400569916\n",
      "Training Loss : 0.1304817944765091\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 106001\n",
      "mean reward (100 episodes) -63.424276\n",
      "best mean reward -63.403405\n",
      "running time 504.029190\n",
      "Train_EnvstepsSoFar : 106001\n",
      "Train_AverageReturn : -63.42427623426918\n",
      "Train_BestReturn : -63.40340549411832\n",
      "TimeSinceStart : 504.029189825058\n",
      "Training Loss : 0.511613130569458\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 107001\n",
      "mean reward (100 episodes) -61.608637\n",
      "best mean reward -61.608637\n",
      "running time 508.202487\n",
      "Train_EnvstepsSoFar : 107001\n",
      "Train_AverageReturn : -61.6086366717729\n",
      "Train_BestReturn : -61.6086366717729\n",
      "TimeSinceStart : 508.2024869918823\n",
      "Training Loss : 0.22528569400310516\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 108001\n",
      "mean reward (100 episodes) -58.166952\n",
      "best mean reward -58.166952\n",
      "running time 513.229275\n",
      "Train_EnvstepsSoFar : 108001\n",
      "Train_AverageReturn : -58.166951869147844\n",
      "Train_BestReturn : -58.166951869147844\n",
      "TimeSinceStart : 513.2292749881744\n",
      "Training Loss : 0.23199288547039032\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 109001\n",
      "mean reward (100 episodes) -53.232925\n",
      "best mean reward -53.232925\n",
      "running time 517.841447\n",
      "Train_EnvstepsSoFar : 109001\n",
      "Train_AverageReturn : -53.23292547131895\n",
      "Train_BestReturn : -53.23292547131895\n",
      "TimeSinceStart : 517.8414468765259\n",
      "Training Loss : 0.23750576376914978\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 110001\n",
      "mean reward (100 episodes) -52.703919\n",
      "best mean reward -52.703919\n",
      "running time 522.537152\n",
      "Train_EnvstepsSoFar : 110001\n",
      "Train_AverageReturn : -52.70391937248301\n",
      "Train_BestReturn : -52.70391937248301\n",
      "TimeSinceStart : 522.5371520519257\n",
      "Training Loss : 0.3751595914363861\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 111001\n",
      "mean reward (100 episodes) -48.645611\n",
      "best mean reward -48.645611\n",
      "running time 526.779515\n",
      "Train_EnvstepsSoFar : 111001\n",
      "Train_AverageReturn : -48.64561062323345\n",
      "Train_BestReturn : -48.64561062323345\n",
      "TimeSinceStart : 526.7795150279999\n",
      "Training Loss : 0.24281558394432068\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 112001\n",
      "mean reward (100 episodes) -47.640977\n",
      "best mean reward -47.640977\n",
      "running time 531.705193\n",
      "Train_EnvstepsSoFar : 112001\n",
      "Train_AverageReturn : -47.640977382073444\n",
      "Train_BestReturn : -47.640977382073444\n",
      "TimeSinceStart : 531.7051930427551\n",
      "Training Loss : 0.23693722486495972\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 113001\n",
      "mean reward (100 episodes) -47.725393\n",
      "best mean reward -47.640977\n",
      "running time 536.661111\n",
      "Train_EnvstepsSoFar : 113001\n",
      "Train_AverageReturn : -47.72539259929722\n",
      "Train_BestReturn : -47.640977382073444\n",
      "TimeSinceStart : 536.6611111164093\n",
      "Training Loss : 0.2941364645957947\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 114001\n",
      "mean reward (100 episodes) -47.021463\n",
      "best mean reward -47.021463\n",
      "running time 541.631473\n",
      "Train_EnvstepsSoFar : 114001\n",
      "Train_AverageReturn : -47.02146286915218\n",
      "Train_BestReturn : -47.02146286915218\n",
      "TimeSinceStart : 541.631472826004\n",
      "Training Loss : 0.2607343792915344\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 115001\n",
      "mean reward (100 episodes) -43.062780\n",
      "best mean reward -43.062780\n",
      "running time 546.196687\n",
      "Train_EnvstepsSoFar : 115001\n",
      "Train_AverageReturn : -43.06278033184448\n",
      "Train_BestReturn : -43.06278033184448\n",
      "TimeSinceStart : 546.1966869831085\n",
      "Training Loss : 0.4050770103931427\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 116001\n",
      "mean reward (100 episodes) -41.237783\n",
      "best mean reward -41.237783\n",
      "running time 551.686354\n",
      "Train_EnvstepsSoFar : 116001\n",
      "Train_AverageReturn : -41.23778259258408\n",
      "Train_BestReturn : -41.23778259258408\n",
      "TimeSinceStart : 551.6863539218903\n",
      "Training Loss : 0.3872072100639343\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 117001\n",
      "mean reward (100 episodes) -37.869009\n",
      "best mean reward -37.869009\n",
      "running time 556.281872\n",
      "Train_EnvstepsSoFar : 117001\n",
      "Train_AverageReturn : -37.86900935579997\n",
      "Train_BestReturn : -37.86900935579997\n",
      "TimeSinceStart : 556.2818717956543\n",
      "Training Loss : 0.34269118309020996\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 118001\n",
      "mean reward (100 episodes) -35.914872\n",
      "best mean reward -35.914872\n",
      "running time 560.534816\n",
      "Train_EnvstepsSoFar : 118001\n",
      "Train_AverageReturn : -35.91487183563164\n",
      "Train_BestReturn : -35.91487183563164\n",
      "TimeSinceStart : 560.5348160266876\n",
      "Training Loss : 0.8898090124130249\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 119001\n",
      "mean reward (100 episodes) -31.462015\n",
      "best mean reward -31.462015\n",
      "running time 565.483051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 119001\n",
      "Train_AverageReturn : -31.462014834901012\n",
      "Train_BestReturn : -31.462014834901012\n",
      "TimeSinceStart : 565.4830510616302\n",
      "Training Loss : 0.5311988592147827\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 120001\n",
      "mean reward (100 episodes) -28.874590\n",
      "best mean reward -28.874590\n",
      "running time 569.844695\n",
      "Train_EnvstepsSoFar : 120001\n",
      "Train_AverageReturn : -28.87459023671616\n",
      "Train_BestReturn : -28.87459023671616\n",
      "TimeSinceStart : 569.8446950912476\n",
      "Training Loss : 0.13801506161689758\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 121001\n",
      "mean reward (100 episodes) -28.735348\n",
      "best mean reward -28.735348\n",
      "running time 574.774840\n",
      "Train_EnvstepsSoFar : 121001\n",
      "Train_AverageReturn : -28.73534796955171\n",
      "Train_BestReturn : -28.73534796955171\n",
      "TimeSinceStart : 574.7748398780823\n",
      "Training Loss : 0.12755540013313293\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 122001\n",
      "mean reward (100 episodes) -25.610255\n",
      "best mean reward -25.610255\n",
      "running time 579.143756\n",
      "Train_EnvstepsSoFar : 122001\n",
      "Train_AverageReturn : -25.610255212716382\n",
      "Train_BestReturn : -25.610255212716382\n",
      "TimeSinceStart : 579.1437561511993\n",
      "Training Loss : 0.58760005235672\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 123001\n",
      "mean reward (100 episodes) -21.909254\n",
      "best mean reward -21.909254\n",
      "running time 583.093227\n",
      "Train_EnvstepsSoFar : 123001\n",
      "Train_AverageReturn : -21.90925426903842\n",
      "Train_BestReturn : -21.90925426903842\n",
      "TimeSinceStart : 583.093227148056\n",
      "Training Loss : 0.12746822834014893\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 124001\n",
      "mean reward (100 episodes) -15.270284\n",
      "best mean reward -15.270284\n",
      "running time 587.326811\n",
      "Train_EnvstepsSoFar : 124001\n",
      "Train_AverageReturn : -15.270284173020682\n",
      "Train_BestReturn : -15.270284173020682\n",
      "TimeSinceStart : 587.326810836792\n",
      "Training Loss : 0.18705914914608002\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 125001\n",
      "mean reward (100 episodes) -15.017893\n",
      "best mean reward -15.017893\n",
      "running time 592.564653\n",
      "Train_EnvstepsSoFar : 125001\n",
      "Train_AverageReturn : -15.017893473145328\n",
      "Train_BestReturn : -15.017893473145328\n",
      "TimeSinceStart : 592.5646531581879\n",
      "Training Loss : 0.796134352684021\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 126001\n",
      "mean reward (100 episodes) -15.323674\n",
      "best mean reward -15.017893\n",
      "running time 597.616744\n",
      "Train_EnvstepsSoFar : 126001\n",
      "Train_AverageReturn : -15.32367436964974\n",
      "Train_BestReturn : -15.017893473145328\n",
      "TimeSinceStart : 597.6167440414429\n",
      "Training Loss : 0.4683833122253418\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 127001\n",
      "mean reward (100 episodes) -12.042239\n",
      "best mean reward -12.042239\n",
      "running time 602.026395\n",
      "Train_EnvstepsSoFar : 127001\n",
      "Train_AverageReturn : -12.042239346869993\n",
      "Train_BestReturn : -12.042239346869993\n",
      "TimeSinceStart : 602.0263948440552\n",
      "Training Loss : 0.2628740966320038\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 128001\n",
      "mean reward (100 episodes) -10.421171\n",
      "best mean reward -10.421171\n",
      "running time 606.672856\n",
      "Train_EnvstepsSoFar : 128001\n",
      "Train_AverageReturn : -10.421170726074363\n",
      "Train_BestReturn : -10.421170726074363\n",
      "TimeSinceStart : 606.6728558540344\n",
      "Training Loss : 0.15369680523872375\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 129001\n",
      "mean reward (100 episodes) -10.670220\n",
      "best mean reward -10.421171\n",
      "running time 611.639891\n",
      "Train_EnvstepsSoFar : 129001\n",
      "Train_AverageReturn : -10.670219811123939\n",
      "Train_BestReturn : -10.421170726074363\n",
      "TimeSinceStart : 611.639890909195\n",
      "Training Loss : 0.10933098196983337\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 130001\n",
      "mean reward (100 episodes) -7.706333\n",
      "best mean reward -7.706333\n",
      "running time 616.208783\n",
      "Train_EnvstepsSoFar : 130001\n",
      "Train_AverageReturn : -7.70633303590759\n",
      "Train_BestReturn : -7.70633303590759\n",
      "TimeSinceStart : 616.2087829113007\n",
      "Training Loss : 0.158122256398201\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 131001\n",
      "mean reward (100 episodes) -5.353332\n",
      "best mean reward -5.353332\n",
      "running time 620.759363\n",
      "Train_EnvstepsSoFar : 131001\n",
      "Train_AverageReturn : -5.353332099850627\n",
      "Train_BestReturn : -5.353332099850627\n",
      "TimeSinceStart : 620.7593629360199\n",
      "Training Loss : 0.18373113870620728\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 132001\n",
      "mean reward (100 episodes) -5.492772\n",
      "best mean reward -5.353332\n",
      "running time 626.128067\n",
      "Train_EnvstepsSoFar : 132001\n",
      "Train_AverageReturn : -5.492771709002863\n",
      "Train_BestReturn : -5.353332099850627\n",
      "TimeSinceStart : 626.128066778183\n",
      "Training Loss : 0.09559901803731918\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 133001\n",
      "mean reward (100 episodes) -5.750290\n",
      "best mean reward -5.353332\n",
      "running time 631.773930\n",
      "Train_EnvstepsSoFar : 133001\n",
      "Train_AverageReturn : -5.750290007182538\n",
      "Train_BestReturn : -5.353332099850627\n",
      "TimeSinceStart : 631.7739300727844\n",
      "Training Loss : 0.17094998061656952\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 134001\n",
      "mean reward (100 episodes) -5.298607\n",
      "best mean reward -5.298607\n",
      "running time 636.908760\n",
      "Train_EnvstepsSoFar : 134001\n",
      "Train_AverageReturn : -5.298607345375667\n",
      "Train_BestReturn : -5.298607345375667\n",
      "TimeSinceStart : 636.9087598323822\n",
      "Training Loss : 0.15830372273921967\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 135001\n",
      "mean reward (100 episodes) -1.779255\n",
      "best mean reward -1.779255\n",
      "running time 642.585300\n",
      "Train_EnvstepsSoFar : 135001\n",
      "Train_AverageReturn : -1.7792545786633134\n",
      "Train_BestReturn : -1.7792545786633134\n",
      "TimeSinceStart : 642.5852999687195\n",
      "Training Loss : 0.14963483810424805\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 136001\n",
      "mean reward (100 episodes) 4.057133\n",
      "best mean reward 4.057133\n",
      "running time 646.613869\n",
      "Train_EnvstepsSoFar : 136001\n",
      "Train_AverageReturn : 4.057133132697038\n",
      "Train_BestReturn : 4.057133132697038\n",
      "TimeSinceStart : 646.6138691902161\n",
      "Training Loss : 0.115826815366745\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 137001\n",
      "mean reward (100 episodes) 8.780822\n",
      "best mean reward 8.780822\n",
      "running time 650.863661\n",
      "Train_EnvstepsSoFar : 137001\n",
      "Train_AverageReturn : 8.780821766575936\n",
      "Train_BestReturn : 8.780821766575936\n",
      "TimeSinceStart : 650.8636610507965\n",
      "Training Loss : 0.17871230840682983\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 138001\n",
      "mean reward (100 episodes) 14.079201\n",
      "best mean reward 14.079201\n",
      "running time 655.144275\n",
      "Train_EnvstepsSoFar : 138001\n",
      "Train_AverageReturn : 14.079201335495686\n",
      "Train_BestReturn : 14.079201335495686\n",
      "TimeSinceStart : 655.1442749500275\n",
      "Training Loss : 0.07213803380727768\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 139001\n",
      "mean reward (100 episodes) 16.706886\n",
      "best mean reward 16.706886\n",
      "running time 660.031980\n",
      "Train_EnvstepsSoFar : 139001\n",
      "Train_AverageReturn : 16.706886068490636\n",
      "Train_BestReturn : 16.706886068490636\n",
      "TimeSinceStart : 660.0319800376892\n",
      "Training Loss : 0.5803687572479248\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 140001\n",
      "mean reward (100 episodes) 17.678491\n",
      "best mean reward 17.678491\n",
      "running time 665.774098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 140001\n",
      "Train_AverageReturn : 17.678490576927025\n",
      "Train_BestReturn : 17.678490576927025\n",
      "TimeSinceStart : 665.7740979194641\n",
      "Training Loss : 0.3348587155342102\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 141001\n",
      "mean reward (100 episodes) 21.299537\n",
      "best mean reward 21.299537\n",
      "running time 669.821908\n",
      "Train_EnvstepsSoFar : 141001\n",
      "Train_AverageReturn : 21.29953704907351\n",
      "Train_BestReturn : 21.29953704907351\n",
      "TimeSinceStart : 669.8219079971313\n",
      "Training Loss : 0.28934594988822937\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 142001\n",
      "mean reward (100 episodes) 22.324878\n",
      "best mean reward 22.324878\n",
      "running time 674.881321\n",
      "Train_EnvstepsSoFar : 142001\n",
      "Train_AverageReturn : 22.324878451793683\n",
      "Train_BestReturn : 22.324878451793683\n",
      "TimeSinceStart : 674.8813209533691\n",
      "Training Loss : 0.16667698323726654\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 143001\n",
      "mean reward (100 episodes) 26.482564\n",
      "best mean reward 26.482564\n",
      "running time 679.801385\n",
      "Train_EnvstepsSoFar : 143001\n",
      "Train_AverageReturn : 26.482563772227937\n",
      "Train_BestReturn : 26.482563772227937\n",
      "TimeSinceStart : 679.8013849258423\n",
      "Training Loss : 0.2753537893295288\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 144001\n",
      "mean reward (100 episodes) 25.878609\n",
      "best mean reward 26.482564\n",
      "running time 684.811104\n",
      "Train_EnvstepsSoFar : 144001\n",
      "Train_AverageReturn : 25.878609278838955\n",
      "Train_BestReturn : 26.482563772227937\n",
      "TimeSinceStart : 684.8111040592194\n",
      "Training Loss : 0.15014119446277618\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 145001\n",
      "mean reward (100 episodes) 25.966028\n",
      "best mean reward 26.482564\n",
      "running time 689.265245\n",
      "Train_EnvstepsSoFar : 145001\n",
      "Train_AverageReturn : 25.966028374121215\n",
      "Train_BestReturn : 26.482563772227937\n",
      "TimeSinceStart : 689.2652449607849\n",
      "Training Loss : 0.12661702930927277\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 146001\n",
      "mean reward (100 episodes) 27.754306\n",
      "best mean reward 27.754306\n",
      "running time 693.942420\n",
      "Train_EnvstepsSoFar : 146001\n",
      "Train_AverageReturn : 27.754305601023972\n",
      "Train_BestReturn : 27.754305601023972\n",
      "TimeSinceStart : 693.9424200057983\n",
      "Training Loss : 0.13961488008499146\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 147001\n",
      "mean reward (100 episodes) 26.325279\n",
      "best mean reward 27.754306\n",
      "running time 698.746737\n",
      "Train_EnvstepsSoFar : 147001\n",
      "Train_AverageReturn : 26.325279061414292\n",
      "Train_BestReturn : 27.754305601023972\n",
      "TimeSinceStart : 698.7467370033264\n",
      "Training Loss : 0.2542712390422821\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 148001\n",
      "mean reward (100 episodes) 30.600215\n",
      "best mean reward 30.600215\n",
      "running time 703.462246\n",
      "Train_EnvstepsSoFar : 148001\n",
      "Train_AverageReturn : 30.60021465881742\n",
      "Train_BestReturn : 30.60021465881742\n",
      "TimeSinceStart : 703.4622459411621\n",
      "Training Loss : 0.3839350938796997\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 149001\n",
      "mean reward (100 episodes) 34.694038\n",
      "best mean reward 34.694038\n",
      "running time 707.401513\n",
      "Train_EnvstepsSoFar : 149001\n",
      "Train_AverageReturn : 34.69403814596056\n",
      "Train_BestReturn : 34.69403814596056\n",
      "TimeSinceStart : 707.4015130996704\n",
      "Training Loss : 0.2915512025356293\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 150001\n",
      "mean reward (100 episodes) 32.607510\n",
      "best mean reward 34.694038\n",
      "running time 712.063055\n",
      "Train_EnvstepsSoFar : 150001\n",
      "Train_AverageReturn : 32.6075099243431\n",
      "Train_BestReturn : 34.69403814596056\n",
      "TimeSinceStart : 712.0630548000336\n",
      "Training Loss : 0.13708250224590302\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 151001\n",
      "mean reward (100 episodes) 32.649450\n",
      "best mean reward 34.694038\n",
      "running time 716.560275\n",
      "Train_EnvstepsSoFar : 151001\n",
      "Train_AverageReturn : 32.64944974789634\n",
      "Train_BestReturn : 34.69403814596056\n",
      "TimeSinceStart : 716.5602750778198\n",
      "Training Loss : 0.7178993225097656\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 152001\n",
      "mean reward (100 episodes) 30.297050\n",
      "best mean reward 34.694038\n",
      "running time 721.857870\n",
      "Train_EnvstepsSoFar : 152001\n",
      "Train_AverageReturn : 30.297049826839093\n",
      "Train_BestReturn : 34.69403814596056\n",
      "TimeSinceStart : 721.8578698635101\n",
      "Training Loss : 0.2260468304157257\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 153001\n",
      "mean reward (100 episodes) 29.680714\n",
      "best mean reward 34.694038\n",
      "running time 726.062169\n",
      "Train_EnvstepsSoFar : 153001\n",
      "Train_AverageReturn : 29.680714247060582\n",
      "Train_BestReturn : 34.69403814596056\n",
      "TimeSinceStart : 726.0621688365936\n",
      "Training Loss : 0.11300954967737198\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 154001\n",
      "mean reward (100 episodes) 29.546103\n",
      "best mean reward 34.694038\n",
      "running time 730.689550\n",
      "Train_EnvstepsSoFar : 154001\n",
      "Train_AverageReturn : 29.546102512571156\n",
      "Train_BestReturn : 34.69403814596056\n",
      "TimeSinceStart : 730.6895499229431\n",
      "Training Loss : 0.41952937841415405\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 155001\n",
      "mean reward (100 episodes) 34.142467\n",
      "best mean reward 34.694038\n",
      "running time 734.906690\n",
      "Train_EnvstepsSoFar : 155001\n",
      "Train_AverageReturn : 34.142466599668246\n",
      "Train_BestReturn : 34.69403814596056\n",
      "TimeSinceStart : 734.906690120697\n",
      "Training Loss : 0.23618507385253906\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 156001\n",
      "mean reward (100 episodes) 39.188117\n",
      "best mean reward 39.188117\n",
      "running time 738.967793\n",
      "Train_EnvstepsSoFar : 156001\n",
      "Train_AverageReturn : 39.18811678576687\n",
      "Train_BestReturn : 39.18811678576687\n",
      "TimeSinceStart : 738.9677929878235\n",
      "Training Loss : 0.3432217538356781\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 157001\n",
      "mean reward (100 episodes) 39.905294\n",
      "best mean reward 39.905294\n",
      "running time 743.535235\n",
      "Train_EnvstepsSoFar : 157001\n",
      "Train_AverageReturn : 39.90529409112835\n",
      "Train_BestReturn : 39.90529409112835\n",
      "TimeSinceStart : 743.5352349281311\n",
      "Training Loss : 0.2856321334838867\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 158001\n",
      "mean reward (100 episodes) 39.453859\n",
      "best mean reward 39.905294\n",
      "running time 748.109525\n",
      "Train_EnvstepsSoFar : 158001\n",
      "Train_AverageReturn : 39.453859000812265\n",
      "Train_BestReturn : 39.90529409112835\n",
      "TimeSinceStart : 748.1095249652863\n",
      "Training Loss : 0.17049583792686462\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 159001\n",
      "mean reward (100 episodes) 39.439902\n",
      "best mean reward 39.905294\n",
      "running time 752.254808\n",
      "Train_EnvstepsSoFar : 159001\n",
      "Train_AverageReturn : 39.43990238648155\n",
      "Train_BestReturn : 39.90529409112835\n",
      "TimeSinceStart : 752.2548079490662\n",
      "Training Loss : 0.13055557012557983\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 160001\n",
      "mean reward (100 episodes) 45.924122\n",
      "best mean reward 45.924122\n",
      "running time 756.447958\n",
      "Train_EnvstepsSoFar : 160001\n",
      "Train_AverageReturn : 45.92412222184969\n",
      "Train_BestReturn : 45.92412222184969\n",
      "TimeSinceStart : 756.4479579925537\n",
      "Training Loss : 0.1697009652853012\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 161001\n",
      "mean reward (100 episodes) 46.658703\n",
      "best mean reward 46.658703\n",
      "running time 760.429567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 161001\n",
      "Train_AverageReturn : 46.658702821473696\n",
      "Train_BestReturn : 46.658702821473696\n",
      "TimeSinceStart : 760.4295670986176\n",
      "Training Loss : 0.15436209738254547\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 162001\n",
      "mean reward (100 episodes) 49.304016\n",
      "best mean reward 49.304016\n",
      "running time 764.871473\n",
      "Train_EnvstepsSoFar : 162001\n",
      "Train_AverageReturn : 49.304016102237675\n",
      "Train_BestReturn : 49.304016102237675\n",
      "TimeSinceStart : 764.8714728355408\n",
      "Training Loss : 0.8248716592788696\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 163001\n",
      "mean reward (100 episodes) 46.527713\n",
      "best mean reward 49.304016\n",
      "running time 768.812597\n",
      "Train_EnvstepsSoFar : 163001\n",
      "Train_AverageReturn : 46.52771275897785\n",
      "Train_BestReturn : 49.304016102237675\n",
      "TimeSinceStart : 768.8125970363617\n",
      "Training Loss : 0.38871175050735474\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 164001\n",
      "mean reward (100 episodes) 53.232707\n",
      "best mean reward 53.232707\n",
      "running time 772.887604\n",
      "Train_EnvstepsSoFar : 164001\n",
      "Train_AverageReturn : 53.232706538261944\n",
      "Train_BestReturn : 53.232706538261944\n",
      "TimeSinceStart : 772.8876039981842\n",
      "Training Loss : 0.2723201811313629\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 165001\n",
      "mean reward (100 episodes) 55.527069\n",
      "best mean reward 55.527069\n",
      "running time 777.230615\n",
      "Train_EnvstepsSoFar : 165001\n",
      "Train_AverageReturn : 55.52706851755279\n",
      "Train_BestReturn : 55.52706851755279\n",
      "TimeSinceStart : 777.2306151390076\n",
      "Training Loss : 0.05410439148545265\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 166001\n",
      "mean reward (100 episodes) 55.476157\n",
      "best mean reward 55.527069\n",
      "running time 783.247702\n",
      "Train_EnvstepsSoFar : 166001\n",
      "Train_AverageReturn : 55.47615653378152\n",
      "Train_BestReturn : 55.52706851755279\n",
      "TimeSinceStart : 783.2477021217346\n",
      "Training Loss : 0.11057861894369125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 167001\n",
      "mean reward (100 episodes) 53.417728\n",
      "best mean reward 55.527069\n",
      "running time 787.969657\n",
      "Train_EnvstepsSoFar : 167001\n",
      "Train_AverageReturn : 53.41772837575391\n",
      "Train_BestReturn : 55.52706851755279\n",
      "TimeSinceStart : 787.9696569442749\n",
      "Training Loss : 0.10562184453010559\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 168001\n",
      "mean reward (100 episodes) 54.561798\n",
      "best mean reward 55.527069\n",
      "running time 793.155190\n",
      "Train_EnvstepsSoFar : 168001\n",
      "Train_AverageReturn : 54.56179772644242\n",
      "Train_BestReturn : 55.52706851755279\n",
      "TimeSinceStart : 793.1551899909973\n",
      "Training Loss : 0.21864262223243713\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 169001\n",
      "mean reward (100 episodes) 53.862912\n",
      "best mean reward 55.527069\n",
      "running time 798.885698\n",
      "Train_EnvstepsSoFar : 169001\n",
      "Train_AverageReturn : 53.86291170437775\n",
      "Train_BestReturn : 55.52706851755279\n",
      "TimeSinceStart : 798.8856980800629\n",
      "Training Loss : 3.779308795928955\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 170001\n",
      "mean reward (100 episodes) 53.688820\n",
      "best mean reward 55.527069\n",
      "running time 804.172661\n",
      "Train_EnvstepsSoFar : 170001\n",
      "Train_AverageReturn : 53.68881990253123\n",
      "Train_BestReturn : 55.52706851755279\n",
      "TimeSinceStart : 804.1726610660553\n",
      "Training Loss : 0.7025796175003052\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 171001\n",
      "mean reward (100 episodes) 54.612274\n",
      "best mean reward 55.527069\n",
      "running time 811.061087\n",
      "Train_EnvstepsSoFar : 171001\n",
      "Train_AverageReturn : 54.612274392315086\n",
      "Train_BestReturn : 55.52706851755279\n",
      "TimeSinceStart : 811.0610868930817\n",
      "Training Loss : 0.5050600171089172\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 172001\n",
      "mean reward (100 episodes) 56.695745\n",
      "best mean reward 56.695745\n",
      "running time 816.132763\n",
      "Train_EnvstepsSoFar : 172001\n",
      "Train_AverageReturn : 56.6957450336082\n",
      "Train_BestReturn : 56.6957450336082\n",
      "TimeSinceStart : 816.1327629089355\n",
      "Training Loss : 0.1310565322637558\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 173001\n",
      "mean reward (100 episodes) 56.538074\n",
      "best mean reward 56.695745\n",
      "running time 821.274731\n",
      "Train_EnvstepsSoFar : 173001\n",
      "Train_AverageReturn : 56.53807429423121\n",
      "Train_BestReturn : 56.6957450336082\n",
      "TimeSinceStart : 821.2747309207916\n",
      "Training Loss : 0.15668731927871704\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 174001\n",
      "mean reward (100 episodes) 57.002752\n",
      "best mean reward 57.002752\n",
      "running time 827.857820\n",
      "Train_EnvstepsSoFar : 174001\n",
      "Train_AverageReturn : 57.002751805357505\n",
      "Train_BestReturn : 57.002751805357505\n",
      "TimeSinceStart : 827.8578200340271\n",
      "Training Loss : 0.07442478835582733\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 175001\n",
      "mean reward (100 episodes) 59.032969\n",
      "best mean reward 59.032969\n",
      "running time 834.468562\n",
      "Train_EnvstepsSoFar : 175001\n",
      "Train_AverageReturn : 59.03296885089925\n",
      "Train_BestReturn : 59.03296885089925\n",
      "TimeSinceStart : 834.4685618877411\n",
      "Training Loss : 0.1521841436624527\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 176001\n",
      "mean reward (100 episodes) 54.786544\n",
      "best mean reward 59.032969\n",
      "running time 839.826554\n",
      "Train_EnvstepsSoFar : 176001\n",
      "Train_AverageReturn : 54.786543563774465\n",
      "Train_BestReturn : 59.03296885089925\n",
      "TimeSinceStart : 839.8265538215637\n",
      "Training Loss : 0.2926311492919922\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 177001\n",
      "mean reward (100 episodes) 60.323255\n",
      "best mean reward 60.323255\n",
      "running time 845.060187\n",
      "Train_EnvstepsSoFar : 177001\n",
      "Train_AverageReturn : 60.32325502061056\n",
      "Train_BestReturn : 60.32325502061056\n",
      "TimeSinceStart : 845.0601868629456\n",
      "Training Loss : 0.06285528838634491\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 178001\n",
      "mean reward (100 episodes) 60.378643\n",
      "best mean reward 60.378643\n",
      "running time 849.545743\n",
      "Train_EnvstepsSoFar : 178001\n",
      "Train_AverageReturn : 60.37864307523524\n",
      "Train_BestReturn : 60.37864307523524\n",
      "TimeSinceStart : 849.5457429885864\n",
      "Training Loss : 0.161239355802536\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 179001\n",
      "mean reward (100 episodes) 57.972044\n",
      "best mean reward 60.378643\n",
      "running time 854.264951\n",
      "Train_EnvstepsSoFar : 179001\n",
      "Train_AverageReturn : 57.97204371312302\n",
      "Train_BestReturn : 60.37864307523524\n",
      "TimeSinceStart : 854.2649509906769\n",
      "Training Loss : 0.08100897818803787\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 180001\n",
      "mean reward (100 episodes) 53.898961\n",
      "best mean reward 60.378643\n",
      "running time 858.983510\n",
      "Train_EnvstepsSoFar : 180001\n",
      "Train_AverageReturn : 53.898960923334606\n",
      "Train_BestReturn : 60.37864307523524\n",
      "TimeSinceStart : 858.983510017395\n",
      "Training Loss : 1.3169974088668823\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 181001\n",
      "mean reward (100 episodes) 55.105968\n",
      "best mean reward 60.378643\n",
      "running time 863.240376\n",
      "Train_EnvstepsSoFar : 181001\n",
      "Train_AverageReturn : 55.10596844319343\n",
      "Train_BestReturn : 60.37864307523524\n",
      "TimeSinceStart : 863.2403757572174\n",
      "Training Loss : 2.824610471725464\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 182001\n",
      "mean reward (100 episodes) 55.283013\n",
      "best mean reward 60.378643\n",
      "running time 867.277946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 182001\n",
      "Train_AverageReturn : 55.283013124363194\n",
      "Train_BestReturn : 60.37864307523524\n",
      "TimeSinceStart : 867.2779459953308\n",
      "Training Loss : 0.324370801448822\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 183001\n",
      "mean reward (100 episodes) 56.578148\n",
      "best mean reward 60.378643\n",
      "running time 871.497681\n",
      "Train_EnvstepsSoFar : 183001\n",
      "Train_AverageReturn : 56.578148183258335\n",
      "Train_BestReturn : 60.37864307523524\n",
      "TimeSinceStart : 871.4976809024811\n",
      "Training Loss : 0.11987411230802536\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 184001\n",
      "mean reward (100 episodes) 57.392839\n",
      "best mean reward 60.378643\n",
      "running time 875.792369\n",
      "Train_EnvstepsSoFar : 184001\n",
      "Train_AverageReturn : 57.39283931752638\n",
      "Train_BestReturn : 60.37864307523524\n",
      "TimeSinceStart : 875.792368888855\n",
      "Training Loss : 0.10790576785802841\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 185001\n",
      "mean reward (100 episodes) 60.319176\n",
      "best mean reward 60.378643\n",
      "running time 880.749747\n",
      "Train_EnvstepsSoFar : 185001\n",
      "Train_AverageReturn : 60.31917641545628\n",
      "Train_BestReturn : 60.37864307523524\n",
      "TimeSinceStart : 880.7497470378876\n",
      "Training Loss : 0.10452752560377121\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 186001\n",
      "mean reward (100 episodes) 61.749072\n",
      "best mean reward 61.749072\n",
      "running time 885.874878\n",
      "Train_EnvstepsSoFar : 186001\n",
      "Train_AverageReturn : 61.74907224094897\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 885.8748779296875\n",
      "Training Loss : 0.08318129926919937\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 187001\n",
      "mean reward (100 episodes) 58.315406\n",
      "best mean reward 61.749072\n",
      "running time 890.450712\n",
      "Train_EnvstepsSoFar : 187001\n",
      "Train_AverageReturn : 58.31540640109543\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 890.4507117271423\n",
      "Training Loss : 0.2911255359649658\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 188001\n",
      "mean reward (100 episodes) 56.521429\n",
      "best mean reward 61.749072\n",
      "running time 894.511809\n",
      "Train_EnvstepsSoFar : 188001\n",
      "Train_AverageReturn : 56.52142913516921\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 894.5118091106415\n",
      "Training Loss : 0.6613560318946838\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 189001\n",
      "mean reward (100 episodes) 54.863174\n",
      "best mean reward 61.749072\n",
      "running time 899.375466\n",
      "Train_EnvstepsSoFar : 189001\n",
      "Train_AverageReturn : 54.863173731180794\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 899.3754661083221\n",
      "Training Loss : 0.07474341988563538\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 190001\n",
      "mean reward (100 episodes) 54.280701\n",
      "best mean reward 61.749072\n",
      "running time 903.895140\n",
      "Train_EnvstepsSoFar : 190001\n",
      "Train_AverageReturn : 54.28070065975087\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 903.8951399326324\n",
      "Training Loss : 0.19608868658542633\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 191001\n",
      "mean reward (100 episodes) 59.165775\n",
      "best mean reward 61.749072\n",
      "running time 907.968245\n",
      "Train_EnvstepsSoFar : 191001\n",
      "Train_AverageReturn : 59.16577533716129\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 907.9682450294495\n",
      "Training Loss : 0.10630634427070618\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 192001\n",
      "mean reward (100 episodes) 56.361994\n",
      "best mean reward 61.749072\n",
      "running time 912.802549\n",
      "Train_EnvstepsSoFar : 192001\n",
      "Train_AverageReturn : 56.36199384724276\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 912.802549123764\n",
      "Training Loss : 0.07929135859012604\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 193001\n",
      "mean reward (100 episodes) 59.357492\n",
      "best mean reward 61.749072\n",
      "running time 916.669703\n",
      "Train_EnvstepsSoFar : 193001\n",
      "Train_AverageReturn : 59.35749164773896\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 916.6697030067444\n",
      "Training Loss : 0.543283998966217\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 194001\n",
      "mean reward (100 episodes) 58.213676\n",
      "best mean reward 61.749072\n",
      "running time 921.458357\n",
      "Train_EnvstepsSoFar : 194001\n",
      "Train_AverageReturn : 58.21367624197378\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 921.4583570957184\n",
      "Training Loss : 0.18867328763008118\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 195001\n",
      "mean reward (100 episodes) 61.161367\n",
      "best mean reward 61.749072\n",
      "running time 926.345701\n",
      "Train_EnvstepsSoFar : 195001\n",
      "Train_AverageReturn : 61.16136697626658\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 926.3457009792328\n",
      "Training Loss : 0.08175693452358246\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 196001\n",
      "mean reward (100 episodes) 57.999147\n",
      "best mean reward 61.749072\n",
      "running time 930.881058\n",
      "Train_EnvstepsSoFar : 196001\n",
      "Train_AverageReturn : 57.99914729656166\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 930.8810579776764\n",
      "Training Loss : 0.28771498799324036\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 197001\n",
      "mean reward (100 episodes) 61.372012\n",
      "best mean reward 61.749072\n",
      "running time 934.748400\n",
      "Train_EnvstepsSoFar : 197001\n",
      "Train_AverageReturn : 61.37201213106735\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 934.7483999729156\n",
      "Training Loss : 0.08624116331338882\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 198001\n",
      "mean reward (100 episodes) 60.455346\n",
      "best mean reward 61.749072\n",
      "running time 939.091792\n",
      "Train_EnvstepsSoFar : 198001\n",
      "Train_AverageReturn : 60.455345937005006\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 939.0917918682098\n",
      "Training Loss : 0.057378098368644714\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 199001\n",
      "mean reward (100 episodes) 60.614735\n",
      "best mean reward 61.749072\n",
      "running time 943.955054\n",
      "Train_EnvstepsSoFar : 199001\n",
      "Train_AverageReturn : 60.6147354479577\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 943.9550538063049\n",
      "Training Loss : 0.039828699082136154\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 200001\n",
      "mean reward (100 episodes) 60.575969\n",
      "best mean reward 61.749072\n",
      "running time 948.777514\n",
      "Train_EnvstepsSoFar : 200001\n",
      "Train_AverageReturn : 60.57596877753358\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 948.7775139808655\n",
      "Training Loss : 0.08464764803647995\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 201001\n",
      "mean reward (100 episodes) 57.905304\n",
      "best mean reward 61.749072\n",
      "running time 953.864191\n",
      "Train_EnvstepsSoFar : 201001\n",
      "Train_AverageReturn : 57.90530382279572\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 953.8641910552979\n",
      "Training Loss : 0.10959351807832718\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 202001\n",
      "mean reward (100 episodes) 56.508244\n",
      "best mean reward 61.749072\n",
      "running time 958.633775\n",
      "Train_EnvstepsSoFar : 202001\n",
      "Train_AverageReturn : 56.50824423085566\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 958.6337749958038\n",
      "Training Loss : 0.6166450381278992\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 203001\n",
      "mean reward (100 episodes) 58.177166\n",
      "best mean reward 61.749072\n",
      "running time 962.685161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 203001\n",
      "Train_AverageReturn : 58.177166253464755\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 962.6851608753204\n",
      "Training Loss : 0.27471429109573364\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 204001\n",
      "mean reward (100 episodes) 56.629506\n",
      "best mean reward 61.749072\n",
      "running time 967.910075\n",
      "Train_EnvstepsSoFar : 204001\n",
      "Train_AverageReturn : 56.629506016494446\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 967.9100749492645\n",
      "Training Loss : 0.3135134279727936\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 205001\n",
      "mean reward (100 episodes) 58.640884\n",
      "best mean reward 61.749072\n",
      "running time 973.618285\n",
      "Train_EnvstepsSoFar : 205001\n",
      "Train_AverageReturn : 58.640884047751015\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 973.6182851791382\n",
      "Training Loss : 0.07075856626033783\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 206001\n",
      "mean reward (100 episodes) 55.555510\n",
      "best mean reward 61.749072\n",
      "running time 978.473077\n",
      "Train_EnvstepsSoFar : 206001\n",
      "Train_AverageReturn : 55.5555104153816\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 978.4730770587921\n",
      "Training Loss : 1.1585922241210938\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 207001\n",
      "mean reward (100 episodes) 55.518739\n",
      "best mean reward 61.749072\n",
      "running time 983.070923\n",
      "Train_EnvstepsSoFar : 207001\n",
      "Train_AverageReturn : 55.51873935063041\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 983.0709230899811\n",
      "Training Loss : 0.12623222172260284\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 208001\n",
      "mean reward (100 episodes) 54.574258\n",
      "best mean reward 61.749072\n",
      "running time 988.463098\n",
      "Train_EnvstepsSoFar : 208001\n",
      "Train_AverageReturn : 54.57425770745167\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 988.4630978107452\n",
      "Training Loss : 0.7542012929916382\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 209001\n",
      "mean reward (100 episodes) 53.638603\n",
      "best mean reward 61.749072\n",
      "running time 993.590424\n",
      "Train_EnvstepsSoFar : 209001\n",
      "Train_AverageReturn : 53.63860310062267\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 993.590423822403\n",
      "Training Loss : 0.09539198130369186\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 210001\n",
      "mean reward (100 episodes) 54.971617\n",
      "best mean reward 61.749072\n",
      "running time 998.734252\n",
      "Train_EnvstepsSoFar : 210001\n",
      "Train_AverageReturn : 54.971616598286545\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 998.7342519760132\n",
      "Training Loss : 0.1306765079498291\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 211001\n",
      "mean reward (100 episodes) 53.458363\n",
      "best mean reward 61.749072\n",
      "running time 1003.585702\n",
      "Train_EnvstepsSoFar : 211001\n",
      "Train_AverageReturn : 53.458362745418434\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 1003.5857019424438\n",
      "Training Loss : 0.2171977311372757\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 212001\n",
      "mean reward (100 episodes) 53.680495\n",
      "best mean reward 61.749072\n",
      "running time 1007.931640\n",
      "Train_EnvstepsSoFar : 212001\n",
      "Train_AverageReturn : 53.68049540317734\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 1007.9316399097443\n",
      "Training Loss : 0.12283195555210114\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 213001\n",
      "mean reward (100 episodes) 53.462808\n",
      "best mean reward 61.749072\n",
      "running time 1012.184292\n",
      "Train_EnvstepsSoFar : 213001\n",
      "Train_AverageReturn : 53.46280781069776\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 1012.1842918395996\n",
      "Training Loss : 0.054169196635484695\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 214001\n",
      "mean reward (100 episodes) 56.161720\n",
      "best mean reward 61.749072\n",
      "running time 1016.537856\n",
      "Train_EnvstepsSoFar : 214001\n",
      "Train_AverageReturn : 56.161720267310905\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 1016.5378561019897\n",
      "Training Loss : 0.0819036215543747\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 215001\n",
      "mean reward (100 episodes) 51.121751\n",
      "best mean reward 61.749072\n",
      "running time 1020.536996\n",
      "Train_EnvstepsSoFar : 215001\n",
      "Train_AverageReturn : 51.12175064640435\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 1020.5369958877563\n",
      "Training Loss : 0.1184840276837349\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 216001\n",
      "mean reward (100 episodes) 51.336330\n",
      "best mean reward 61.749072\n",
      "running time 1026.195249\n",
      "Train_EnvstepsSoFar : 216001\n",
      "Train_AverageReturn : 51.33633017553539\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 1026.1952488422394\n",
      "Training Loss : 2.2480862140655518\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 217001\n",
      "mean reward (100 episodes) 52.597687\n",
      "best mean reward 61.749072\n",
      "running time 1031.330598\n",
      "Train_EnvstepsSoFar : 217001\n",
      "Train_AverageReturn : 52.597686624373246\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 1031.3305978775024\n",
      "Training Loss : 0.1661374568939209\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 218001\n",
      "mean reward (100 episodes) 52.571468\n",
      "best mean reward 61.749072\n",
      "running time 1035.573508\n",
      "Train_EnvstepsSoFar : 218001\n",
      "Train_AverageReturn : 52.571468420584644\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 1035.5735077857971\n",
      "Training Loss : 0.10663816332817078\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 219001\n",
      "mean reward (100 episodes) 50.408086\n",
      "best mean reward 61.749072\n",
      "running time 1040.162180\n",
      "Train_EnvstepsSoFar : 219001\n",
      "Train_AverageReturn : 50.40808611766452\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 1040.1621799468994\n",
      "Training Loss : 0.1334783434867859\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 220001\n",
      "mean reward (100 episodes) 53.011584\n",
      "best mean reward 61.749072\n",
      "running time 1045.236102\n",
      "Train_EnvstepsSoFar : 220001\n",
      "Train_AverageReturn : 53.011584305034376\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 1045.2361018657684\n",
      "Training Loss : 0.06665974855422974\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 221001\n",
      "mean reward (100 episodes) 55.174798\n",
      "best mean reward 61.749072\n",
      "running time 1050.222315\n",
      "Train_EnvstepsSoFar : 221001\n",
      "Train_AverageReturn : 55.17479805062063\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 1050.2223150730133\n",
      "Training Loss : 0.08660966157913208\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 222001\n",
      "mean reward (100 episodes) 58.428199\n",
      "best mean reward 61.749072\n",
      "running time 1055.244792\n",
      "Train_EnvstepsSoFar : 222001\n",
      "Train_AverageReturn : 58.42819855891168\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 1055.2447917461395\n",
      "Training Loss : 0.3083075284957886\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 223001\n",
      "mean reward (100 episodes) 60.194243\n",
      "best mean reward 61.749072\n",
      "running time 1059.808627\n",
      "Train_EnvstepsSoFar : 223001\n",
      "Train_AverageReturn : 60.19424281906976\n",
      "Train_BestReturn : 61.74907224094897\n",
      "TimeSinceStart : 1059.8086268901825\n",
      "Training Loss : 0.07881143689155579\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 224001\n",
      "mean reward (100 episodes) 64.352991\n",
      "best mean reward 64.352991\n",
      "running time 1064.286170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 224001\n",
      "Train_AverageReturn : 64.35299128695638\n",
      "Train_BestReturn : 64.35299128695638\n",
      "TimeSinceStart : 1064.2861700057983\n",
      "Training Loss : 0.07396013289690018\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 225001\n",
      "mean reward (100 episodes) 68.941797\n",
      "best mean reward 68.941797\n",
      "running time 1068.101576\n",
      "Train_EnvstepsSoFar : 225001\n",
      "Train_AverageReturn : 68.94179679750329\n",
      "Train_BestReturn : 68.94179679750329\n",
      "TimeSinceStart : 1068.1015758514404\n",
      "Training Loss : 0.08676868677139282\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 226001\n",
      "mean reward (100 episodes) 67.157239\n",
      "best mean reward 68.941797\n",
      "running time 1073.313509\n",
      "Train_EnvstepsSoFar : 226001\n",
      "Train_AverageReturn : 67.15723878158747\n",
      "Train_BestReturn : 68.94179679750329\n",
      "TimeSinceStart : 1073.3135089874268\n",
      "Training Loss : 0.07635335624217987\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 227001\n",
      "mean reward (100 episodes) 67.545385\n",
      "best mean reward 68.941797\n",
      "running time 1079.093063\n",
      "Train_EnvstepsSoFar : 227001\n",
      "Train_AverageReturn : 67.54538542118684\n",
      "Train_BestReturn : 68.94179679750329\n",
      "TimeSinceStart : 1079.093062877655\n",
      "Training Loss : 0.38569357991218567\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 228001\n",
      "mean reward (100 episodes) 65.187392\n",
      "best mean reward 68.941797\n",
      "running time 1083.605686\n",
      "Train_EnvstepsSoFar : 228001\n",
      "Train_AverageReturn : 65.18739190161804\n",
      "Train_BestReturn : 68.94179679750329\n",
      "TimeSinceStart : 1083.6056859493256\n",
      "Training Loss : 0.26064783334732056\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 229001\n",
      "mean reward (100 episodes) 65.719430\n",
      "best mean reward 68.941797\n",
      "running time 1088.492008\n",
      "Train_EnvstepsSoFar : 229001\n",
      "Train_AverageReturn : 65.71942981055963\n",
      "Train_BestReturn : 68.94179679750329\n",
      "TimeSinceStart : 1088.49200797081\n",
      "Training Loss : 0.2505370080471039\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 230001\n",
      "mean reward (100 episodes) 67.650118\n",
      "best mean reward 68.941797\n",
      "running time 1092.758974\n",
      "Train_EnvstepsSoFar : 230001\n",
      "Train_AverageReturn : 67.65011800035845\n",
      "Train_BestReturn : 68.94179679750329\n",
      "TimeSinceStart : 1092.7589740753174\n",
      "Training Loss : 0.4270652234554291\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 231001\n",
      "mean reward (100 episodes) 69.684095\n",
      "best mean reward 69.684095\n",
      "running time 1097.222410\n",
      "Train_EnvstepsSoFar : 231001\n",
      "Train_AverageReturn : 69.68409529034679\n",
      "Train_BestReturn : 69.68409529034679\n",
      "TimeSinceStart : 1097.2224099636078\n",
      "Training Loss : 0.12680767476558685\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 232001\n",
      "mean reward (100 episodes) 71.556712\n",
      "best mean reward 71.556712\n",
      "running time 1101.619951\n",
      "Train_EnvstepsSoFar : 232001\n",
      "Train_AverageReturn : 71.55671198088221\n",
      "Train_BestReturn : 71.55671198088221\n",
      "TimeSinceStart : 1101.6199507713318\n",
      "Training Loss : 0.07547462731599808\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 233001\n",
      "mean reward (100 episodes) 77.639899\n",
      "best mean reward 77.639899\n",
      "running time 1106.319823\n",
      "Train_EnvstepsSoFar : 233001\n",
      "Train_AverageReturn : 77.63989910166104\n",
      "Train_BestReturn : 77.63989910166104\n",
      "TimeSinceStart : 1106.3198227882385\n",
      "Training Loss : 0.07027518004179001\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 234001\n",
      "mean reward (100 episodes) 77.208907\n",
      "best mean reward 77.639899\n",
      "running time 1110.924472\n",
      "Train_EnvstepsSoFar : 234001\n",
      "Train_AverageReturn : 77.20890690200649\n",
      "Train_BestReturn : 77.63989910166104\n",
      "TimeSinceStart : 1110.9244718551636\n",
      "Training Loss : 0.17894762754440308\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 235001\n",
      "mean reward (100 episodes) 79.198851\n",
      "best mean reward 79.198851\n",
      "running time 1115.330249\n",
      "Train_EnvstepsSoFar : 235001\n",
      "Train_AverageReturn : 79.19885112019708\n",
      "Train_BestReturn : 79.19885112019708\n",
      "TimeSinceStart : 1115.3302490711212\n",
      "Training Loss : 0.0933452919125557\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 236001\n",
      "mean reward (100 episodes) 78.164865\n",
      "best mean reward 79.198851\n",
      "running time 1120.067029\n",
      "Train_EnvstepsSoFar : 236001\n",
      "Train_AverageReturn : 78.16486462193379\n",
      "Train_BestReturn : 79.19885112019708\n",
      "TimeSinceStart : 1120.0670289993286\n",
      "Training Loss : 0.39579838514328003\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 237001\n",
      "mean reward (100 episodes) 78.255427\n",
      "best mean reward 79.198851\n",
      "running time 1124.473567\n",
      "Train_EnvstepsSoFar : 237001\n",
      "Train_AverageReturn : 78.25542688115307\n",
      "Train_BestReturn : 79.19885112019708\n",
      "TimeSinceStart : 1124.4735670089722\n",
      "Training Loss : 0.30942225456237793\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 238001\n",
      "mean reward (100 episodes) 80.423861\n",
      "best mean reward 80.423861\n",
      "running time 1128.498281\n",
      "Train_EnvstepsSoFar : 238001\n",
      "Train_AverageReturn : 80.42386134315512\n",
      "Train_BestReturn : 80.42386134315512\n",
      "TimeSinceStart : 1128.4982810020447\n",
      "Training Loss : 0.09982345998287201\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 239001\n",
      "mean reward (100 episodes) 83.907338\n",
      "best mean reward 83.907338\n",
      "running time 1132.682144\n",
      "Train_EnvstepsSoFar : 239001\n",
      "Train_AverageReturn : 83.90733781654595\n",
      "Train_BestReturn : 83.90733781654595\n",
      "TimeSinceStart : 1132.6821439266205\n",
      "Training Loss : 1.1476596593856812\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 240001\n",
      "mean reward (100 episodes) 82.567014\n",
      "best mean reward 83.907338\n",
      "running time 1137.364150\n",
      "Train_EnvstepsSoFar : 240001\n",
      "Train_AverageReturn : 82.56701432442976\n",
      "Train_BestReturn : 83.90733781654595\n",
      "TimeSinceStart : 1137.3641500473022\n",
      "Training Loss : 0.6120758652687073\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 241001\n",
      "mean reward (100 episodes) 87.818356\n",
      "best mean reward 87.818356\n",
      "running time 1141.426705\n",
      "Train_EnvstepsSoFar : 241001\n",
      "Train_AverageReturn : 87.81835612549794\n",
      "Train_BestReturn : 87.81835612549794\n",
      "TimeSinceStart : 1141.4267048835754\n",
      "Training Loss : 0.05918761342763901\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 242001\n",
      "mean reward (100 episodes) 85.372412\n",
      "best mean reward 87.818356\n",
      "running time 1146.401948\n",
      "Train_EnvstepsSoFar : 242001\n",
      "Train_AverageReturn : 85.37241241011114\n",
      "Train_BestReturn : 87.81835612549794\n",
      "TimeSinceStart : 1146.4019479751587\n",
      "Training Loss : 0.08538886904716492\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 243001\n",
      "mean reward (100 episodes) 84.574507\n",
      "best mean reward 87.818356\n",
      "running time 1150.893937\n",
      "Train_EnvstepsSoFar : 243001\n",
      "Train_AverageReturn : 84.57450743889554\n",
      "Train_BestReturn : 87.81835612549794\n",
      "TimeSinceStart : 1150.8939368724823\n",
      "Training Loss : 0.30503448843955994\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 244001\n",
      "mean reward (100 episodes) 81.919200\n",
      "best mean reward 87.818356\n",
      "running time 1156.362506\n",
      "Train_EnvstepsSoFar : 244001\n",
      "Train_AverageReturn : 81.91920012193064\n",
      "Train_BestReturn : 87.81835612549794\n",
      "TimeSinceStart : 1156.3625059127808\n",
      "Training Loss : 0.06927119195461273\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 245001\n",
      "mean reward (100 episodes) 83.058068\n",
      "best mean reward 87.818356\n",
      "running time 1160.899704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 245001\n",
      "Train_AverageReturn : 83.05806822475547\n",
      "Train_BestReturn : 87.81835612549794\n",
      "TimeSinceStart : 1160.8997037410736\n",
      "Training Loss : 0.13402274250984192\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 246001\n",
      "mean reward (100 episodes) 82.673599\n",
      "best mean reward 87.818356\n",
      "running time 1165.322554\n",
      "Train_EnvstepsSoFar : 246001\n",
      "Train_AverageReturn : 82.67359900673542\n",
      "Train_BestReturn : 87.81835612549794\n",
      "TimeSinceStart : 1165.3225538730621\n",
      "Training Loss : 0.076202392578125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 247001\n",
      "mean reward (100 episodes) 81.127046\n",
      "best mean reward 87.818356\n",
      "running time 1169.990861\n",
      "Train_EnvstepsSoFar : 247001\n",
      "Train_AverageReturn : 81.1270463349923\n",
      "Train_BestReturn : 87.81835612549794\n",
      "TimeSinceStart : 1169.9908609390259\n",
      "Training Loss : 0.14393244683742523\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 248001\n",
      "mean reward (100 episodes) 81.162522\n",
      "best mean reward 87.818356\n",
      "running time 1174.953297\n",
      "Train_EnvstepsSoFar : 248001\n",
      "Train_AverageReturn : 81.1625219767373\n",
      "Train_BestReturn : 87.81835612549794\n",
      "TimeSinceStart : 1174.9532968997955\n",
      "Training Loss : 0.10381174087524414\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 249001\n",
      "mean reward (100 episodes) 81.621718\n",
      "best mean reward 87.818356\n",
      "running time 1180.083408\n",
      "Train_EnvstepsSoFar : 249001\n",
      "Train_AverageReturn : 81.62171838198613\n",
      "Train_BestReturn : 87.81835612549794\n",
      "TimeSinceStart : 1180.0834078788757\n",
      "Training Loss : 0.15960527956485748\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 250001\n",
      "mean reward (100 episodes) 83.477973\n",
      "best mean reward 87.818356\n",
      "running time 1183.966661\n",
      "Train_EnvstepsSoFar : 250001\n",
      "Train_AverageReturn : 83.4779729188045\n",
      "Train_BestReturn : 87.81835612549794\n",
      "TimeSinceStart : 1183.96666097641\n",
      "Training Loss : 0.5213491916656494\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 251001\n",
      "mean reward (100 episodes) 81.568039\n",
      "best mean reward 87.818356\n",
      "running time 1189.080585\n",
      "Train_EnvstepsSoFar : 251001\n",
      "Train_AverageReturn : 81.56803906036194\n",
      "Train_BestReturn : 87.81835612549794\n",
      "TimeSinceStart : 1189.0805850028992\n",
      "Training Loss : 0.1227361187338829\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 252001\n",
      "mean reward (100 episodes) 86.768306\n",
      "best mean reward 87.818356\n",
      "running time 1194.199625\n",
      "Train_EnvstepsSoFar : 252001\n",
      "Train_AverageReturn : 86.76830554211706\n",
      "Train_BestReturn : 87.81835612549794\n",
      "TimeSinceStart : 1194.1996250152588\n",
      "Training Loss : 0.06513717770576477\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 253001\n",
      "mean reward (100 episodes) 86.859318\n",
      "best mean reward 87.818356\n",
      "running time 1199.157043\n",
      "Train_EnvstepsSoFar : 253001\n",
      "Train_AverageReturn : 86.85931753577839\n",
      "Train_BestReturn : 87.81835612549794\n",
      "TimeSinceStart : 1199.157042980194\n",
      "Training Loss : 0.9218758344650269\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 254001\n",
      "mean reward (100 episodes) 87.229107\n",
      "best mean reward 87.818356\n",
      "running time 1206.232618\n",
      "Train_EnvstepsSoFar : 254001\n",
      "Train_AverageReturn : 87.2291068838779\n",
      "Train_BestReturn : 87.81835612549794\n",
      "TimeSinceStart : 1206.2326180934906\n",
      "Training Loss : 0.18106357753276825\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 255001\n",
      "mean reward (100 episodes) 87.462117\n",
      "best mean reward 87.818356\n",
      "running time 1211.250895\n",
      "Train_EnvstepsSoFar : 255001\n",
      "Train_AverageReturn : 87.46211735920917\n",
      "Train_BestReturn : 87.81835612549794\n",
      "TimeSinceStart : 1211.2508947849274\n",
      "Training Loss : 0.2631077766418457\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 256001\n",
      "mean reward (100 episodes) 87.374761\n",
      "best mean reward 87.818356\n",
      "running time 1216.997321\n",
      "Train_EnvstepsSoFar : 256001\n",
      "Train_AverageReturn : 87.37476060414019\n",
      "Train_BestReturn : 87.81835612549794\n",
      "TimeSinceStart : 1216.9973208904266\n",
      "Training Loss : 0.12016411125659943\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 257001\n",
      "mean reward (100 episodes) 95.645401\n",
      "best mean reward 95.645401\n",
      "running time 1221.464053\n",
      "Train_EnvstepsSoFar : 257001\n",
      "Train_AverageReturn : 95.6454012246274\n",
      "Train_BestReturn : 95.6454012246274\n",
      "TimeSinceStart : 1221.4640529155731\n",
      "Training Loss : 0.829509437084198\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 258001\n",
      "mean reward (100 episodes) 93.526383\n",
      "best mean reward 95.645401\n",
      "running time 1226.577322\n",
      "Train_EnvstepsSoFar : 258001\n",
      "Train_AverageReturn : 93.5263828938896\n",
      "Train_BestReturn : 95.6454012246274\n",
      "TimeSinceStart : 1226.5773220062256\n",
      "Training Loss : 0.18462282419204712\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 259001\n",
      "mean reward (100 episodes) 96.111632\n",
      "best mean reward 96.111632\n",
      "running time 1231.221931\n",
      "Train_EnvstepsSoFar : 259001\n",
      "Train_AverageReturn : 96.11163233129626\n",
      "Train_BestReturn : 96.11163233129626\n",
      "TimeSinceStart : 1231.2219309806824\n",
      "Training Loss : 0.09341372549533844\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 260001\n",
      "mean reward (100 episodes) 95.741014\n",
      "best mean reward 96.111632\n",
      "running time 1236.832199\n",
      "Train_EnvstepsSoFar : 260001\n",
      "Train_AverageReturn : 95.74101360058785\n",
      "Train_BestReturn : 96.11163233129626\n",
      "TimeSinceStart : 1236.832198858261\n",
      "Training Loss : 0.170026034116745\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 261001\n",
      "mean reward (100 episodes) 93.612709\n",
      "best mean reward 96.111632\n",
      "running time 1241.429098\n",
      "Train_EnvstepsSoFar : 261001\n",
      "Train_AverageReturn : 93.61270887460901\n",
      "Train_BestReturn : 96.11163233129626\n",
      "TimeSinceStart : 1241.4290981292725\n",
      "Training Loss : 0.12742818892002106\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 262001\n",
      "mean reward (100 episodes) 91.625805\n",
      "best mean reward 96.111632\n",
      "running time 1245.827627\n",
      "Train_EnvstepsSoFar : 262001\n",
      "Train_AverageReturn : 91.62580469465478\n",
      "Train_BestReturn : 96.11163233129626\n",
      "TimeSinceStart : 1245.8276269435883\n",
      "Training Loss : 0.616551399230957\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 263001\n",
      "mean reward (100 episodes) 100.091419\n",
      "best mean reward 100.091419\n",
      "running time 1250.204619\n",
      "Train_EnvstepsSoFar : 263001\n",
      "Train_AverageReturn : 100.09141911277864\n",
      "Train_BestReturn : 100.09141911277864\n",
      "TimeSinceStart : 1250.2046189308167\n",
      "Training Loss : 0.07462149113416672\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 264001\n",
      "mean reward (100 episodes) 98.402129\n",
      "best mean reward 100.091419\n",
      "running time 1255.024416\n",
      "Train_EnvstepsSoFar : 264001\n",
      "Train_AverageReturn : 98.40212889398845\n",
      "Train_BestReturn : 100.09141911277864\n",
      "TimeSinceStart : 1255.0244159698486\n",
      "Training Loss : 0.17029209434986115\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 265001\n",
      "mean reward (100 episodes) 95.387641\n",
      "best mean reward 100.091419\n",
      "running time 1260.359004\n",
      "Train_EnvstepsSoFar : 265001\n",
      "Train_AverageReturn : 95.38764094499213\n",
      "Train_BestReturn : 100.09141911277864\n",
      "TimeSinceStart : 1260.359004020691\n",
      "Training Loss : 0.10614876449108124\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 266001\n",
      "mean reward (100 episodes) 97.299543\n",
      "best mean reward 100.091419\n",
      "running time 1264.578172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 266001\n",
      "Train_AverageReturn : 97.29954314832132\n",
      "Train_BestReturn : 100.09141911277864\n",
      "TimeSinceStart : 1264.57817196846\n",
      "Training Loss : 0.0925464928150177\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 267001\n",
      "mean reward (100 episodes) 98.843703\n",
      "best mean reward 100.091419\n",
      "running time 1268.612122\n",
      "Train_EnvstepsSoFar : 267001\n",
      "Train_AverageReturn : 98.84370349326305\n",
      "Train_BestReturn : 100.09141911277864\n",
      "TimeSinceStart : 1268.6121218204498\n",
      "Training Loss : 0.17558664083480835\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 268001\n",
      "mean reward (100 episodes) 98.131027\n",
      "best mean reward 100.091419\n",
      "running time 1272.857733\n",
      "Train_EnvstepsSoFar : 268001\n",
      "Train_AverageReturn : 98.13102660398968\n",
      "Train_BestReturn : 100.09141911277864\n",
      "TimeSinceStart : 1272.8577330112457\n",
      "Training Loss : 0.17761093378067017\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 269001\n",
      "mean reward (100 episodes) 97.361264\n",
      "best mean reward 100.091419\n",
      "running time 1276.624280\n",
      "Train_EnvstepsSoFar : 269001\n",
      "Train_AverageReturn : 97.3612639831562\n",
      "Train_BestReturn : 100.09141911277864\n",
      "TimeSinceStart : 1276.6242797374725\n",
      "Training Loss : 0.13743290305137634\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 270001\n",
      "mean reward (100 episodes) 96.248902\n",
      "best mean reward 100.091419\n",
      "running time 1280.610648\n",
      "Train_EnvstepsSoFar : 270001\n",
      "Train_AverageReturn : 96.24890182654289\n",
      "Train_BestReturn : 100.09141911277864\n",
      "TimeSinceStart : 1280.6106479167938\n",
      "Training Loss : 0.14792770147323608\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 271001\n",
      "mean reward (100 episodes) 93.630763\n",
      "best mean reward 100.091419\n",
      "running time 1286.891776\n",
      "Train_EnvstepsSoFar : 271001\n",
      "Train_AverageReturn : 93.63076255634607\n",
      "Train_BestReturn : 100.09141911277864\n",
      "TimeSinceStart : 1286.8917760849\n",
      "Training Loss : 0.17033040523529053\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 272001\n",
      "mean reward (100 episodes) 103.058849\n",
      "best mean reward 103.058849\n",
      "running time 1291.152977\n",
      "Train_EnvstepsSoFar : 272001\n",
      "Train_AverageReturn : 103.05884896663953\n",
      "Train_BestReturn : 103.05884896663953\n",
      "TimeSinceStart : 1291.152976989746\n",
      "Training Loss : 0.46955907344818115\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 273001\n",
      "mean reward (100 episodes) 100.203074\n",
      "best mean reward 103.058849\n",
      "running time 1295.467517\n",
      "Train_EnvstepsSoFar : 273001\n",
      "Train_AverageReturn : 100.20307445014268\n",
      "Train_BestReturn : 103.05884896663953\n",
      "TimeSinceStart : 1295.467516899109\n",
      "Training Loss : 3.1798036098480225\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 274001\n",
      "mean reward (100 episodes) 99.528510\n",
      "best mean reward 103.058849\n",
      "running time 1299.481781\n",
      "Train_EnvstepsSoFar : 274001\n",
      "Train_AverageReturn : 99.52850985872062\n",
      "Train_BestReturn : 103.05884896663953\n",
      "TimeSinceStart : 1299.4817810058594\n",
      "Training Loss : 0.37999647855758667\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 275001\n",
      "mean reward (100 episodes) 96.453086\n",
      "best mean reward 103.058849\n",
      "running time 1303.670083\n",
      "Train_EnvstepsSoFar : 275001\n",
      "Train_AverageReturn : 96.45308564644418\n",
      "Train_BestReturn : 103.05884896663953\n",
      "TimeSinceStart : 1303.670082807541\n",
      "Training Loss : 0.14456215500831604\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 276001\n",
      "mean reward (100 episodes) 97.034917\n",
      "best mean reward 103.058849\n",
      "running time 1307.834089\n",
      "Train_EnvstepsSoFar : 276001\n",
      "Train_AverageReturn : 97.03491670269489\n",
      "Train_BestReturn : 103.05884896663953\n",
      "TimeSinceStart : 1307.8340890407562\n",
      "Training Loss : 2.2913966178894043\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 277001\n",
      "mean reward (100 episodes) 95.663114\n",
      "best mean reward 103.058849\n",
      "running time 1312.029125\n",
      "Train_EnvstepsSoFar : 277001\n",
      "Train_AverageReturn : 95.66311371580093\n",
      "Train_BestReturn : 103.05884896663953\n",
      "TimeSinceStart : 1312.0291249752045\n",
      "Training Loss : 0.1114702969789505\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 278001\n",
      "mean reward (100 episodes) 92.406910\n",
      "best mean reward 103.058849\n",
      "running time 1316.373911\n",
      "Train_EnvstepsSoFar : 278001\n",
      "Train_AverageReturn : 92.40691019440274\n",
      "Train_BestReturn : 103.05884896663953\n",
      "TimeSinceStart : 1316.3739111423492\n",
      "Training Loss : 0.20391763746738434\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 279001\n",
      "mean reward (100 episodes) 95.667153\n",
      "best mean reward 103.058849\n",
      "running time 1320.399317\n",
      "Train_EnvstepsSoFar : 279001\n",
      "Train_AverageReturn : 95.667152755963\n",
      "Train_BestReturn : 103.05884896663953\n",
      "TimeSinceStart : 1320.3993170261383\n",
      "Training Loss : 0.402700275182724\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 280001\n",
      "mean reward (100 episodes) 95.959379\n",
      "best mean reward 103.058849\n",
      "running time 1324.193093\n",
      "Train_EnvstepsSoFar : 280001\n",
      "Train_AverageReturn : 95.95937901002323\n",
      "Train_BestReturn : 103.05884896663953\n",
      "TimeSinceStart : 1324.1930928230286\n",
      "Training Loss : 0.4262996017932892\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 281001\n",
      "mean reward (100 episodes) 98.454402\n",
      "best mean reward 103.058849\n",
      "running time 1328.275873\n",
      "Train_EnvstepsSoFar : 281001\n",
      "Train_AverageReturn : 98.45440192899758\n",
      "Train_BestReturn : 103.05884896663953\n",
      "TimeSinceStart : 1328.2758729457855\n",
      "Training Loss : 3.3833985328674316\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 282001\n",
      "mean reward (100 episodes) 98.433477\n",
      "best mean reward 103.058849\n",
      "running time 1332.248335\n",
      "Train_EnvstepsSoFar : 282001\n",
      "Train_AverageReturn : 98.43347686178484\n",
      "Train_BestReturn : 103.05884896663953\n",
      "TimeSinceStart : 1332.2483348846436\n",
      "Training Loss : 0.26062649488449097\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 283001\n",
      "mean reward (100 episodes) 96.617954\n",
      "best mean reward 103.058849\n",
      "running time 1336.376066\n",
      "Train_EnvstepsSoFar : 283001\n",
      "Train_AverageReturn : 96.61795407654463\n",
      "Train_BestReturn : 103.05884896663953\n",
      "TimeSinceStart : 1336.3760659694672\n",
      "Training Loss : 0.1380475014448166\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 284001\n",
      "mean reward (100 episodes) 97.128257\n",
      "best mean reward 103.058849\n",
      "running time 1341.011982\n",
      "Train_EnvstepsSoFar : 284001\n",
      "Train_AverageReturn : 97.128256540332\n",
      "Train_BestReturn : 103.05884896663953\n",
      "TimeSinceStart : 1341.0119819641113\n",
      "Training Loss : 0.07524507492780685\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 285001\n",
      "mean reward (100 episodes) 95.572682\n",
      "best mean reward 103.058849\n",
      "running time 1345.440883\n",
      "Train_EnvstepsSoFar : 285001\n",
      "Train_AverageReturn : 95.57268241590444\n",
      "Train_BestReturn : 103.05884896663953\n",
      "TimeSinceStart : 1345.4408829212189\n",
      "Training Loss : 0.15333382785320282\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 286001\n",
      "mean reward (100 episodes) 100.090102\n",
      "best mean reward 103.058849\n",
      "running time 1349.559764\n",
      "Train_EnvstepsSoFar : 286001\n",
      "Train_AverageReturn : 100.09010195689416\n",
      "Train_BestReturn : 103.05884896663953\n",
      "TimeSinceStart : 1349.5597639083862\n",
      "Training Loss : 0.09016546607017517\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 287001\n",
      "mean reward (100 episodes) 98.177643\n",
      "best mean reward 103.058849\n",
      "running time 1353.711090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 287001\n",
      "Train_AverageReturn : 98.17764345655901\n",
      "Train_BestReturn : 103.05884896663953\n",
      "TimeSinceStart : 1353.711089849472\n",
      "Training Loss : 0.5750413537025452\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 288001\n",
      "mean reward (100 episodes) 100.629095\n",
      "best mean reward 103.058849\n",
      "running time 1357.820476\n",
      "Train_EnvstepsSoFar : 288001\n",
      "Train_AverageReturn : 100.62909468096058\n",
      "Train_BestReturn : 103.05884896663953\n",
      "TimeSinceStart : 1357.8204758167267\n",
      "Training Loss : 0.07169114798307419\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 289001\n",
      "mean reward (100 episodes) 102.298332\n",
      "best mean reward 103.058849\n",
      "running time 1362.302156\n",
      "Train_EnvstepsSoFar : 289001\n",
      "Train_AverageReturn : 102.29833247350729\n",
      "Train_BestReturn : 103.05884896663953\n",
      "TimeSinceStart : 1362.302155971527\n",
      "Training Loss : 0.268636018037796\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 290001\n",
      "mean reward (100 episodes) 101.992892\n",
      "best mean reward 103.058849\n",
      "running time 1366.552940\n",
      "Train_EnvstepsSoFar : 290001\n",
      "Train_AverageReturn : 101.99289169294505\n",
      "Train_BestReturn : 103.05884896663953\n",
      "TimeSinceStart : 1366.5529401302338\n",
      "Training Loss : 0.13409216701984406\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 291001\n",
      "mean reward (100 episodes) 104.456573\n",
      "best mean reward 104.456573\n",
      "running time 1371.187732\n",
      "Train_EnvstepsSoFar : 291001\n",
      "Train_AverageReturn : 104.45657271302564\n",
      "Train_BestReturn : 104.45657271302564\n",
      "TimeSinceStart : 1371.1877319812775\n",
      "Training Loss : 0.7596920728683472\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 292001\n",
      "mean reward (100 episodes) 104.144476\n",
      "best mean reward 104.456573\n",
      "running time 1375.316658\n",
      "Train_EnvstepsSoFar : 292001\n",
      "Train_AverageReturn : 104.1444756435088\n",
      "Train_BestReturn : 104.45657271302564\n",
      "TimeSinceStart : 1375.3166580200195\n",
      "Training Loss : 0.19845201075077057\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 293001\n",
      "mean reward (100 episodes) 106.096313\n",
      "best mean reward 106.096313\n",
      "running time 1379.907373\n",
      "Train_EnvstepsSoFar : 293001\n",
      "Train_AverageReturn : 106.09631263391158\n",
      "Train_BestReturn : 106.09631263391158\n",
      "TimeSinceStart : 1379.9073731899261\n",
      "Training Loss : 0.12474997341632843\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 294001\n",
      "mean reward (100 episodes) 105.962161\n",
      "best mean reward 106.096313\n",
      "running time 1384.637468\n",
      "Train_EnvstepsSoFar : 294001\n",
      "Train_AverageReturn : 105.96216117943372\n",
      "Train_BestReturn : 106.09631263391158\n",
      "TimeSinceStart : 1384.6374678611755\n",
      "Training Loss : 1.223004937171936\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 295001\n",
      "mean reward (100 episodes) 105.957660\n",
      "best mean reward 106.096313\n",
      "running time 1389.176994\n",
      "Train_EnvstepsSoFar : 295001\n",
      "Train_AverageReturn : 105.95765983629911\n",
      "Train_BestReturn : 106.09631263391158\n",
      "TimeSinceStart : 1389.176994085312\n",
      "Training Loss : 0.10721646249294281\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 296001\n",
      "mean reward (100 episodes) 105.260204\n",
      "best mean reward 106.096313\n",
      "running time 1393.399195\n",
      "Train_EnvstepsSoFar : 296001\n",
      "Train_AverageReturn : 105.26020434255817\n",
      "Train_BestReturn : 106.09631263391158\n",
      "TimeSinceStart : 1393.3991949558258\n",
      "Training Loss : 0.1630597710609436\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 297001\n",
      "mean reward (100 episodes) 108.735140\n",
      "best mean reward 108.735140\n",
      "running time 1397.687163\n",
      "Train_EnvstepsSoFar : 297001\n",
      "Train_AverageReturn : 108.73513977559034\n",
      "Train_BestReturn : 108.73513977559034\n",
      "TimeSinceStart : 1397.6871628761292\n",
      "Training Loss : 3.377565860748291\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 298001\n",
      "mean reward (100 episodes) 111.054048\n",
      "best mean reward 111.054048\n",
      "running time 1401.817393\n",
      "Train_EnvstepsSoFar : 298001\n",
      "Train_AverageReturn : 111.05404824106654\n",
      "Train_BestReturn : 111.05404824106654\n",
      "TimeSinceStart : 1401.8173928260803\n",
      "Training Loss : 1.1209499835968018\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 299001\n",
      "mean reward (100 episodes) 109.400299\n",
      "best mean reward 111.054048\n",
      "running time 1406.562277\n",
      "Train_EnvstepsSoFar : 299001\n",
      "Train_AverageReturn : 109.40029873782629\n",
      "Train_BestReturn : 111.05404824106654\n",
      "TimeSinceStart : 1406.56227684021\n",
      "Training Loss : 1.3740582466125488\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 300001\n",
      "mean reward (100 episodes) 106.437657\n",
      "best mean reward 111.054048\n",
      "running time 1411.886063\n",
      "Train_EnvstepsSoFar : 300001\n",
      "Train_AverageReturn : 106.43765735613074\n",
      "Train_BestReturn : 111.05404824106654\n",
      "TimeSinceStart : 1411.886062860489\n",
      "Training Loss : 0.3690662682056427\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 301001\n",
      "mean reward (100 episodes) 110.800383\n",
      "best mean reward 111.054048\n",
      "running time 1415.588115\n",
      "Train_EnvstepsSoFar : 301001\n",
      "Train_AverageReturn : 110.80038276692453\n",
      "Train_BestReturn : 111.05404824106654\n",
      "TimeSinceStart : 1415.588114976883\n",
      "Training Loss : 0.7742207050323486\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 302001\n",
      "mean reward (100 episodes) 113.478413\n",
      "best mean reward 113.478413\n",
      "running time 1419.392932\n",
      "Train_EnvstepsSoFar : 302001\n",
      "Train_AverageReturn : 113.4784128322057\n",
      "Train_BestReturn : 113.4784128322057\n",
      "TimeSinceStart : 1419.3929319381714\n",
      "Training Loss : 0.5086656212806702\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 303001\n",
      "mean reward (100 episodes) 110.693771\n",
      "best mean reward 113.478413\n",
      "running time 1423.516742\n",
      "Train_EnvstepsSoFar : 303001\n",
      "Train_AverageReturn : 110.69377145402017\n",
      "Train_BestReturn : 113.4784128322057\n",
      "TimeSinceStart : 1423.516741991043\n",
      "Training Loss : 0.5431360602378845\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 304001\n",
      "mean reward (100 episodes) 112.526760\n",
      "best mean reward 113.478413\n",
      "running time 1427.925620\n",
      "Train_EnvstepsSoFar : 304001\n",
      "Train_AverageReturn : 112.52675961435467\n",
      "Train_BestReturn : 113.4784128322057\n",
      "TimeSinceStart : 1427.9256200790405\n",
      "Training Loss : 0.08281712979078293\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 305001\n",
      "mean reward (100 episodes) 115.088645\n",
      "best mean reward 115.088645\n",
      "running time 1431.743937\n",
      "Train_EnvstepsSoFar : 305001\n",
      "Train_AverageReturn : 115.08864508395168\n",
      "Train_BestReturn : 115.08864508395168\n",
      "TimeSinceStart : 1431.7439370155334\n",
      "Training Loss : 0.30978235602378845\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 306001\n",
      "mean reward (100 episodes) 119.375590\n",
      "best mean reward 119.375590\n",
      "running time 1435.692470\n",
      "Train_EnvstepsSoFar : 306001\n",
      "Train_AverageReturn : 119.37558963255029\n",
      "Train_BestReturn : 119.37558963255029\n",
      "TimeSinceStart : 1435.6924700737\n",
      "Training Loss : 0.33760350942611694\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 307001\n",
      "mean reward (100 episodes) 123.465802\n",
      "best mean reward 123.465802\n",
      "running time 1439.877306\n",
      "Train_EnvstepsSoFar : 307001\n",
      "Train_AverageReturn : 123.46580186635167\n",
      "Train_BestReturn : 123.46580186635167\n",
      "TimeSinceStart : 1439.877305984497\n",
      "Training Loss : 0.26274579763412476\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 308001\n",
      "mean reward (100 episodes) 122.508583\n",
      "best mean reward 123.465802\n",
      "running time 1443.897312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 308001\n",
      "Train_AverageReturn : 122.50858347477974\n",
      "Train_BestReturn : 123.46580186635167\n",
      "TimeSinceStart : 1443.897311925888\n",
      "Training Loss : 0.34545692801475525\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 309001\n",
      "mean reward (100 episodes) 116.827799\n",
      "best mean reward 123.465802\n",
      "running time 1448.322484\n",
      "Train_EnvstepsSoFar : 309001\n",
      "Train_AverageReturn : 116.8277992114639\n",
      "Train_BestReturn : 123.46580186635167\n",
      "TimeSinceStart : 1448.3224840164185\n",
      "Training Loss : 0.33344218134880066\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 310001\n",
      "mean reward (100 episodes) 119.853353\n",
      "best mean reward 123.465802\n",
      "running time 1452.546265\n",
      "Train_EnvstepsSoFar : 310001\n",
      "Train_AverageReturn : 119.85335280823998\n",
      "Train_BestReturn : 123.46580186635167\n",
      "TimeSinceStart : 1452.546264886856\n",
      "Training Loss : 0.30405616760253906\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 311001\n",
      "mean reward (100 episodes) 122.868697\n",
      "best mean reward 123.465802\n",
      "running time 1456.929803\n",
      "Train_EnvstepsSoFar : 311001\n",
      "Train_AverageReturn : 122.86869710213085\n",
      "Train_BestReturn : 123.46580186635167\n",
      "TimeSinceStart : 1456.9298031330109\n",
      "Training Loss : 0.17570148408412933\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 312001\n",
      "mean reward (100 episodes) 127.196370\n",
      "best mean reward 127.196370\n",
      "running time 1462.750017\n",
      "Train_EnvstepsSoFar : 312001\n",
      "Train_AverageReturn : 127.19637001154697\n",
      "Train_BestReturn : 127.19637001154697\n",
      "TimeSinceStart : 1462.7500171661377\n",
      "Training Loss : 0.15696050226688385\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 313001\n",
      "mean reward (100 episodes) 122.236055\n",
      "best mean reward 127.196370\n",
      "running time 1466.881369\n",
      "Train_EnvstepsSoFar : 313001\n",
      "Train_AverageReturn : 122.23605511820213\n",
      "Train_BestReturn : 127.19637001154697\n",
      "TimeSinceStart : 1466.8813688755035\n",
      "Training Loss : 0.19105476140975952\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 314001\n",
      "mean reward (100 episodes) 121.423637\n",
      "best mean reward 127.196370\n",
      "running time 1470.952341\n",
      "Train_EnvstepsSoFar : 314001\n",
      "Train_AverageReturn : 121.42363693119512\n",
      "Train_BestReturn : 127.19637001154697\n",
      "TimeSinceStart : 1470.952341079712\n",
      "Training Loss : 0.29174333810806274\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 315001\n",
      "mean reward (100 episodes) 120.846917\n",
      "best mean reward 127.196370\n",
      "running time 1474.817333\n",
      "Train_EnvstepsSoFar : 315001\n",
      "Train_AverageReturn : 120.84691681655953\n",
      "Train_BestReturn : 127.19637001154697\n",
      "TimeSinceStart : 1474.817332983017\n",
      "Training Loss : 0.18300959467887878\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 316001\n",
      "mean reward (100 episodes) 118.480512\n",
      "best mean reward 127.196370\n",
      "running time 1479.058851\n",
      "Train_EnvstepsSoFar : 316001\n",
      "Train_AverageReturn : 118.48051183668\n",
      "Train_BestReturn : 127.19637001154697\n",
      "TimeSinceStart : 1479.0588507652283\n",
      "Training Loss : 0.1377394050359726\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 317001\n",
      "mean reward (100 episodes) 107.695788\n",
      "best mean reward 127.196370\n",
      "running time 1482.791496\n",
      "Train_EnvstepsSoFar : 317001\n",
      "Train_AverageReturn : 107.69578787560243\n",
      "Train_BestReturn : 127.19637001154697\n",
      "TimeSinceStart : 1482.791496038437\n",
      "Training Loss : 0.5327110290527344\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 318001\n",
      "mean reward (100 episodes) 100.037214\n",
      "best mean reward 127.196370\n",
      "running time 1486.529745\n",
      "Train_EnvstepsSoFar : 318001\n",
      "Train_AverageReturn : 100.03721408178257\n",
      "Train_BestReturn : 127.19637001154697\n",
      "TimeSinceStart : 1486.5297448635101\n",
      "Training Loss : 0.3737311363220215\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 319001\n",
      "mean reward (100 episodes) 96.064524\n",
      "best mean reward 127.196370\n",
      "running time 1490.290731\n",
      "Train_EnvstepsSoFar : 319001\n",
      "Train_AverageReturn : 96.06452390697383\n",
      "Train_BestReturn : 127.19637001154697\n",
      "TimeSinceStart : 1490.2907309532166\n",
      "Training Loss : 3.418113946914673\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 320001\n",
      "mean reward (100 episodes) 94.598642\n",
      "best mean reward 127.196370\n",
      "running time 1494.073545\n",
      "Train_EnvstepsSoFar : 320001\n",
      "Train_AverageReturn : 94.59864187159798\n",
      "Train_BestReturn : 127.19637001154697\n",
      "TimeSinceStart : 1494.0735449790955\n",
      "Training Loss : 0.7265368700027466\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 321001\n",
      "mean reward (100 episodes) 96.445901\n",
      "best mean reward 127.196370\n",
      "running time 1497.782357\n",
      "Train_EnvstepsSoFar : 321001\n",
      "Train_AverageReturn : 96.44590093421124\n",
      "Train_BestReturn : 127.19637001154697\n",
      "TimeSinceStart : 1497.7823569774628\n",
      "Training Loss : 0.8424091339111328\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 322001\n",
      "mean reward (100 episodes) 95.868422\n",
      "best mean reward 127.196370\n",
      "running time 1501.965985\n",
      "Train_EnvstepsSoFar : 322001\n",
      "Train_AverageReturn : 95.86842205340112\n",
      "Train_BestReturn : 127.19637001154697\n",
      "TimeSinceStart : 1501.9659850597382\n",
      "Training Loss : 0.35357555747032166\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 323001\n",
      "mean reward (100 episodes) 94.824977\n",
      "best mean reward 127.196370\n",
      "running time 1505.830678\n",
      "Train_EnvstepsSoFar : 323001\n",
      "Train_AverageReturn : 94.82497736572975\n",
      "Train_BestReturn : 127.19637001154697\n",
      "TimeSinceStart : 1505.830677986145\n",
      "Training Loss : 5.605563163757324\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 324001\n",
      "mean reward (100 episodes) 95.158870\n",
      "best mean reward 127.196370\n",
      "running time 1510.176416\n",
      "Train_EnvstepsSoFar : 324001\n",
      "Train_AverageReturn : 95.15886993752298\n",
      "Train_BestReturn : 127.19637001154697\n",
      "TimeSinceStart : 1510.1764159202576\n",
      "Training Loss : 0.1670795977115631\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 325001\n",
      "mean reward (100 episodes) 97.584126\n",
      "best mean reward 127.196370\n",
      "running time 1516.360076\n",
      "Train_EnvstepsSoFar : 325001\n",
      "Train_AverageReturn : 97.58412579042393\n",
      "Train_BestReturn : 127.19637001154697\n",
      "TimeSinceStart : 1516.3600759506226\n",
      "Training Loss : 0.3481596112251282\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 326001\n",
      "mean reward (100 episodes) 98.346521\n",
      "best mean reward 127.196370\n",
      "running time 1522.134734\n",
      "Train_EnvstepsSoFar : 326001\n",
      "Train_AverageReturn : 98.34652101117777\n",
      "Train_BestReturn : 127.19637001154697\n",
      "TimeSinceStart : 1522.134733915329\n",
      "Training Loss : 0.2205498069524765\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 327001\n",
      "mean reward (100 episodes) 96.542480\n",
      "best mean reward 127.196370\n",
      "running time 1527.005387\n",
      "Train_EnvstepsSoFar : 327001\n",
      "Train_AverageReturn : 96.54248020843517\n",
      "Train_BestReturn : 127.19637001154697\n",
      "TimeSinceStart : 1527.0053870677948\n",
      "Training Loss : 0.2034107744693756\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 328001\n",
      "mean reward (100 episodes) 94.952677\n",
      "best mean reward 127.196370\n",
      "running time 1532.284366\n",
      "Train_EnvstepsSoFar : 328001\n",
      "Train_AverageReturn : 94.95267708083456\n",
      "Train_BestReturn : 127.19637001154697\n",
      "TimeSinceStart : 1532.2843658924103\n",
      "Training Loss : 2.9909307956695557\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 329001\n",
      "mean reward (100 episodes) 93.732381\n",
      "best mean reward 127.196370\n",
      "running time 1537.054657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 329001\n",
      "Train_AverageReturn : 93.73238126658745\n",
      "Train_BestReturn : 127.19637001154697\n",
      "TimeSinceStart : 1537.0546569824219\n",
      "Training Loss : 3.0453031063079834\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 330001\n",
      "mean reward (100 episodes) 97.784415\n",
      "best mean reward 127.196370\n",
      "running time 1541.017111\n",
      "Train_EnvstepsSoFar : 330001\n",
      "Train_AverageReturn : 97.78441474854701\n",
      "Train_BestReturn : 127.19637001154697\n",
      "TimeSinceStart : 1541.0171110630035\n",
      "Training Loss : 0.4218275845050812\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 331001\n",
      "mean reward (100 episodes) 98.118947\n",
      "best mean reward 127.196370\n",
      "running time 1546.230082\n",
      "Train_EnvstepsSoFar : 331001\n",
      "Train_AverageReturn : 98.11894682087451\n",
      "Train_BestReturn : 127.19637001154697\n",
      "TimeSinceStart : 1546.2300820350647\n",
      "Training Loss : 0.08261734992265701\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 332001\n",
      "mean reward (100 episodes) 98.260756\n",
      "best mean reward 127.196370\n",
      "running time 1552.273753\n",
      "Train_EnvstepsSoFar : 332001\n",
      "Train_AverageReturn : 98.26075620970616\n",
      "Train_BestReturn : 127.19637001154697\n",
      "TimeSinceStart : 1552.2737529277802\n",
      "Training Loss : 1.086351752281189\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 333001\n",
      "mean reward (100 episodes) 93.638479\n",
      "best mean reward 127.196370\n",
      "running time 1557.161599\n",
      "Train_EnvstepsSoFar : 333001\n",
      "Train_AverageReturn : 93.63847883811997\n",
      "Train_BestReturn : 127.19637001154697\n",
      "TimeSinceStart : 1557.1615989208221\n",
      "Training Loss : 0.3625735640525818\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 334001\n",
      "mean reward (100 episodes) 99.671508\n",
      "best mean reward 127.196370\n",
      "running time 1564.069707\n",
      "Train_EnvstepsSoFar : 334001\n",
      "Train_AverageReturn : 99.67150770981904\n",
      "Train_BestReturn : 127.19637001154697\n",
      "TimeSinceStart : 1564.0697071552277\n",
      "Training Loss : 0.11510398238897324\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 335001\n",
      "mean reward (100 episodes) 100.911535\n",
      "best mean reward 127.196370\n",
      "running time 1568.120830\n",
      "Train_EnvstepsSoFar : 335001\n",
      "Train_AverageReturn : 100.91153487187104\n",
      "Train_BestReturn : 127.19637001154697\n",
      "TimeSinceStart : 1568.1208300590515\n",
      "Training Loss : 4.868130683898926\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 336001\n",
      "mean reward (100 episodes) 102.506636\n",
      "best mean reward 127.196370\n",
      "running time 1572.228921\n",
      "Train_EnvstepsSoFar : 336001\n",
      "Train_AverageReturn : 102.50663648596849\n",
      "Train_BestReturn : 127.19637001154697\n",
      "TimeSinceStart : 1572.2289209365845\n",
      "Training Loss : 0.47682270407676697\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 337001\n",
      "mean reward (100 episodes) 116.761537\n",
      "best mean reward 127.196370\n",
      "running time 1576.371874\n",
      "Train_EnvstepsSoFar : 337001\n",
      "Train_AverageReturn : 116.76153652694371\n",
      "Train_BestReturn : 127.19637001154697\n",
      "TimeSinceStart : 1576.3718740940094\n",
      "Training Loss : 3.164581298828125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 338001\n",
      "mean reward (100 episodes) 123.852089\n",
      "best mean reward 127.196370\n",
      "running time 1580.749514\n",
      "Train_EnvstepsSoFar : 338001\n",
      "Train_AverageReturn : 123.85208934219887\n",
      "Train_BestReturn : 127.19637001154697\n",
      "TimeSinceStart : 1580.7495141029358\n",
      "Training Loss : 1.7070438861846924\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 339001\n",
      "mean reward (100 episodes) 123.030479\n",
      "best mean reward 127.196370\n",
      "running time 1586.888640\n",
      "Train_EnvstepsSoFar : 339001\n",
      "Train_AverageReturn : 123.03047925824943\n",
      "Train_BestReturn : 127.19637001154697\n",
      "TimeSinceStart : 1586.8886399269104\n",
      "Training Loss : 0.5282835364341736\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 340001\n",
      "mean reward (100 episodes) 120.769265\n",
      "best mean reward 127.196370\n",
      "running time 1594.035231\n",
      "Train_EnvstepsSoFar : 340001\n",
      "Train_AverageReturn : 120.76926451286764\n",
      "Train_BestReturn : 127.19637001154697\n",
      "TimeSinceStart : 1594.0352311134338\n",
      "Training Loss : 0.651905357837677\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 341001\n",
      "mean reward (100 episodes) 123.802320\n",
      "best mean reward 127.196370\n",
      "running time 1598.106269\n",
      "Train_EnvstepsSoFar : 341001\n",
      "Train_AverageReturn : 123.80231975802273\n",
      "Train_BestReturn : 127.19637001154697\n",
      "TimeSinceStart : 1598.1062688827515\n",
      "Training Loss : 0.9931063055992126\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 342001\n",
      "mean reward (100 episodes) 126.080934\n",
      "best mean reward 127.196370\n",
      "running time 1601.943200\n",
      "Train_EnvstepsSoFar : 342001\n",
      "Train_AverageReturn : 126.08093401016305\n",
      "Train_BestReturn : 127.19637001154697\n",
      "TimeSinceStart : 1601.9432001113892\n",
      "Training Loss : 0.23317155241966248\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 343001\n",
      "mean reward (100 episodes) 126.421004\n",
      "best mean reward 127.196370\n",
      "running time 1605.849861\n",
      "Train_EnvstepsSoFar : 343001\n",
      "Train_AverageReturn : 126.4210044676191\n",
      "Train_BestReturn : 127.19637001154697\n",
      "TimeSinceStart : 1605.849860906601\n",
      "Training Loss : 0.1737617552280426\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 344001\n",
      "mean reward (100 episodes) 127.956127\n",
      "best mean reward 127.956127\n",
      "running time 1609.770708\n",
      "Train_EnvstepsSoFar : 344001\n",
      "Train_AverageReturn : 127.95612683589984\n",
      "Train_BestReturn : 127.95612683589984\n",
      "TimeSinceStart : 1609.7707080841064\n",
      "Training Loss : 0.16847795248031616\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 345001\n",
      "mean reward (100 episodes) 135.037329\n",
      "best mean reward 135.037329\n",
      "running time 1614.023335\n",
      "Train_EnvstepsSoFar : 345001\n",
      "Train_AverageReturn : 135.037329359498\n",
      "Train_BestReturn : 135.037329359498\n",
      "TimeSinceStart : 1614.023334980011\n",
      "Training Loss : 0.24329239130020142\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 346001\n",
      "mean reward (100 episodes) 136.118453\n",
      "best mean reward 136.118453\n",
      "running time 1618.471090\n",
      "Train_EnvstepsSoFar : 346001\n",
      "Train_AverageReturn : 136.11845299788757\n",
      "Train_BestReturn : 136.11845299788757\n",
      "TimeSinceStart : 1618.4710898399353\n",
      "Training Loss : 0.3682614862918854\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 347001\n",
      "mean reward (100 episodes) 139.502569\n",
      "best mean reward 139.502569\n",
      "running time 1622.457674\n",
      "Train_EnvstepsSoFar : 347001\n",
      "Train_AverageReturn : 139.50256926255486\n",
      "Train_BestReturn : 139.50256926255486\n",
      "TimeSinceStart : 1622.4576740264893\n",
      "Training Loss : 3.2528722286224365\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 348001\n",
      "mean reward (100 episodes) 138.831411\n",
      "best mean reward 139.502569\n",
      "running time 1626.475919\n",
      "Train_EnvstepsSoFar : 348001\n",
      "Train_AverageReturn : 138.83141128271455\n",
      "Train_BestReturn : 139.50256926255486\n",
      "TimeSinceStart : 1626.4759187698364\n",
      "Training Loss : 0.13882951438426971\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 349001\n",
      "mean reward (100 episodes) 137.812891\n",
      "best mean reward 139.502569\n",
      "running time 1630.805197\n",
      "Train_EnvstepsSoFar : 349001\n",
      "Train_AverageReturn : 137.81289135436293\n",
      "Train_BestReturn : 139.50256926255486\n",
      "TimeSinceStart : 1630.805196762085\n",
      "Training Loss : 3.159355640411377\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 350001\n",
      "mean reward (100 episodes) 140.205278\n",
      "best mean reward 140.205278\n",
      "running time 1635.944960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 350001\n",
      "Train_AverageReturn : 140.2052781094483\n",
      "Train_BestReturn : 140.2052781094483\n",
      "TimeSinceStart : 1635.94496011734\n",
      "Training Loss : 0.11286661773920059\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 351001\n",
      "mean reward (100 episodes) 142.789345\n",
      "best mean reward 142.789345\n",
      "running time 1640.830972\n",
      "Train_EnvstepsSoFar : 351001\n",
      "Train_AverageReturn : 142.7893445951395\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1640.830971956253\n",
      "Training Loss : 1.6925592422485352\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 352001\n",
      "mean reward (100 episodes) 137.040683\n",
      "best mean reward 142.789345\n",
      "running time 1645.132080\n",
      "Train_EnvstepsSoFar : 352001\n",
      "Train_AverageReturn : 137.0406827398798\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1645.1320798397064\n",
      "Training Loss : 0.10047511756420135\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 353001\n",
      "mean reward (100 episodes) 136.924441\n",
      "best mean reward 142.789345\n",
      "running time 1649.589250\n",
      "Train_EnvstepsSoFar : 353001\n",
      "Train_AverageReturn : 136.92444080388594\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1649.589250087738\n",
      "Training Loss : 0.13964340090751648\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 354001\n",
      "mean reward (100 episodes) 130.180857\n",
      "best mean reward 142.789345\n",
      "running time 1653.856590\n",
      "Train_EnvstepsSoFar : 354001\n",
      "Train_AverageReturn : 130.18085672696844\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1653.856589794159\n",
      "Training Loss : 1.5441843271255493\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 355001\n",
      "mean reward (100 episodes) 131.937759\n",
      "best mean reward 142.789345\n",
      "running time 1657.895802\n",
      "Train_EnvstepsSoFar : 355001\n",
      "Train_AverageReturn : 131.93775927190575\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1657.895801782608\n",
      "Training Loss : 5.424043655395508\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 356001\n",
      "mean reward (100 episodes) 131.594584\n",
      "best mean reward 142.789345\n",
      "running time 1662.697135\n",
      "Train_EnvstepsSoFar : 356001\n",
      "Train_AverageReturn : 131.59458352463304\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1662.6971349716187\n",
      "Training Loss : 4.538567543029785\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 357001\n",
      "mean reward (100 episodes) 128.360202\n",
      "best mean reward 142.789345\n",
      "running time 1666.473206\n",
      "Train_EnvstepsSoFar : 357001\n",
      "Train_AverageReturn : 128.3602023034744\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1666.4732060432434\n",
      "Training Loss : 0.3493284285068512\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 358001\n",
      "mean reward (100 episodes) 125.172996\n",
      "best mean reward 142.789345\n",
      "running time 1670.150335\n",
      "Train_EnvstepsSoFar : 358001\n",
      "Train_AverageReturn : 125.17299578118536\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1670.150335073471\n",
      "Training Loss : 2.6373870372772217\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 359001\n",
      "mean reward (100 episodes) 123.785981\n",
      "best mean reward 142.789345\n",
      "running time 1673.910662\n",
      "Train_EnvstepsSoFar : 359001\n",
      "Train_AverageReturn : 123.78598143280216\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1673.9106619358063\n",
      "Training Loss : 0.3183761537075043\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 360001\n",
      "mean reward (100 episodes) 120.087304\n",
      "best mean reward 142.789345\n",
      "running time 1678.059261\n",
      "Train_EnvstepsSoFar : 360001\n",
      "Train_AverageReturn : 120.08730443915724\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1678.0592608451843\n",
      "Training Loss : 0.08212724328041077\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 361001\n",
      "mean reward (100 episodes) 115.159347\n",
      "best mean reward 142.789345\n",
      "running time 1682.147108\n",
      "Train_EnvstepsSoFar : 361001\n",
      "Train_AverageReturn : 115.15934705300253\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1682.1471078395844\n",
      "Training Loss : 0.4494878053665161\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 362001\n",
      "mean reward (100 episodes) 115.365547\n",
      "best mean reward 142.789345\n",
      "running time 1686.329369\n",
      "Train_EnvstepsSoFar : 362001\n",
      "Train_AverageReturn : 115.36554691908096\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1686.3293688297272\n",
      "Training Loss : 0.11859142035245895\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 363001\n",
      "mean reward (100 episodes) 112.049640\n",
      "best mean reward 142.789345\n",
      "running time 1690.659591\n",
      "Train_EnvstepsSoFar : 363001\n",
      "Train_AverageReturn : 112.04964014556022\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1690.659590959549\n",
      "Training Loss : 0.2099551409482956\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 364001\n",
      "mean reward (100 episodes) 114.032246\n",
      "best mean reward 142.789345\n",
      "running time 1694.517188\n",
      "Train_EnvstepsSoFar : 364001\n",
      "Train_AverageReturn : 114.03224551878382\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1694.517187833786\n",
      "Training Loss : 0.7186890840530396\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 365001\n",
      "mean reward (100 episodes) 114.893308\n",
      "best mean reward 142.789345\n",
      "running time 1699.300988\n",
      "Train_EnvstepsSoFar : 365001\n",
      "Train_AverageReturn : 114.89330758856879\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1699.300987958908\n",
      "Training Loss : 0.43818649649620056\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 366001\n",
      "mean reward (100 episodes) 114.809672\n",
      "best mean reward 142.789345\n",
      "running time 1704.584208\n",
      "Train_EnvstepsSoFar : 366001\n",
      "Train_AverageReturn : 114.80967176590742\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1704.5842080116272\n",
      "Training Loss : 0.5692715048789978\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 367001\n",
      "mean reward (100 episodes) 112.434853\n",
      "best mean reward 142.789345\n",
      "running time 1709.014048\n",
      "Train_EnvstepsSoFar : 367001\n",
      "Train_AverageReturn : 112.43485329825165\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1709.0140480995178\n",
      "Training Loss : 0.36879295110702515\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 368001\n",
      "mean reward (100 episodes) 113.015592\n",
      "best mean reward 142.789345\n",
      "running time 1713.810149\n",
      "Train_EnvstepsSoFar : 368001\n",
      "Train_AverageReturn : 113.01559163764222\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1713.8101489543915\n",
      "Training Loss : 2.652379035949707\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 369001\n",
      "mean reward (100 episodes) 115.990814\n",
      "best mean reward 142.789345\n",
      "running time 1718.868255\n",
      "Train_EnvstepsSoFar : 369001\n",
      "Train_AverageReturn : 115.99081396444723\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1718.8682548999786\n",
      "Training Loss : 0.43520328402519226\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 370001\n",
      "mean reward (100 episodes) 117.706093\n",
      "best mean reward 142.789345\n",
      "running time 1723.406172\n",
      "Train_EnvstepsSoFar : 370001\n",
      "Train_AverageReturn : 117.70609273416073\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1723.4061720371246\n",
      "Training Loss : 0.18545576930046082\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 371001\n",
      "mean reward (100 episodes) 112.801140\n",
      "best mean reward 142.789345\n",
      "running time 1727.803289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 371001\n",
      "Train_AverageReturn : 112.80113955979819\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1727.803288936615\n",
      "Training Loss : 1.1597628593444824\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 372001\n",
      "mean reward (100 episodes) 114.913166\n",
      "best mean reward 142.789345\n",
      "running time 1732.101292\n",
      "Train_EnvstepsSoFar : 372001\n",
      "Train_AverageReturn : 114.91316614574323\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1732.1012921333313\n",
      "Training Loss : 0.2048526257276535\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 373001\n",
      "mean reward (100 episodes) 114.961343\n",
      "best mean reward 142.789345\n",
      "running time 1736.246670\n",
      "Train_EnvstepsSoFar : 373001\n",
      "Train_AverageReturn : 114.96134311203409\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1736.246669769287\n",
      "Training Loss : 1.8314226865768433\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 374001\n",
      "mean reward (100 episodes) 121.688809\n",
      "best mean reward 142.789345\n",
      "running time 1740.097724\n",
      "Train_EnvstepsSoFar : 374001\n",
      "Train_AverageReturn : 121.68880902597746\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1740.0977239608765\n",
      "Training Loss : 0.18271009624004364\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 375001\n",
      "mean reward (100 episodes) 119.294561\n",
      "best mean reward 142.789345\n",
      "running time 1743.910715\n",
      "Train_EnvstepsSoFar : 375001\n",
      "Train_AverageReturn : 119.29456085393497\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1743.9107148647308\n",
      "Training Loss : 0.8914111852645874\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 376001\n",
      "mean reward (100 episodes) 116.518677\n",
      "best mean reward 142.789345\n",
      "running time 1747.561165\n",
      "Train_EnvstepsSoFar : 376001\n",
      "Train_AverageReturn : 116.518677053266\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1747.561164855957\n",
      "Training Loss : 0.9149863719940186\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 377001\n",
      "mean reward (100 episodes) 117.122286\n",
      "best mean reward 142.789345\n",
      "running time 1751.291989\n",
      "Train_EnvstepsSoFar : 377001\n",
      "Train_AverageReturn : 117.12228550272651\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1751.2919890880585\n",
      "Training Loss : 1.4295047521591187\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 378001\n",
      "mean reward (100 episodes) 121.591581\n",
      "best mean reward 142.789345\n",
      "running time 1754.969046\n",
      "Train_EnvstepsSoFar : 378001\n",
      "Train_AverageReturn : 121.59158125874008\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1754.9690461158752\n",
      "Training Loss : 0.20975908637046814\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 379001\n",
      "mean reward (100 episodes) 118.565456\n",
      "best mean reward 142.789345\n",
      "running time 1758.685565\n",
      "Train_EnvstepsSoFar : 379001\n",
      "Train_AverageReturn : 118.56545571588366\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1758.685564994812\n",
      "Training Loss : 2.1431198120117188\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 380001\n",
      "mean reward (100 episodes) 116.377201\n",
      "best mean reward 142.789345\n",
      "running time 1762.357747\n",
      "Train_EnvstepsSoFar : 380001\n",
      "Train_AverageReturn : 116.37720059625924\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1762.357747077942\n",
      "Training Loss : 0.4294202923774719\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 381001\n",
      "mean reward (100 episodes) 116.698085\n",
      "best mean reward 142.789345\n",
      "running time 1766.002193\n",
      "Train_EnvstepsSoFar : 381001\n",
      "Train_AverageReturn : 116.69808464062424\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1766.0021929740906\n",
      "Training Loss : 0.3436565101146698\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 382001\n",
      "mean reward (100 episodes) 117.237222\n",
      "best mean reward 142.789345\n",
      "running time 1769.705321\n",
      "Train_EnvstepsSoFar : 382001\n",
      "Train_AverageReturn : 117.23722179984784\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1769.705321073532\n",
      "Training Loss : 0.12121017277240753\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 383001\n",
      "mean reward (100 episodes) 114.866318\n",
      "best mean reward 142.789345\n",
      "running time 1773.353693\n",
      "Train_EnvstepsSoFar : 383001\n",
      "Train_AverageReturn : 114.86631824265268\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1773.3536930084229\n",
      "Training Loss : 1.823012351989746\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 384001\n",
      "mean reward (100 episodes) 114.693460\n",
      "best mean reward 142.789345\n",
      "running time 1777.677702\n",
      "Train_EnvstepsSoFar : 384001\n",
      "Train_AverageReturn : 114.69345972871538\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1777.6777019500732\n",
      "Training Loss : 0.32241979241371155\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 385001\n",
      "mean reward (100 episodes) 119.718361\n",
      "best mean reward 142.789345\n",
      "running time 1781.271866\n",
      "Train_EnvstepsSoFar : 385001\n",
      "Train_AverageReturn : 119.7183606977183\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1781.2718658447266\n",
      "Training Loss : 4.449862957000732\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 386001\n",
      "mean reward (100 episodes) 122.063554\n",
      "best mean reward 142.789345\n",
      "running time 1784.951728\n",
      "Train_EnvstepsSoFar : 386001\n",
      "Train_AverageReturn : 122.06355393726992\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1784.9517278671265\n",
      "Training Loss : 0.16995447874069214\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 387001\n",
      "mean reward (100 episodes) 120.914768\n",
      "best mean reward 142.789345\n",
      "running time 1788.583223\n",
      "Train_EnvstepsSoFar : 387001\n",
      "Train_AverageReturn : 120.91476758218525\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1788.583223104477\n",
      "Training Loss : 1.7607483863830566\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 388001\n",
      "mean reward (100 episodes) 121.493212\n",
      "best mean reward 142.789345\n",
      "running time 1792.218385\n",
      "Train_EnvstepsSoFar : 388001\n",
      "Train_AverageReturn : 121.49321175332787\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1792.2183849811554\n",
      "Training Loss : 2.2372477054595947\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 389001\n",
      "mean reward (100 episodes) 125.971575\n",
      "best mean reward 142.789345\n",
      "running time 1795.798344\n",
      "Train_EnvstepsSoFar : 389001\n",
      "Train_AverageReturn : 125.97157476810929\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1795.7983438968658\n",
      "Training Loss : 2.4745259284973145\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 390001\n",
      "mean reward (100 episodes) 127.784159\n",
      "best mean reward 142.789345\n",
      "running time 1799.382283\n",
      "Train_EnvstepsSoFar : 390001\n",
      "Train_AverageReturn : 127.78415931362001\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1799.3822829723358\n",
      "Training Loss : 0.12817396223545074\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 391001\n",
      "mean reward (100 episodes) 131.926816\n",
      "best mean reward 142.789345\n",
      "running time 1803.637709\n",
      "Train_EnvstepsSoFar : 391001\n",
      "Train_AverageReturn : 131.92681612029702\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1803.637708902359\n",
      "Training Loss : 1.912980318069458\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 392001\n",
      "mean reward (100 episodes) 122.581035\n",
      "best mean reward 142.789345\n",
      "running time 1807.482048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 392001\n",
      "Train_AverageReturn : 122.58103527504592\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1807.482048034668\n",
      "Training Loss : 0.4453897774219513\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 393001\n",
      "mean reward (100 episodes) 122.909106\n",
      "best mean reward 142.789345\n",
      "running time 1811.092639\n",
      "Train_EnvstepsSoFar : 393001\n",
      "Train_AverageReturn : 122.90910586837593\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1811.0926389694214\n",
      "Training Loss : 0.4382851719856262\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 394001\n",
      "mean reward (100 episodes) 117.930441\n",
      "best mean reward 142.789345\n",
      "running time 1814.991692\n",
      "Train_EnvstepsSoFar : 394001\n",
      "Train_AverageReturn : 117.93044083252099\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1814.9916920661926\n",
      "Training Loss : 2.275508165359497\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 395001\n",
      "mean reward (100 episodes) 118.331934\n",
      "best mean reward 142.789345\n",
      "running time 1819.233903\n",
      "Train_EnvstepsSoFar : 395001\n",
      "Train_AverageReturn : 118.33193365998959\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1819.2339029312134\n",
      "Training Loss : 0.21988102793693542\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 396001\n",
      "mean reward (100 episodes) 118.230063\n",
      "best mean reward 142.789345\n",
      "running time 1822.998611\n",
      "Train_EnvstepsSoFar : 396001\n",
      "Train_AverageReturn : 118.23006303170703\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1822.9986109733582\n",
      "Training Loss : 0.20680207014083862\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 397001\n",
      "mean reward (100 episodes) 123.705270\n",
      "best mean reward 142.789345\n",
      "running time 1826.710767\n",
      "Train_EnvstepsSoFar : 397001\n",
      "Train_AverageReturn : 123.70527017063765\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1826.7107667922974\n",
      "Training Loss : 0.7038389444351196\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 398001\n",
      "mean reward (100 episodes) 128.036443\n",
      "best mean reward 142.789345\n",
      "running time 1830.340100\n",
      "Train_EnvstepsSoFar : 398001\n",
      "Train_AverageReturn : 128.0364434932547\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1830.340099811554\n",
      "Training Loss : 0.6598795056343079\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 399001\n",
      "mean reward (100 episodes) 125.117763\n",
      "best mean reward 142.789345\n",
      "running time 1834.131378\n",
      "Train_EnvstepsSoFar : 399001\n",
      "Train_AverageReturn : 125.11776294293128\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1834.1313779354095\n",
      "Training Loss : 0.30327659845352173\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 400001\n",
      "mean reward (100 episodes) 125.847090\n",
      "best mean reward 142.789345\n",
      "running time 1837.743908\n",
      "Train_EnvstepsSoFar : 400001\n",
      "Train_AverageReturn : 125.84708984535004\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1837.7439079284668\n",
      "Training Loss : 0.38659340143203735\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 401001\n",
      "mean reward (100 episodes) 128.775154\n",
      "best mean reward 142.789345\n",
      "running time 1841.349912\n",
      "Train_EnvstepsSoFar : 401001\n",
      "Train_AverageReturn : 128.77515418297673\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1841.3499119281769\n",
      "Training Loss : 0.8498333096504211\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 402001\n",
      "mean reward (100 episodes) 124.957146\n",
      "best mean reward 142.789345\n",
      "running time 1844.936129\n",
      "Train_EnvstepsSoFar : 402001\n",
      "Train_AverageReturn : 124.95714578302625\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1844.9361290931702\n",
      "Training Loss : 0.2353609949350357\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 403001\n",
      "mean reward (100 episodes) 123.883440\n",
      "best mean reward 142.789345\n",
      "running time 1848.566416\n",
      "Train_EnvstepsSoFar : 403001\n",
      "Train_AverageReturn : 123.88343977038444\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1848.5664157867432\n",
      "Training Loss : 0.7129518985748291\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 404001\n",
      "mean reward (100 episodes) 121.472895\n",
      "best mean reward 142.789345\n",
      "running time 1852.474152\n",
      "Train_EnvstepsSoFar : 404001\n",
      "Train_AverageReturn : 121.47289535283907\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1852.4741518497467\n",
      "Training Loss : 0.8878695368766785\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 405001\n",
      "mean reward (100 episodes) 120.581325\n",
      "best mean reward 142.789345\n",
      "running time 1856.215504\n",
      "Train_EnvstepsSoFar : 405001\n",
      "Train_AverageReturn : 120.58132457328495\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1856.2155039310455\n",
      "Training Loss : 0.25163719058036804\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 406001\n",
      "mean reward (100 episodes) 120.243151\n",
      "best mean reward 142.789345\n",
      "running time 1859.812513\n",
      "Train_EnvstepsSoFar : 406001\n",
      "Train_AverageReturn : 120.24315126973372\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1859.8125131130219\n",
      "Training Loss : 0.18389372527599335\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 407001\n",
      "mean reward (100 episodes) 127.791385\n",
      "best mean reward 142.789345\n",
      "running time 1863.665388\n",
      "Train_EnvstepsSoFar : 407001\n",
      "Train_AverageReturn : 127.79138501085882\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1863.6653881072998\n",
      "Training Loss : 0.1887623518705368\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 408001\n",
      "mean reward (100 episodes) 126.079802\n",
      "best mean reward 142.789345\n",
      "running time 1867.268342\n",
      "Train_EnvstepsSoFar : 408001\n",
      "Train_AverageReturn : 126.07980236478949\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1867.2683420181274\n",
      "Training Loss : 0.14702162146568298\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 409001\n",
      "mean reward (100 episodes) 134.549591\n",
      "best mean reward 142.789345\n",
      "running time 1871.078388\n",
      "Train_EnvstepsSoFar : 409001\n",
      "Train_AverageReturn : 134.54959076909492\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1871.0783879756927\n",
      "Training Loss : 5.158688545227051\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 410001\n",
      "mean reward (100 episodes) 129.331970\n",
      "best mean reward 142.789345\n",
      "running time 1875.216705\n",
      "Train_EnvstepsSoFar : 410001\n",
      "Train_AverageReturn : 129.33197006713948\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1875.216705083847\n",
      "Training Loss : 0.3215366303920746\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 411001\n",
      "mean reward (100 episodes) 134.303123\n",
      "best mean reward 142.789345\n",
      "running time 1879.378857\n",
      "Train_EnvstepsSoFar : 411001\n",
      "Train_AverageReturn : 134.30312348927535\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1879.3788568973541\n",
      "Training Loss : 0.10411693155765533\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 412001\n",
      "mean reward (100 episodes) 132.475050\n",
      "best mean reward 142.789345\n",
      "running time 1883.140872\n",
      "Train_EnvstepsSoFar : 412001\n",
      "Train_AverageReturn : 132.4750500801075\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1883.140872001648\n",
      "Training Loss : 0.1217225044965744\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 413001\n",
      "mean reward (100 episodes) 133.303926\n",
      "best mean reward 142.789345\n",
      "running time 1886.878389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 413001\n",
      "Train_AverageReturn : 133.30392573762677\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1886.878389120102\n",
      "Training Loss : 0.48190179467201233\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 414001\n",
      "mean reward (100 episodes) 135.634789\n",
      "best mean reward 142.789345\n",
      "running time 1890.828728\n",
      "Train_EnvstepsSoFar : 414001\n",
      "Train_AverageReturn : 135.63478919531053\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1890.828727722168\n",
      "Training Loss : 0.2411002218723297\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 415001\n",
      "mean reward (100 episodes) 135.733225\n",
      "best mean reward 142.789345\n",
      "running time 1894.968113\n",
      "Train_EnvstepsSoFar : 415001\n",
      "Train_AverageReturn : 135.73322500173077\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1894.9681129455566\n",
      "Training Loss : 0.14736312627792358\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 416001\n",
      "mean reward (100 episodes) 139.613901\n",
      "best mean reward 142.789345\n",
      "running time 1898.632991\n",
      "Train_EnvstepsSoFar : 416001\n",
      "Train_AverageReturn : 139.61390069915865\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1898.6329910755157\n",
      "Training Loss : 4.419212341308594\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 417001\n",
      "mean reward (100 episodes) 136.710605\n",
      "best mean reward 142.789345\n",
      "running time 1902.554056\n",
      "Train_EnvstepsSoFar : 417001\n",
      "Train_AverageReturn : 136.7106048216688\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1902.554055929184\n",
      "Training Loss : 1.0184078216552734\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 418001\n",
      "mean reward (100 episodes) 137.154145\n",
      "best mean reward 142.789345\n",
      "running time 1906.415803\n",
      "Train_EnvstepsSoFar : 418001\n",
      "Train_AverageReturn : 137.15414521422255\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1906.4158029556274\n",
      "Training Loss : 0.24564599990844727\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 419001\n",
      "mean reward (100 episodes) 140.612098\n",
      "best mean reward 142.789345\n",
      "running time 1910.788520\n",
      "Train_EnvstepsSoFar : 419001\n",
      "Train_AverageReturn : 140.61209788956097\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1910.788519859314\n",
      "Training Loss : 0.6569184064865112\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 420001\n",
      "mean reward (100 episodes) 138.085269\n",
      "best mean reward 142.789345\n",
      "running time 1914.567574\n",
      "Train_EnvstepsSoFar : 420001\n",
      "Train_AverageReturn : 138.08526871735367\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1914.5675740242004\n",
      "Training Loss : 0.19065101444721222\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 421001\n",
      "mean reward (100 episodes) 138.378509\n",
      "best mean reward 142.789345\n",
      "running time 1918.372740\n",
      "Train_EnvstepsSoFar : 421001\n",
      "Train_AverageReturn : 138.3785087789832\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1918.3727397918701\n",
      "Training Loss : 0.15471632778644562\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 422001\n",
      "mean reward (100 episodes) 139.017194\n",
      "best mean reward 142.789345\n",
      "running time 1922.195296\n",
      "Train_EnvstepsSoFar : 422001\n",
      "Train_AverageReturn : 139.01719393432384\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1922.195296049118\n",
      "Training Loss : 0.4581553041934967\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 423001\n",
      "mean reward (100 episodes) 140.810271\n",
      "best mean reward 142.789345\n",
      "running time 1925.889337\n",
      "Train_EnvstepsSoFar : 423001\n",
      "Train_AverageReturn : 140.81027141545704\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1925.8893368244171\n",
      "Training Loss : 2.799797296524048\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 424001\n",
      "mean reward (100 episodes) 139.375574\n",
      "best mean reward 142.789345\n",
      "running time 1929.882841\n",
      "Train_EnvstepsSoFar : 424001\n",
      "Train_AverageReturn : 139.37557357913877\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1929.882840871811\n",
      "Training Loss : 0.22579096257686615\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 425001\n",
      "mean reward (100 episodes) 134.279832\n",
      "best mean reward 142.789345\n",
      "running time 1934.335960\n",
      "Train_EnvstepsSoFar : 425001\n",
      "Train_AverageReturn : 134.27983176666686\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1934.3359599113464\n",
      "Training Loss : 1.3549294471740723\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 426001\n",
      "mean reward (100 episodes) 127.472717\n",
      "best mean reward 142.789345\n",
      "running time 1940.141572\n",
      "Train_EnvstepsSoFar : 426001\n",
      "Train_AverageReturn : 127.47271725446107\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1940.1415717601776\n",
      "Training Loss : 1.2391682863235474\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 427001\n",
      "mean reward (100 episodes) 124.219879\n",
      "best mean reward 142.789345\n",
      "running time 1943.819667\n",
      "Train_EnvstepsSoFar : 427001\n",
      "Train_AverageReturn : 124.21987922759543\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1943.8196668624878\n",
      "Training Loss : 0.4134316146373749\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 428001\n",
      "mean reward (100 episodes) 124.574712\n",
      "best mean reward 142.789345\n",
      "running time 1947.491777\n",
      "Train_EnvstepsSoFar : 428001\n",
      "Train_AverageReturn : 124.57471208246199\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1947.4917769432068\n",
      "Training Loss : 0.30080458521842957\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 429001\n",
      "mean reward (100 episodes) 122.069781\n",
      "best mean reward 142.789345\n",
      "running time 1951.178017\n",
      "Train_EnvstepsSoFar : 429001\n",
      "Train_AverageReturn : 122.06978129436722\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1951.1780171394348\n",
      "Training Loss : 0.11003511399030685\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 430001\n",
      "mean reward (100 episodes) 118.407543\n",
      "best mean reward 142.789345\n",
      "running time 1954.867629\n",
      "Train_EnvstepsSoFar : 430001\n",
      "Train_AverageReturn : 118.40754273837166\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1954.8676290512085\n",
      "Training Loss : 0.25668686628341675\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 431001\n",
      "mean reward (100 episodes) 121.619252\n",
      "best mean reward 142.789345\n",
      "running time 1958.570383\n",
      "Train_EnvstepsSoFar : 431001\n",
      "Train_AverageReturn : 121.61925177125995\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1958.5703828334808\n",
      "Training Loss : 0.7917452454566956\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 432001\n",
      "mean reward (100 episodes) 127.255875\n",
      "best mean reward 142.789345\n",
      "running time 1962.246771\n",
      "Train_EnvstepsSoFar : 432001\n",
      "Train_AverageReturn : 127.25587450407778\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1962.2467708587646\n",
      "Training Loss : 0.28225889801979065\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 433001\n",
      "mean reward (100 episodes) 130.071715\n",
      "best mean reward 142.789345\n",
      "running time 1965.938358\n",
      "Train_EnvstepsSoFar : 433001\n",
      "Train_AverageReturn : 130.07171532771454\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1965.9383580684662\n",
      "Training Loss : 0.7577194571495056\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 434001\n",
      "mean reward (100 episodes) 126.474410\n",
      "best mean reward 142.789345\n",
      "running time 1969.673306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 434001\n",
      "Train_AverageReturn : 126.4744099109083\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1969.6733059883118\n",
      "Training Loss : 1.5191841125488281\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 435001\n",
      "mean reward (100 episodes) 132.309693\n",
      "best mean reward 142.789345\n",
      "running time 1973.314711\n",
      "Train_EnvstepsSoFar : 435001\n",
      "Train_AverageReturn : 132.30969280799002\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1973.3147110939026\n",
      "Training Loss : 2.764617443084717\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 436000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 436001\n",
      "mean reward (100 episodes) 132.126307\n",
      "best mean reward 142.789345\n",
      "running time 1976.928221\n",
      "Train_EnvstepsSoFar : 436001\n",
      "Train_AverageReturn : 132.12630748632992\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1976.92822098732\n",
      "Training Loss : 1.7505033016204834\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 437001\n",
      "mean reward (100 episodes) 128.485314\n",
      "best mean reward 142.789345\n",
      "running time 1980.598382\n",
      "Train_EnvstepsSoFar : 437001\n",
      "Train_AverageReturn : 128.48531366601296\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1980.5983817577362\n",
      "Training Loss : 2.1655430793762207\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 438001\n",
      "mean reward (100 episodes) 133.269073\n",
      "best mean reward 142.789345\n",
      "running time 1984.289079\n",
      "Train_EnvstepsSoFar : 438001\n",
      "Train_AverageReturn : 133.26907284769848\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1984.2890791893005\n",
      "Training Loss : 0.2976571023464203\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 439001\n",
      "mean reward (100 episodes) 134.400327\n",
      "best mean reward 142.789345\n",
      "running time 1988.223789\n",
      "Train_EnvstepsSoFar : 439001\n",
      "Train_AverageReturn : 134.40032679736635\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1988.2237889766693\n",
      "Training Loss : 0.8107284307479858\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 440001\n",
      "mean reward (100 episodes) 140.619649\n",
      "best mean reward 142.789345\n",
      "running time 1992.033816\n",
      "Train_EnvstepsSoFar : 440001\n",
      "Train_AverageReturn : 140.61964862864758\n",
      "Train_BestReturn : 142.7893445951395\n",
      "TimeSinceStart : 1992.0338158607483\n",
      "Training Loss : 2.3078904151916504\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 441001\n",
      "mean reward (100 episodes) 144.680938\n",
      "best mean reward 144.680938\n",
      "running time 1995.660808\n",
      "Train_EnvstepsSoFar : 441001\n",
      "Train_AverageReturn : 144.68093845317787\n",
      "Train_BestReturn : 144.68093845317787\n",
      "TimeSinceStart : 1995.6608078479767\n",
      "Training Loss : 0.8584529757499695\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 442001\n",
      "mean reward (100 episodes) 151.660594\n",
      "best mean reward 151.660594\n",
      "running time 1999.279464\n",
      "Train_EnvstepsSoFar : 442001\n",
      "Train_AverageReturn : 151.6605944153883\n",
      "Train_BestReturn : 151.6605944153883\n",
      "TimeSinceStart : 1999.279464006424\n",
      "Training Loss : 0.3880144953727722\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 443001\n",
      "mean reward (100 episodes) 151.883334\n",
      "best mean reward 151.883334\n",
      "running time 2002.921536\n",
      "Train_EnvstepsSoFar : 443001\n",
      "Train_AverageReturn : 151.88333360035114\n",
      "Train_BestReturn : 151.88333360035114\n",
      "TimeSinceStart : 2002.9215359687805\n",
      "Training Loss : 0.17625212669372559\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 444001\n",
      "mean reward (100 episodes) 152.000510\n",
      "best mean reward 152.000510\n",
      "running time 2006.593298\n",
      "Train_EnvstepsSoFar : 444001\n",
      "Train_AverageReturn : 152.00050961519125\n",
      "Train_BestReturn : 152.00050961519125\n",
      "TimeSinceStart : 2006.593297958374\n",
      "Training Loss : 0.3068827986717224\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 445001\n",
      "mean reward (100 episodes) 152.159215\n",
      "best mean reward 152.159215\n",
      "running time 2010.455663\n",
      "Train_EnvstepsSoFar : 445001\n",
      "Train_AverageReturn : 152.15921525328358\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2010.4556629657745\n",
      "Training Loss : 0.48350271582603455\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 446001\n",
      "mean reward (100 episodes) 148.593553\n",
      "best mean reward 152.159215\n",
      "running time 2014.418635\n",
      "Train_EnvstepsSoFar : 446001\n",
      "Train_AverageReturn : 148.59355312436364\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2014.4186351299286\n",
      "Training Loss : 0.1605096161365509\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 447001\n",
      "mean reward (100 episodes) 145.178080\n",
      "best mean reward 152.159215\n",
      "running time 2018.138224\n",
      "Train_EnvstepsSoFar : 447001\n",
      "Train_AverageReturn : 145.17807955336787\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2018.1382238864899\n",
      "Training Loss : 0.17421704530715942\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 448001\n",
      "mean reward (100 episodes) 147.683960\n",
      "best mean reward 152.159215\n",
      "running time 2021.856278\n",
      "Train_EnvstepsSoFar : 448001\n",
      "Train_AverageReturn : 147.68396020193993\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2021.8562779426575\n",
      "Training Loss : 0.734224259853363\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 449001\n",
      "mean reward (100 episodes) 145.356532\n",
      "best mean reward 152.159215\n",
      "running time 2025.562711\n",
      "Train_EnvstepsSoFar : 449001\n",
      "Train_AverageReturn : 145.35653221739912\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2025.5627110004425\n",
      "Training Loss : 1.170292615890503\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 450001\n",
      "mean reward (100 episodes) 141.238622\n",
      "best mean reward 152.159215\n",
      "running time 2029.213350\n",
      "Train_EnvstepsSoFar : 450001\n",
      "Train_AverageReturn : 141.23862171522117\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2029.2133498191833\n",
      "Training Loss : 0.8617304563522339\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 451001\n",
      "mean reward (100 episodes) 138.657574\n",
      "best mean reward 152.159215\n",
      "running time 2033.091761\n",
      "Train_EnvstepsSoFar : 451001\n",
      "Train_AverageReturn : 138.65757434114852\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2033.0917608737946\n",
      "Training Loss : 0.18834607303142548\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 452001\n",
      "mean reward (100 episodes) 146.099134\n",
      "best mean reward 152.159215\n",
      "running time 2036.906865\n",
      "Train_EnvstepsSoFar : 452001\n",
      "Train_AverageReturn : 146.09913353124213\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2036.9068648815155\n",
      "Training Loss : 0.2481555938720703\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 453001\n",
      "mean reward (100 episodes) 145.016224\n",
      "best mean reward 152.159215\n",
      "running time 2040.734899\n",
      "Train_EnvstepsSoFar : 453001\n",
      "Train_AverageReturn : 145.01622428247413\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2040.7348990440369\n",
      "Training Loss : 0.4423520863056183\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 454001\n",
      "mean reward (100 episodes) 148.531414\n",
      "best mean reward 152.159215\n",
      "running time 2044.755090\n",
      "Train_EnvstepsSoFar : 454001\n",
      "Train_AverageReturn : 148.5314142218658\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2044.7550899982452\n",
      "Training Loss : 0.4389723241329193\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 455001\n",
      "mean reward (100 episodes) 149.684198\n",
      "best mean reward 152.159215\n",
      "running time 2048.471421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 455001\n",
      "Train_AverageReturn : 149.68419810478292\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2048.4714210033417\n",
      "Training Loss : 0.1774071902036667\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 456001\n",
      "mean reward (100 episodes) 142.348715\n",
      "best mean reward 152.159215\n",
      "running time 2052.645479\n",
      "Train_EnvstepsSoFar : 456001\n",
      "Train_AverageReturn : 142.34871533808527\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2052.645478963852\n",
      "Training Loss : 0.3210744857788086\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 457001\n",
      "mean reward (100 episodes) 138.145982\n",
      "best mean reward 152.159215\n",
      "running time 2056.828600\n",
      "Train_EnvstepsSoFar : 457001\n",
      "Train_AverageReturn : 138.14598155385536\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2056.8285999298096\n",
      "Training Loss : 0.20742270350456238\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 458001\n",
      "mean reward (100 episodes) 139.468691\n",
      "best mean reward 152.159215\n",
      "running time 2061.110703\n",
      "Train_EnvstepsSoFar : 458001\n",
      "Train_AverageReturn : 139.4686909495782\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2061.1107029914856\n",
      "Training Loss : 1.1085832118988037\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 459001\n",
      "mean reward (100 episodes) 139.689962\n",
      "best mean reward 152.159215\n",
      "running time 2065.515898\n",
      "Train_EnvstepsSoFar : 459001\n",
      "Train_AverageReturn : 139.68996218409458\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2065.515897989273\n",
      "Training Loss : 0.2362976223230362\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 460001\n",
      "mean reward (100 episodes) 138.507119\n",
      "best mean reward 152.159215\n",
      "running time 2069.261791\n",
      "Train_EnvstepsSoFar : 460001\n",
      "Train_AverageReturn : 138.50711922450438\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2069.2617909908295\n",
      "Training Loss : 0.4045313894748688\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 461001\n",
      "mean reward (100 episodes) 141.626038\n",
      "best mean reward 152.159215\n",
      "running time 2072.839533\n",
      "Train_EnvstepsSoFar : 461001\n",
      "Train_AverageReturn : 141.62603807997363\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2072.8395330905914\n",
      "Training Loss : 1.2147541046142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 462001\n",
      "mean reward (100 episodes) 138.954735\n",
      "best mean reward 152.159215\n",
      "running time 2076.449444\n",
      "Train_EnvstepsSoFar : 462001\n",
      "Train_AverageReturn : 138.95473493881332\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2076.4494438171387\n",
      "Training Loss : 0.5964968204498291\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 463001\n",
      "mean reward (100 episodes) 140.152322\n",
      "best mean reward 152.159215\n",
      "running time 2080.090497\n",
      "Train_EnvstepsSoFar : 463001\n",
      "Train_AverageReturn : 140.15232160107797\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2080.0904970169067\n",
      "Training Loss : 0.15665261447429657\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 464001\n",
      "mean reward (100 episodes) 141.195247\n",
      "best mean reward 152.159215\n",
      "running time 2083.732887\n",
      "Train_EnvstepsSoFar : 464001\n",
      "Train_AverageReturn : 141.1952469963567\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2083.732887029648\n",
      "Training Loss : 0.05336332693696022\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 465001\n",
      "mean reward (100 episodes) 139.555589\n",
      "best mean reward 152.159215\n",
      "running time 2087.362115\n",
      "Train_EnvstepsSoFar : 465001\n",
      "Train_AverageReturn : 139.55558939228285\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2087.362114906311\n",
      "Training Loss : 2.4340431690216064\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 466001\n",
      "mean reward (100 episodes) 138.337844\n",
      "best mean reward 152.159215\n",
      "running time 2090.984492\n",
      "Train_EnvstepsSoFar : 466001\n",
      "Train_AverageReturn : 138.33784350928843\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2090.9844920635223\n",
      "Training Loss : 0.2206617146730423\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 467001\n",
      "mean reward (100 episodes) 131.572464\n",
      "best mean reward 152.159215\n",
      "running time 2094.651290\n",
      "Train_EnvstepsSoFar : 467001\n",
      "Train_AverageReturn : 131.57246416718942\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2094.6512899398804\n",
      "Training Loss : 0.8556816577911377\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 468001\n",
      "mean reward (100 episodes) 129.340931\n",
      "best mean reward 152.159215\n",
      "running time 2098.607011\n",
      "Train_EnvstepsSoFar : 468001\n",
      "Train_AverageReturn : 129.34093071214994\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2098.6070108413696\n",
      "Training Loss : 1.0385020971298218\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 469001\n",
      "mean reward (100 episodes) 125.699361\n",
      "best mean reward 152.159215\n",
      "running time 2102.241973\n",
      "Train_EnvstepsSoFar : 469001\n",
      "Train_AverageReturn : 125.69936062622706\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2102.241972923279\n",
      "Training Loss : 1.2237364053726196\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 470001\n",
      "mean reward (100 episodes) 129.198221\n",
      "best mean reward 152.159215\n",
      "running time 2106.138325\n",
      "Train_EnvstepsSoFar : 470001\n",
      "Train_AverageReturn : 129.19822118986667\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2106.1383249759674\n",
      "Training Loss : 0.23179295659065247\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 471001\n",
      "mean reward (100 episodes) 130.938707\n",
      "best mean reward 152.159215\n",
      "running time 2109.808372\n",
      "Train_EnvstepsSoFar : 471001\n",
      "Train_AverageReturn : 130.93870688007323\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2109.8083720207214\n",
      "Training Loss : 0.7099411487579346\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 472001\n",
      "mean reward (100 episodes) 125.738110\n",
      "best mean reward 152.159215\n",
      "running time 2113.474391\n",
      "Train_EnvstepsSoFar : 472001\n",
      "Train_AverageReturn : 125.7381095532463\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2113.4743909835815\n",
      "Training Loss : 0.22617343068122864\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 473001\n",
      "mean reward (100 episodes) 128.859226\n",
      "best mean reward 152.159215\n",
      "running time 2117.095741\n",
      "Train_EnvstepsSoFar : 473001\n",
      "Train_AverageReturn : 128.8592255902877\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2117.095741033554\n",
      "Training Loss : 0.27954789996147156\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 474001\n",
      "mean reward (100 episodes) 132.748106\n",
      "best mean reward 152.159215\n",
      "running time 2120.787880\n",
      "Train_EnvstepsSoFar : 474001\n",
      "Train_AverageReturn : 132.7481059874201\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2120.7878799438477\n",
      "Training Loss : 0.3894282877445221\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 475001\n",
      "mean reward (100 episodes) 131.106915\n",
      "best mean reward 152.159215\n",
      "running time 2124.402199\n",
      "Train_EnvstepsSoFar : 475001\n",
      "Train_AverageReturn : 131.10691491412075\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2124.4021990299225\n",
      "Training Loss : 2.2487905025482178\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 476001\n",
      "mean reward (100 episodes) 133.751728\n",
      "best mean reward 152.159215\n",
      "running time 2128.074029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 476001\n",
      "Train_AverageReturn : 133.75172778044828\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2128.074028968811\n",
      "Training Loss : 0.3200947940349579\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 477001\n",
      "mean reward (100 episodes) 133.148790\n",
      "best mean reward 152.159215\n",
      "running time 2131.694787\n",
      "Train_EnvstepsSoFar : 477001\n",
      "Train_AverageReturn : 133.14878984269617\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2131.694786787033\n",
      "Training Loss : 0.2311696857213974\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 478001\n",
      "mean reward (100 episodes) 129.001348\n",
      "best mean reward 152.159215\n",
      "running time 2135.326839\n",
      "Train_EnvstepsSoFar : 478001\n",
      "Train_AverageReturn : 129.00134808316\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2135.3268389701843\n",
      "Training Loss : 0.3433936536312103\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 479001\n",
      "mean reward (100 episodes) 132.126617\n",
      "best mean reward 152.159215\n",
      "running time 2138.969105\n",
      "Train_EnvstepsSoFar : 479001\n",
      "Train_AverageReturn : 132.12661707425417\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2138.9691050052643\n",
      "Training Loss : 0.14532162249088287\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 480001\n",
      "mean reward (100 episodes) 134.943925\n",
      "best mean reward 152.159215\n",
      "running time 2142.741053\n",
      "Train_EnvstepsSoFar : 480001\n",
      "Train_AverageReturn : 134.9439253884316\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2142.741052865982\n",
      "Training Loss : 0.10002323240041733\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 481001\n",
      "mean reward (100 episodes) 138.024917\n",
      "best mean reward 152.159215\n",
      "running time 2146.377172\n",
      "Train_EnvstepsSoFar : 481001\n",
      "Train_AverageReturn : 138.0249174245014\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2146.3771719932556\n",
      "Training Loss : 0.18213525414466858\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 482001\n",
      "mean reward (100 episodes) 140.304606\n",
      "best mean reward 152.159215\n",
      "running time 2149.982276\n",
      "Train_EnvstepsSoFar : 482001\n",
      "Train_AverageReturn : 140.304605833606\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2149.9822759628296\n",
      "Training Loss : 0.1300044059753418\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 483001\n",
      "mean reward (100 episodes) 139.057930\n",
      "best mean reward 152.159215\n",
      "running time 2153.609591\n",
      "Train_EnvstepsSoFar : 483001\n",
      "Train_AverageReturn : 139.05792964095914\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2153.6095910072327\n",
      "Training Loss : 6.550146102905273\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 484001\n",
      "mean reward (100 episodes) 139.355119\n",
      "best mean reward 152.159215\n",
      "running time 2157.220222\n",
      "Train_EnvstepsSoFar : 484001\n",
      "Train_AverageReturn : 139.3551191645865\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2157.2202219963074\n",
      "Training Loss : 0.31362712383270264\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 485001\n",
      "mean reward (100 episodes) 135.719472\n",
      "best mean reward 152.159215\n",
      "running time 2160.839861\n",
      "Train_EnvstepsSoFar : 485001\n",
      "Train_AverageReturn : 135.71947226747926\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2160.8398609161377\n",
      "Training Loss : 0.2056102156639099\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 486001\n",
      "mean reward (100 episodes) 139.746047\n",
      "best mean reward 152.159215\n",
      "running time 2164.445170\n",
      "Train_EnvstepsSoFar : 486001\n",
      "Train_AverageReturn : 139.74604713187898\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2164.4451699256897\n",
      "Training Loss : 1.5719852447509766\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 487001\n",
      "mean reward (100 episodes) 138.733744\n",
      "best mean reward 152.159215\n",
      "running time 2168.114899\n",
      "Train_EnvstepsSoFar : 487001\n",
      "Train_AverageReturn : 138.7337444984632\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2168.114898920059\n",
      "Training Loss : 1.6516203880310059\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 488001\n",
      "mean reward (100 episodes) 136.594575\n",
      "best mean reward 152.159215\n",
      "running time 2171.749265\n",
      "Train_EnvstepsSoFar : 488001\n",
      "Train_AverageReturn : 136.59457545259187\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2171.7492649555206\n",
      "Training Loss : 0.32273581624031067\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 489001\n",
      "mean reward (100 episodes) 134.579845\n",
      "best mean reward 152.159215\n",
      "running time 2175.375207\n",
      "Train_EnvstepsSoFar : 489001\n",
      "Train_AverageReturn : 134.57984466133885\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2175.3752069473267\n",
      "Training Loss : 1.3589752912521362\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 490001\n",
      "mean reward (100 episodes) 135.159778\n",
      "best mean reward 152.159215\n",
      "running time 2179.039507\n",
      "Train_EnvstepsSoFar : 490001\n",
      "Train_AverageReturn : 135.15977828018399\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2179.0395069122314\n",
      "Training Loss : 1.0043330192565918\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 491001\n",
      "mean reward (100 episodes) 130.932138\n",
      "best mean reward 152.159215\n",
      "running time 2182.709071\n",
      "Train_EnvstepsSoFar : 491001\n",
      "Train_AverageReturn : 130.93213838307102\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2182.709070920944\n",
      "Training Loss : 2.469438076019287\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 492001\n",
      "mean reward (100 episodes) 130.604478\n",
      "best mean reward 152.159215\n",
      "running time 2186.298172\n",
      "Train_EnvstepsSoFar : 492001\n",
      "Train_AverageReturn : 130.60447776435203\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2186.2981719970703\n",
      "Training Loss : 0.9225459694862366\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 493001\n",
      "mean reward (100 episodes) 130.349975\n",
      "best mean reward 152.159215\n",
      "running time 2189.890808\n",
      "Train_EnvstepsSoFar : 493001\n",
      "Train_AverageReturn : 130.34997509782048\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2189.89080786705\n",
      "Training Loss : 2.6257591247558594\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 494001\n",
      "mean reward (100 episodes) 127.098297\n",
      "best mean reward 152.159215\n",
      "running time 2193.608195\n",
      "Train_EnvstepsSoFar : 494001\n",
      "Train_AverageReturn : 127.09829697329972\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2193.6081948280334\n",
      "Training Loss : 1.1840275526046753\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 495001\n",
      "mean reward (100 episodes) 127.283835\n",
      "best mean reward 152.159215\n",
      "running time 2197.397712\n",
      "Train_EnvstepsSoFar : 495001\n",
      "Train_AverageReturn : 127.28383546737257\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2197.397711992264\n",
      "Training Loss : 1.9595444202423096\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 496001\n",
      "mean reward (100 episodes) 130.044505\n",
      "best mean reward 152.159215\n",
      "running time 2201.187944\n",
      "Train_EnvstepsSoFar : 496001\n",
      "Train_AverageReturn : 130.04450464893708\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2201.1879439353943\n",
      "Training Loss : 0.34701600670814514\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 497001\n",
      "mean reward (100 episodes) 133.036155\n",
      "best mean reward 152.159215\n",
      "running time 2204.835242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 497001\n",
      "Train_AverageReturn : 133.03615477807654\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2204.8352420330048\n",
      "Training Loss : 0.644425094127655\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 498001\n",
      "mean reward (100 episodes) 132.500988\n",
      "best mean reward 152.159215\n",
      "running time 2208.465910\n",
      "Train_EnvstepsSoFar : 498001\n",
      "Train_AverageReturn : 132.50098807127966\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2208.4659099578857\n",
      "Training Loss : 2.8511452674865723\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 499000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 499001\n",
      "mean reward (100 episodes) 136.360393\n",
      "best mean reward 152.159215\n",
      "running time 2212.102291\n",
      "Train_EnvstepsSoFar : 499001\n",
      "Train_AverageReturn : 136.36039334839845\n",
      "Train_BestReturn : 152.15921525328358\n",
      "TimeSinceStart : 2212.102290868759\n",
      "Training Loss : 1.2850500345230103\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run with double DQN\n",
    "dqn_args = dict(dqn_base_args_dict)\n",
    "\n",
    "env_str = 'LunarLander'\n",
    "dqn_args['env_name'] = '{}-v3'.format(env_str)\n",
    "dqn_args['double_q'] = True\n",
    "\n",
    "# Delete all previous logs\n",
    "remove_folder('logs/dqn/{}/double_dqn'.format(env_str))\n",
    "\n",
    "for seed in range(3):\n",
    "    print(\"Running DQN experiment with seed\", seed)\n",
    "    dqn_args['seed'] = seed\n",
    "    dqn_args['logdir'] = 'logs/dqn/{}/double_dqn/seed{}'.format(env_str, seed)\n",
    "    dqntrainer = DQN_Trainer(dqn_args)\n",
    "    dqntrainer.run_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b096e91d44896167\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b096e91d44896167\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6012;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Visualize all DQN results on Lunar Lander\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/dqn/LunarLander/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
